{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-on Introduction to DataFrame API"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* DataFrames are lazily evaluated\n",
    "* They are implemented on top of RDDs\n",
    "* Spark will not process the data on calling the `transformation`, it will start processing when an `action` is called\n",
    "* Spark application starts with initializing a `SparkSession`\n",
    "* In the case of a pyspark shell, the shell automatically creates the session in the variable `spark` for users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (SparkSession\n",
    "    .builder\n",
    "    .appName(\"FirstProgram\")\n",
    "    .getOrCreate())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A pyspark DataFrame can be created via `pyspark.sql.SparkSession.createDataFrame` by passing a list of lists, tuples, dictionaries, pyspark.sql.Rows,Pandas DataFrame and an RDD\n",
    "* `pyspark.sql.SparkSession.createDataFrame` takes schema argument. If schema is not provided, then pyspark infers the schema from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = spark.createDataFrame([(\"Amar\",21),(\"Akbar\",25),(\"John\",28),(\"Harika\",32),(\"Amar\",35),(\"Akbar\",40)],\n",
    "                             [\"name\",\"age\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+\n",
      "|  name|age|\n",
      "+------+---+\n",
      "|  Amar| 21|\n",
      "| Akbar| 25|\n",
      "|  John| 28|\n",
      "|Harika| 32|\n",
      "|  Amar| 35|\n",
      "| Akbar| 40|\n",
      "+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Spark Python Data Types](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/data_types.html)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schemas and Creating DataFrames"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Spark can infer the schema from the data\n",
    "* If the dataset is large, it will be a overhead for spark to read a portion of the file and ascertain the datatype. This will be expensive and time-consuming\n",
    "* It is a good practice to define the schema upfront"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method-1 to define a Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_info = [(\"John\", \"Doe\", \"The Great Gatsby\", 1000.00, 5),\n",
    "        (\"Jane\", \"Smith\", \"To Kill a Mockingbird\", 1200.05, 3),\n",
    "        (\"Bob\", \"Johnson\", \"Pride and Prejudice\", 800.05, 4),\n",
    "        (\"Alice\", \"Davis\", \"The Catcher in the Rye\", 900.00, 2),\n",
    "        (\"Charlie\", \"Brown\", \"Moby-Dick\", 700.30, 6),\n",
    "        (\"Emily\", \"Wilson\", \"Wuthering Heights\", 1100.05, 1),\n",
    "        (\"Frank\", \"Garcia\", \"1984\", 1300.06, 7),\n",
    "        (\"Grace\", \"Martinez\", \"The Odyssey\", 600.00, 3),\n",
    "        (\"Henry\", \"Anderson\", \"War and Peace\", 1400.75, 8),\n",
    "        (\"Isabella\", \"Taylor\", \"The Divine Comedy\", 500.00, 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "schema = StructType(\n",
    "    [StructField(\"name\", StringType(), False),\n",
    "     StructField(\"surname\", StringType(), False),\n",
    "     StructField(\"book\", StringType(), False),\n",
    "     StructField(\"price\", FloatType(), False),\n",
    "     StructField(\"rating\", IntegerType(), False)\n",
    "     ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_data = spark.createDataFrame(author_info,schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viewing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+--------------------+-------+------+\n",
      "| name|surname|                book|  price|rating|\n",
      "+-----+-------+--------------------+-------+------+\n",
      "| John|    Doe|    The Great Gatsby| 1000.0|     5|\n",
      "| Jane|  Smith|To Kill a Mocking...|1200.05|     3|\n",
      "|  Bob|Johnson| Pride and Prejudice| 800.05|     4|\n",
      "|Alice|  Davis|The Catcher in th...|  900.0|     2|\n",
      "+-----+-------+--------------------+-------+------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "author_data.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-----------------------\n",
      " name    | John                 \n",
      " surname | Doe                  \n",
      " book    | The Great Gatsby     \n",
      " price   | 1000.0               \n",
      " rating  | 5                    \n",
      "-RECORD 1-----------------------\n",
      " name    | Jane                 \n",
      " surname | Smith                \n",
      " book    | To Kill a Mocking... \n",
      " price   | 1200.05              \n",
      " rating  | 3                    \n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "author_data.show(2,vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = false)\n",
      " |-- surname: string (nullable = false)\n",
      " |-- book: string (nullable = false)\n",
      " |-- price: float (nullable = false)\n",
      " |-- rating: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "author_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('name', StringType(), False), StructField('surname', StringType(), False), StructField('book', StringType(), False), StructField('price', FloatType(), False), StructField('rating', IntegerType(), False)])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "author_data.schema"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method-2 to define a schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_2 = \"name STRING, surname STRING, book STRING, price FLOAT, rating INT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_data_2 = spark.createDataFrame(author_info,schema_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+--------------------+-------+------+\n",
      "|    name| surname|                book|  price|rating|\n",
      "+--------+--------+--------------------+-------+------+\n",
      "|    John|     Doe|    The Great Gatsby| 1000.0|     5|\n",
      "|    Jane|   Smith|To Kill a Mocking...|1200.05|     3|\n",
      "|     Bob| Johnson| Pride and Prejudice| 800.05|     4|\n",
      "|   Alice|   Davis|The Catcher in th...|  900.0|     2|\n",
      "| Charlie|   Brown|           Moby-Dick|  700.3|     6|\n",
      "|   Emily|  Wilson|   Wuthering Heights|1100.05|     1|\n",
      "|   Frank|  Garcia|                1984|1300.06|     7|\n",
      "|   Grace|Martinez|         The Odyssey|  600.0|     3|\n",
      "|   Henry|Anderson|       War and Peace|1400.75|     8|\n",
      "|Isabella|  Taylor|   The Divine Comedy|  500.0|     2|\n",
      "+--------+--------+--------------------+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "author_data_2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- surname: string (nullable = true)\n",
      " |-- book: string (nullable = true)\n",
      " |-- price: float (nullable = true)\n",
      " |-- rating: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "author_data_2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['name', 'surname', 'book', 'price', 'rating']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting the column names of the pyspark DataFrame\n",
    "author_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+-----------------+\n",
      "|summary|            price|           rating|\n",
      "+-------+-----------------+-----------------+\n",
      "|  count|               10|               10|\n",
      "|   mean|950.1260131835937|              4.1|\n",
      "| stddev|302.8737534314304|2.330951164939612|\n",
      "|    min|            500.0|                1|\n",
      "|    max|          1400.75|                8|\n",
      "+-------+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(author_data.select(\"price\",\"rating\")\n",
    "            .describe()\n",
    "            .show())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`DataFrame.collect()` collects the data on the driver (equivaluent to local data in python). If the dataset is large and if it cannot be accomodated on the driver, it will throw a `out-of-memory` error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name='John', surname='Doe', book='The Great Gatsby', price=1000.0, rating=5),\n",
       " Row(name='Jane', surname='Smith', book='To Kill a Mockingbird', price=1200.050048828125, rating=3),\n",
       " Row(name='Bob', surname='Johnson', book='Pride and Prejudice', price=800.0499877929688, rating=4),\n",
       " Row(name='Alice', surname='Davis', book='The Catcher in the Rye', price=900.0, rating=2),\n",
       " Row(name='Charlie', surname='Brown', book='Moby-Dick', price=700.2999877929688, rating=6),\n",
       " Row(name='Emily', surname='Wilson', book='Wuthering Heights', price=1100.050048828125, rating=1),\n",
       " Row(name='Frank', surname='Garcia', book='1984', price=1300.06005859375, rating=7),\n",
       " Row(name='Grace', surname='Martinez', book='The Odyssey', price=600.0, rating=3),\n",
       " Row(name='Henry', surname='Anderson', book='War and Peace', price=1400.75, rating=8),\n",
       " Row(name='Isabella', surname='Taylor', book='The Divine Comedy', price=500.0, rating=2)]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Avoid using collect() method\n",
    "author_data.collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid `out-of-memory` error use `DataFrame.take()` or `DataFrame.tail()` methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name='John', surname='Doe', book='The Great Gatsby', price=1000.0, rating=5)]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "author_data.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert pyspark dataframe to a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_df = author_data.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(pandas_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(author_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting and Accessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import column\n",
    "from pyspark.sql.functions import upper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                book|\n",
      "+--------------------+\n",
      "|    The Great Gatsby|\n",
      "|To Kill a Mocking...|\n",
      "| Pride and Prejudice|\n",
      "|The Catcher in th...|\n",
      "|           Moby-Dick|\n",
      "|   Wuthering Heights|\n",
      "|                1984|\n",
      "|         The Odyssey|\n",
      "|       War and Peace|\n",
      "|   The Divine Comedy|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "author_data.select(\"book\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+--------------------+-------+------+--------------------+\n",
      "|    name| surname|                book|  price|rating|         upper_title|\n",
      "+--------+--------+--------------------+-------+------+--------------------+\n",
      "|    John|     Doe|    The Great Gatsby| 1000.0|     5|    THE GREAT GATSBY|\n",
      "|    Jane|   Smith|To Kill a Mocking...|1200.05|     3|TO KILL A MOCKING...|\n",
      "|     Bob| Johnson| Pride and Prejudice| 800.05|     4| PRIDE AND PREJUDICE|\n",
      "|   Alice|   Davis|The Catcher in th...|  900.0|     2|THE CATCHER IN TH...|\n",
      "| Charlie|   Brown|           Moby-Dick|  700.3|     6|           MOBY-DICK|\n",
      "|   Emily|  Wilson|   Wuthering Heights|1100.05|     1|   WUTHERING HEIGHTS|\n",
      "|   Frank|  Garcia|                1984|1300.06|     7|                1984|\n",
      "|   Grace|Martinez|         The Odyssey|  600.0|     3|         THE ODYSSEY|\n",
      "|   Henry|Anderson|       War and Peace|1400.75|     8|       WAR AND PEACE|\n",
      "|Isabella|  Taylor|   The Divine Comedy|  500.0|     2|   THE DIVINE COMEDY|\n",
      "+--------+--------+--------------------+-------+------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating a new column from existing column\n",
    "author_data.withColumn('upper_title', upper(author_data.book)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+--------------------+-------+------+\n",
      "| name| surname|                book|  price|rating|\n",
      "+-----+--------+--------------------+-------+------+\n",
      "| Jane|   Smith|To Kill a Mocking...|1200.05|     3|\n",
      "|Emily|  Wilson|   Wuthering Heights|1100.05|     1|\n",
      "|Frank|  Garcia|                1984|1300.06|     7|\n",
      "|Henry|Anderson|       War and Peace|1400.75|     8|\n",
      "+-----+--------+--------------------+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filtering the data\n",
    "author_data.filter(author_data.price > 1000).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying a Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* pyspark supports User Defined Functions (UDFs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.functions import pandas_udf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`@pandas_udf('long')` is a decorator used to define a Pandas UDF (User-Defined Function) in PySpark. The `'long'` argument specifies the return type of the UDF. In this case, it indicates that the UDF returns a value of type `long`, which is an alias for `bigint` in PySpark.\n",
    "\n",
    "Pandas UDFs allow you to use Pandas functions in PySpark and can provide significant performance improvements over regular UDFs. The return type must be specified when defining a Pandas UDF so that PySpark can correctly handle the data returned by the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pandas_udf('long')\n",
    "def pandas_multiply_ten(series: pd.Series) -> pd.Series:\n",
    "    return series*10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------------+\n",
      "|  price|pandas_multiply_ten(price)|\n",
      "+-------+--------------------------+\n",
      "| 1000.0|                     10000|\n",
      "|1200.05|                     12000|\n",
      "| 800.05|                      8000|\n",
      "|  900.0|                      9000|\n",
      "|  700.3|                      7003|\n",
      "|1100.05|                     11000|\n",
      "|1300.06|                     13000|\n",
      "|  600.0|                      6000|\n",
      "|1400.75|                     14007|\n",
      "|  500.0|                      5000|\n",
      "+-------+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "author_data.select(author_data.price,pandas_multiply_ten(author_data.price)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_df = author_data.select(author_data.price,pandas_multiply_ten(author_data.price))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(price_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    (\"A\", 1, 100),\n",
    "    (\"B\", 2, 200),\n",
    "    (\"A\", 3, 300),\n",
    "    (\"B\", 4, 400),\n",
    "    (\"C\", 5, 500)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(data,[\"key\", \"value1\", \"value2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+\n",
      "|key|value1|value2|\n",
      "+---+------+------+\n",
      "|  A|     1|   100|\n",
      "|  B|     2|   200|\n",
      "|  A|     3|   300|\n",
      "|  B|     4|   400|\n",
      "|  C|     5|   500|\n",
      "+---+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+-----------+\n",
      "|key|avg(value1)|avg(value2)|\n",
      "+---+-----------+-----------+\n",
      "|  A|        2.0|      200.0|\n",
      "|  B|        3.0|      300.0|\n",
      "|  C|        5.0|      500.0|\n",
      "+---+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupby('key').avg().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save in CSV Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+\n",
      "|key|value1|value2|\n",
      "+---+------+------+\n",
      "|  B|     4|   400|\n",
      "|  A|     3|   300|\n",
      "|  B|     2|   200|\n",
      "|  A|     1|   100|\n",
      "|  C|     5|   500|\n",
      "+---+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.write.csv('output.csv',header=True)\n",
    "spark.read.csv('output.csv',header=True).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save in Parquet Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+\n",
      "|key|value1|value2|\n",
      "+---+------+------+\n",
      "|  A|     1|   100|\n",
      "|  B|     4|   400|\n",
      "|  A|     3|   300|\n",
      "|  B|     2|   200|\n",
      "|  C|     5|   500|\n",
      "+---+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.write.parquet('output.parquet')\n",
    "spark.read.parquet('output.parquet').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save in ORC format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+\n",
      "|key|value1|value2|\n",
      "+---+------+------+\n",
      "|  C|     5|   500|\n",
      "|  B|     2|   200|\n",
      "|  B|     4|   400|\n",
      "|  A|     3|   300|\n",
      "|  A|     1|   100|\n",
      "+---+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.write.orc('output.orc')\n",
    "spark.read.orc('output.orc').show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark_learn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
