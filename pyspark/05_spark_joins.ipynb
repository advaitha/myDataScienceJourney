{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Joins"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark has five distinct join strategies  \n",
    "* Broadcast hash join \n",
    "* Shuffle hash join\n",
    "* Shuffle sort merge join\n",
    "* Broadcast nested loop join\n",
    "* shuffle and replicated nested loop join or cartesian product join"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Broadcast Hash Join"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* It is employed when we want to join a small dataset with a large dataset\n",
    "* The smaller dataset is broadcasted by driver to all executors\n",
    "* Moving the larger dataset is avoided\n",
    "* By default spark will use broadcast join if the smaller dataset is less than 10 MB\n",
    "* configuration for broadcast join `spark.sql.autoBroadcastJoinThreshold`\n",
    "* We can either increase or decrease the threshold as required  \n",
    "  \n",
    "![Broadcast Hash Join](/Images/broadcast_hash_join.png)\n",
    "  \n",
    "![Broadcast Hash Join](/Images/broadcast_hash_join_2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import broadcast\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/06 17:39:52 WARN Utils: Your hostname, thulasiram resolves to a loopback address: 127.0.1.1; using 192.168.0.105 instead (on interface wlp0s20f3)\n",
      "23/05/06 17:39:52 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/06 17:39:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('joins').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "largeDF = spark.range(1, 10000)\n",
    "smallDF = spark.range(1, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "joinedDF = largeDF.join(smallDF,\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [id#0L]\n",
      "   +- BroadcastHashJoin [id#0L], [id#2L], Inner, BuildRight, false\n",
      "      :- Range (1, 10000, step=1, splits=16)\n",
      "      +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, false]),false), [plan_id=16]\n",
      "         +- Range (1, 100, step=1, splits=16)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joinedDF.explain(mode='simple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan (6)\n",
      "+- Project (5)\n",
      "   +- BroadcastHashJoin Inner BuildRight (4)\n",
      "      :- Range (1)\n",
      "      +- BroadcastExchange (3)\n",
      "         +- Range (2)\n",
      "\n",
      "\n",
      "(1) Range\n",
      "Output [1]: [id#0L]\n",
      "Arguments: Range (1, 10000, step=1, splits=Some(16))\n",
      "\n",
      "(2) Range\n",
      "Output [1]: [id#2L]\n",
      "Arguments: Range (1, 100, step=1, splits=Some(16))\n",
      "\n",
      "(3) BroadcastExchange\n",
      "Input [1]: [id#2L]\n",
      "Arguments: HashedRelationBroadcastMode(List(input[0, bigint, false]),false), [plan_id=16]\n",
      "\n",
      "(4) BroadcastHashJoin\n",
      "Left keys [1]: [id#0L]\n",
      "Right keys [1]: [id#2L]\n",
      "Join condition: None\n",
      "\n",
      "(5) Project\n",
      "Output [1]: [id#0L]\n",
      "Input [2]: [id#0L, id#2L]\n",
      "\n",
      "(6) AdaptiveSparkPlan\n",
      "Output [1]: [id#0L]\n",
      "Arguments: isFinalPlan=false\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joinedDF.explain(mode='formatted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffle Hash Join"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Moving the data with the same value of join key in the same executor node  \n",
    "* Followed by hash join  \n",
    "* Join key does not need to be sortable  \n",
    "* Expensive join that involves both shuffling and Hashing  \n",
    "  \n",
    "![Shuffle Hash Join](/Images/shuffle_hash_join.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffle Sort Merge Join"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Shuffle sort-merge Join](/Images/sort_merge_join.png)\n",
    "\n",
    "* Shuffle sort-merge join involves shuffling of data to get the same join_key with the same worker\n",
    "* sort-merge join operation at the partition level in the worker nodes\n",
    "* The join keys need to be sortable\n",
    "* sortMergeJoin is enabled via `spark.sql.join.preferSortMergeJoin`\n",
    "* Default Join in spark\n",
    "\n",
    "![Shuffle sort-merge Join](/Images/shuffle_sort_merge_join.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizing Shuffle Sort Merge Join"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Bucketing is an optimization technqiue\n",
    "* Using Bucketing to determine data partitioning and avoid data shuffle\n",
    "* use `bucketBy` method on the datasets so that keys are co-located\n",
    "* The number of buckets and the bucketing columns have to be the same across DataFrames participating in join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "largeDF =spark.range(1, 100000)\n",
    "smallDF = spark.range(1, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                        (0 + 16) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/06 18:39:05 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "23/05/06 18:39:05 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "23/05/06 18:39:05 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "23/05/06 18:39:05 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 69.09% for 11 writers\n",
      "23/05/06 18:39:05 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 63.33% for 12 writers\n",
      "23/05/06 18:39:05 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 58.46% for 13 writers\n",
      "23/05/06 18:39:05 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 54.29% for 14 writers\n",
      "23/05/06 18:39:05 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 50.67% for 15 writers\n",
      "23/05/06 18:39:05 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 47.50% for 16 writers\n",
      "23/05/06 18:39:05 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 50.67% for 15 writers\n",
      "23/05/06 18:39:05 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 54.29% for 14 writers\n",
      "23/05/06 18:39:05 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 58.46% for 13 writers\n",
      "23/05/06 18:39:05 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 63.33% for 12 writers\n",
      "23/05/06 18:39:05 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 69.09% for 11 writers\n",
      "23/05/06 18:39:05 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "23/05/06 18:39:05 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "23/05/06 18:39:05 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "23/05/06 18:39:05 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "23/05/06 18:39:05 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "23/05/06 18:39:05 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "23/05/06 18:39:05 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "23/05/06 18:39:05 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "23/05/06 18:39:05 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 69.09% for 11 writers\n",
      "23/05/06 18:39:05 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "23/05/06 18:39:05 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "23/05/06 18:39:05 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "23/05/06 18:39:05 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "23/05/06 18:39:05 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "23/05/06 18:39:05 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/06 18:39:06 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "23/05/06 18:39:06 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "23/05/06 18:39:06 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "23/05/06 18:39:06 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "23/05/06 18:39:06 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "23/05/06 18:39:06 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Bucket the dataframe\n",
    "(largeDF.write.bucketBy(12, \"id\")\n",
    ".sortBy(\"id\")\n",
    ".saveAsTable(\"largeTable\")\n",
    ")\n",
    "\n",
    "(smallDF.write.bucketBy(12, \"id\")\n",
    " .sortBy(\"id\")\n",
    " .saveAsTable(\"smallTable\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the bucketed tables\n",
    "bucketedLargeDF = spark.read.table(\"largeTable\")\n",
    "bucketedSmallDF = spark.read.table(\"smallTable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "joinedDF = bucketedLargeDF.join(bucketedSmallDF, \"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Join UsingJoin(Inner,Buffer(id))\n",
      ":- SubqueryAlias spark_catalog.default.largetable\n",
      ":  +- Relation default.largetable[id#12L] parquet\n",
      "+- SubqueryAlias spark_catalog.default.smalltable\n",
      "   +- Relation default.smalltable[id#14L] parquet\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "id: bigint\n",
      "Project [id#12L]\n",
      "+- Join Inner, (id#12L = id#14L)\n",
      "   :- SubqueryAlias spark_catalog.default.largetable\n",
      "   :  +- Relation default.largetable[id#12L] parquet\n",
      "   +- SubqueryAlias spark_catalog.default.smalltable\n",
      "      +- Relation default.smalltable[id#14L] parquet\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [id#12L]\n",
      "+- Join Inner, (id#12L = id#14L)\n",
      "   :- Filter isnotnull(id#12L)\n",
      "   :  +- Relation default.largetable[id#12L] parquet\n",
      "   +- Filter isnotnull(id#14L)\n",
      "      +- Relation default.smalltable[id#14L] parquet\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [id#12L]\n",
      "   +- SortMergeJoin [id#12L], [id#14L], Inner\n",
      "      :- Sort [id#12L ASC NULLS FIRST], false, 0\n",
      "      :  +- Filter isnotnull(id#12L)\n",
      "      :     +- FileScan parquet default.largetable[id#12L] Batched: true, Bucketed: true, DataFilters: [isnotnull(id#12L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/home/thulasiram/personal/data_engineering/spark/pyspark/spark-wa..., PartitionFilters: [], PushedFilters: [IsNotNull(id)], ReadSchema: struct<id:bigint>, SelectedBucketsCount: 12 out of 12\n",
      "      +- Sort [id#14L ASC NULLS FIRST], false, 0\n",
      "         +- Filter isnotnull(id#14L)\n",
      "            +- FileScan parquet default.smalltable[id#14L] Batched: true, Bucketed: true, DataFilters: [isnotnull(id#14L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/home/thulasiram/personal/data_engineering/spark/pyspark/spark-wa..., PartitionFilters: [], PushedFilters: [IsNotNull(id)], ReadSchema: struct<id:bigint>, SelectedBucketsCount: 12 out of 12\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joinedDF.explain(mode='extended')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cartesian Join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The cartesian product of the two relations is calculated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Broadcast nested loop Join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* supports non-equi joins"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Slow operation\n",
    "for record_1 in relation_1:\n",
    "  for record_2 in relation_2:\n",
    "    # join condition is executed"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference\n",
    "[Medium article on Spark Join Strategies](https://towardsdatascience.com/strategies-of-spark-join-c0e7b4572bcf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark_learn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
