{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Delta Lakes with Apache Spark"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Data Lakes are required?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The importance of an Optimal Storage Solution:  \n",
    "* Scalability and performance\n",
    "* Transaction support\n",
    "* Support for diverse data formats\n",
    "* Support for diverse workloads\n",
    "* Openness\n",
    "  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different storage solutions available:  \n",
    "* Databases\n",
    "* Data lakes\n",
    "* Lake houses\n",
    "  \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Databases"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Designed to store structured data as tables\n",
    "* Adherence to strict schema\n",
    "* Strong transactional ACID guarantees\n",
    "* SQL workloads\n",
    "    * OLTP\n",
    "    * OLAP (Supported by Spark)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitations of Databases"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trends in the Industry  \n",
    "* Growth in data sizes\n",
    "* Growth in diversity of analytics\n",
    "\n",
    "Limitations of the Databases  \n",
    "* Databases are extremely expensive to scale out\n",
    "* Databases do not support non-sql analytics well\n",
    "\n",
    "These development led to the Growth of Data Lakes  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Lakes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data lakes decouple storage and the compute.\n",
    "Data lakes are built by choosing the following:  \n",
    "* Storage system - HDFS or cloud object store\n",
    "* File format - Parquet, ORC, JSON\n",
    "* Processing engine - Spark, Presto, Flink \n",
    "Data lakes provide a cheaper solution than databases."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why use Apache Spark for Building Data Lakes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Support for diverse workloads\n",
    "  * Batch processing\n",
    "  * Stream processing\n",
    "  * ETL\n",
    "  * SQL workloads\n",
    "  * ML\n",
    "* Support for diverse file formats\n",
    "* Support for diverse file systems\n",
    "  * Read and write to different storage systems"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitations of Data Lakes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Fail to provide ACID guarantees\n",
    "  * No mechanism to roll back files already written\n",
    "  * No isolation when concurrent workloads modify the data\n",
    "  * Inconsistent view of data due to failed writes\n",
    "  * Writing out files in a format and schema inconsistent with existing data "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lakehouses"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* It combines the best elements of databases and data lakes\n",
    "* ACID guarantees for transaction support\n",
    "* Schema enforcement\n",
    "* Support for diverse datatypes\n",
    "* Support for diverse workloads\n",
    "* Support for upserts and deletes\n",
    "* Data governance\n",
    "\n",
    "Lakehouse systems available:  \n",
    "* Apache Hudi\n",
    "* Apache Iceberg\n",
    "* Delta Lake \n",
    "  * strong integration with Apace Spark\n",
    "  * Developed by the creators of Spark\n",
    "\n",
    "They do the following:  \n",
    "* Store large volumes of data in structured file formats\n",
    "* Scalable filesystems\n",
    "* Maintain a transaction log to record timeline of atomic changes to the data\n",
    "* Use log to define versions of the table data\n",
    "* Isolation guarantees between readers and writers\n",
    "* Support reading and writing with Apache Spark"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delta Lake"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It supports:  \n",
    "  * Open data storage format\n",
    "  * Provides transactional guarantees  \n",
    "  * Schema enforcement  \n",
    "  * Schema evolution  \n",
    "  * Support structured streaming  \n",
    "  * Update, delete and merge operations in Java, Scala and Python  \n",
    "  * Time travel  \n",
    "  * Rollback to previous versions  \n",
    "  * Isolation between multiple concurrent writers   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Delta Lake"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install Delta in python environment\n",
    "```\n",
    "pip install delta-spark\n",
    "```\n",
    "and configure SparkSession with `configure_spark_with_delta_pip()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from delta import *\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/07 09:59:52 WARN Utils: Your hostname, thulasiram resolves to a loopback address: 127.0.1.1; using 192.168.0.105 instead (on interface wlp0s20f3)\n",
      "23/05/07 09:59:52 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      ":: loading settings :: url = jar:file:/home/thulasiram/mambaforge/envs/spark_learn/lib/python3.9/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/thulasiram/.ivy2/cache\n",
      "The jars for the packages stored in: /home/thulasiram/.ivy2/jars\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-38aecb9f-d28c-45a4-85dc-63c8a58a46ee;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-core_2.12;2.3.0 in central\n",
      "\tfound io.delta#delta-storage;2.3.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.8 in central\n",
      "downloading https://repo1.maven.org/maven2/io/delta/delta-core_2.12/2.3.0/delta-core_2.12-2.3.0.jar ...\n",
      "\t[SUCCESSFUL ] io.delta#delta-core_2.12;2.3.0!delta-core_2.12.jar (3864ms)\n",
      "downloading https://repo1.maven.org/maven2/io/delta/delta-storage/2.3.0/delta-storage-2.3.0.jar ...\n",
      "\t[SUCCESSFUL ] io.delta#delta-storage;2.3.0!delta-storage.jar (622ms)\n",
      "downloading https://repo1.maven.org/maven2/org/antlr/antlr4-runtime/4.8/antlr4-runtime-4.8.jar ...\n",
      "\t[SUCCESSFUL ] org.antlr#antlr4-runtime;4.8!antlr4-runtime.jar (741ms)\n",
      ":: resolution report :: resolve 7161ms :: artifacts dl 5233ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-core_2.12;2.3.0 from central in [default]\n",
      "\tio.delta#delta-storage;2.3.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.8 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   3   |   3   |   0   ||   3   |   3   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-38aecb9f-d28c-45a4-85dc-63c8a58a46ee\n",
      "\tconfs: [default]\n",
      "\t3 artifacts copied, 0 already retrieved (4246kB/6ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/07 10:00:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "builder = SparkSession.builder.appName(\"Myapp\") \\\n",
    "    .config(\"spark.sql.extensions\",\"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\",\"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data into a Delta Lake Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sourcePath = '/home/thulasiram/personal/data_engineering/data/loan-risks.snappy.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Delta Lake path\n",
    "deltaPath = '/home/thulasiram/personal/data_engineering/data/loans_delta'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/07 10:28:25 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Create the Delta lake table with the loans data downloaded\n",
    "(spark.read.format('parquet')\n",
    " .load(sourcePath)\n",
    " .write.format(\"delta\")\n",
    " .save(deltaPath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a view on the data\n",
    "(spark.read.format(\"delta\")\n",
    " .load(deltaPath)\n",
    " .createOrReplaceTempView(\"loans_delta\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|   14705|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Explore the data\n",
    "spark.sql(\"select count(*) from loans_delta\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+---------+----------+\n",
      "|loan_id|funded_amnt|paid_amnt|addr_state|\n",
      "+-------+-----------+---------+----------+\n",
      "|      0|       1000|   182.22|        CA|\n",
      "|      1|       1000|   361.19|        WA|\n",
      "|      2|       1000|   176.26|        TX|\n",
      "|      3|       1000|   1000.0|        OK|\n",
      "|      4|       1000|   249.98|        PA|\n",
      "+-------+-----------+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from loans_delta limit 5\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enforcing Schema on Write to Prevent Data Corruption"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A common problem with managing data with Spark using formats like JSON, Parquet and ORC is accidental data corruption caused by writing incorrectly formated data format. \n",
    "* These formats define the layout of individual files and not of an entire table\n",
    "* There is no guarantees of consistency for the entire table of many parquet files\n",
    "* Delta lake format records the schema as table-level metadata\n",
    "* Delta lake table can verify if the data being written has a schema compatible with that of the table\n",
    "* If the schema is not compatible, spark will throw an error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "A schema mismatch detected when writing to the Delta table (Table ID: 00d13bf8-8fb3-49b5-b56a-887a2b772dde).\nTo enable schema migration using DataFrameWriter or DataStreamWriter, please set:\n'.option(\"mergeSchema\", \"true\")'.\nFor other operations, set the session configuration\nspark.databricks.delta.schema.autoMerge.enabled to \"true\". See the documentation\nspecific to the operation for details.\n\nTable schema:\nroot\n-- loan_id: long (nullable = true)\n-- funded_amnt: integer (nullable = true)\n-- paid_amnt: double (nullable = true)\n-- addr_state: string (nullable = true)\n\n\nData schema:\nroot\n-- loan_id: long (nullable = true)\n-- funded_amnt: integer (nullable = true)\n-- paid_amnt: double (nullable = true)\n-- addr_state: string (nullable = true)\n-- closed: boolean (nullable = true)\n\n         ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 12\u001b[0m\n\u001b[1;32m      5\u001b[0m items \u001b[39m=\u001b[39m [\n\u001b[1;32m      6\u001b[0m    (\u001b[39m1111111\u001b[39m, \u001b[39m1000\u001b[39m, \u001b[39m1000.0\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mTX\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mTrue\u001b[39;00m), \n\u001b[1;32m      7\u001b[0m    (\u001b[39m2222222\u001b[39m, \u001b[39m2000\u001b[39m, \u001b[39m0.0\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mCA\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m      8\u001b[0m ]\n\u001b[1;32m     10\u001b[0m loanUpdates \u001b[39m=\u001b[39m (spark\u001b[39m.\u001b[39mcreateDataFrame(items, cols)\n\u001b[1;32m     11\u001b[0m   \u001b[39m.\u001b[39mwithColumn(\u001b[39m\"\u001b[39m\u001b[39mfunded_amnt\u001b[39m\u001b[39m\"\u001b[39m, col(\u001b[39m\"\u001b[39m\u001b[39mfunded_amnt\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mcast(\u001b[39m\"\u001b[39m\u001b[39mint\u001b[39m\u001b[39m\"\u001b[39m)))\n\u001b[0;32m---> 12\u001b[0m loanUpdates\u001b[39m.\u001b[39;49mwrite\u001b[39m.\u001b[39;49mformat(\u001b[39m\"\u001b[39;49m\u001b[39mdelta\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39;49mmode(\u001b[39m\"\u001b[39;49m\u001b[39mappend\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39;49msave(deltaPath)\n",
      "File \u001b[0;32m~/mambaforge/envs/spark_learn/lib/python3.9/site-packages/pyspark/sql/readwriter.py:968\u001b[0m, in \u001b[0;36mDataFrameWriter.save\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m    966\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jwrite\u001b[39m.\u001b[39msave()\n\u001b[1;32m    967\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 968\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jwrite\u001b[39m.\u001b[39;49msave(path)\n",
      "File \u001b[0;32m~/mambaforge/envs/spark_learn/lib/python3.9/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/mambaforge/envs/spark_learn/lib/python3.9/site-packages/pyspark/sql/utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    192\u001b[0m converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n\u001b[1;32m    193\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    194\u001b[0m     \u001b[39m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[39m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     \u001b[39mraise\u001b[39;00m converted \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: A schema mismatch detected when writing to the Delta table (Table ID: 00d13bf8-8fb3-49b5-b56a-887a2b772dde).\nTo enable schema migration using DataFrameWriter or DataStreamWriter, please set:\n'.option(\"mergeSchema\", \"true\")'.\nFor other operations, set the session configuration\nspark.databricks.delta.schema.autoMerge.enabled to \"true\". See the documentation\nspecific to the operation for details.\n\nTable schema:\nroot\n-- loan_id: long (nullable = true)\n-- funded_amnt: integer (nullable = true)\n-- paid_amnt: double (nullable = true)\n-- addr_state: string (nullable = true)\n\n\nData schema:\nroot\n-- loan_id: long (nullable = true)\n-- funded_amnt: integer (nullable = true)\n-- paid_amnt: double (nullable = true)\n-- addr_state: string (nullable = true)\n-- closed: boolean (nullable = true)\n\n         "
     ]
    }
   ],
   "source": [
    "# 'closed' column is not there in the data\n",
    "# This will give error with schema mismatch\n",
    "from pyspark.sql.functions import *\n",
    "cols = ['loan_id','funded_amnt','paid_amnt',\n",
    "        'addr_state','closed']\n",
    "items = [\n",
    "   (1111111, 1000, 1000.0, 'TX', True), \n",
    "   (2222222, 2000, 0.0, 'CA', False)\n",
    "]\n",
    "\n",
    "loanUpdates = (spark.createDataFrame(items, cols)\n",
    "  .withColumn(\"funded_amnt\", col(\"funded_amnt\").cast(\"int\")))\n",
    "loanUpdates.write.format(\"delta\").mode(\"append\").save(deltaPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schema Evolution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "(loanUpdates.write.format(\"delta\").mode(\"append\")\n",
    "  .option(\"mergeSchema\", \"true\")\n",
    "  .save(deltaPath))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming Existing Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common use case when managing data is fixing errors in the data. Suppose, upon reviewing the data, we realized that all of the loans assigned to addr_state = 'OR' should have been assigned to addr_state = 'WA'. If the loan table were a Parquet table, then to do such an update we would need to:\n",
    "\n",
    "* Copy all of the rows that are not affected into a new table.\n",
    "* Copy all of the rows that are affected into a DataFrame, then perform the data modification.\n",
    "* Insert the previously noted DataFrame’s rows into the new table.\n",
    "* Remove the old table and rename the new table to the old table name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from delta.tables import *\n",
    "\n",
    "deltaTable = DeltaTable.forPath(spark,deltaPath)\n",
    "deltaTable.update(\"addr_state = 'OR'\", {\"addr_state\":\"'WA'\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taking care of GDPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting the data on all loans that have been paid\n",
    "deltaTable = DeltaTable.forPath(spark,deltaPath)\n",
    "deltaTable.delete(\"funded_amnt >= paid_amnt\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upserts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Taking care of change data capture\n",
    "* Replicate changes to OLTP to OLAP workloads\n",
    "* To continue with our loan data example, say we have another table of new loan information, some of which are new loans and others of which are updates to existing loans. In addition, let’s say this changes table has the same schema as the loan_delta table. You can upsert these changes into the table using the DeltaTable.merge() operation, which is based on the MERGE SQL command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "(deltaTable\n",
    ".alias(\"t\")\n",
    ".merge(loanUpdates.alias(\"s\"), \"t.loan_id = s.loan_id\")\n",
    ".whenMatchedUpdateAll()\n",
    ".whenNotMatchedInsertAll()\n",
    ".execute()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auditing Data Changes with Operation History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "|version|           timestamp|userId|userName|operation| operationParameters| job|notebook|clusterId|readVersion|isolationLevel|isBlindAppend|    operationMetrics|userMetadata|          engineInfo|\n",
      "+-------+--------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "|      4|2023-05-07 11:08:...|  null|    null|    MERGE|{predicate -> (t....|null|    null|     null|          3|  Serializable|        false|{numTargetRowsCop...|        null|Apache-Spark/3.3....|\n",
      "|      3|2023-05-07 11:00:...|  null|    null|   DELETE|{predicate -> [\"(...|null|    null|     null|          2|  Serializable|        false|{numRemovedFiles ...|        null|Apache-Spark/3.3....|\n",
      "|      2|2023-05-07 10:55:...|  null|    null|   UPDATE|{predicate -> (ad...|null|    null|     null|          1|  Serializable|        false|{numRemovedFiles ...|        null|Apache-Spark/3.3....|\n",
      "|      1|2023-05-07 10:50:...|  null|    null|    WRITE|{mode -> Append, ...|null|    null|     null|          0|  Serializable|         true|{numFiles -> 3, n...|        null|Apache-Spark/3.3....|\n",
      "|      0|2023-05-07 10:28:...|  null|    null|    WRITE|{mode -> ErrorIfE...|null|    null|     null|       null|  Serializable|         true|{numFiles -> 1, n...|        null|Apache-Spark/3.3....|\n",
      "+-------+--------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "deltaTable.history().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------------+---------+\n",
      "|version|timestamp              |operation|\n",
      "+-------+-----------------------+---------+\n",
      "|4      |2023-05-07 11:08:35.63 |MERGE    |\n",
      "|3      |2023-05-07 11:00:01.792|DELETE   |\n",
      "|2      |2023-05-07 10:55:50.402|UPDATE   |\n",
      "+-------+-----------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Printing key columns \n",
    "(deltaTable\n",
    " .history(3)\n",
    " .select(\"version\",\"timestamp\",\"operation\")\n",
    " .show(truncate=False) \n",
    " )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Travel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+---------+----------+------+\n",
      "|loan_id|funded_amnt|paid_amnt|addr_state|closed|\n",
      "+-------+-----------+---------+----------+------+\n",
      "|1111111|       1000|   1000.0|        TX|  true|\n",
      "|2222222|       2000|      0.0|        CA| false|\n",
      "+-------+-----------+---------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(spark.read\n",
    " .format(\"delta\")\n",
    " .option(\"timesatmpAsOf\",\"2023-05-07\")\n",
    " .load(deltaPath)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+---------+----------+------+\n",
      "|loan_id|funded_amnt|paid_amnt|addr_state|closed|\n",
      "+-------+-----------+---------+----------+------+\n",
      "|1111111|       1000|   1000.0|        TX|  true|\n",
      "|2222222|       2000|      0.0|        CA| false|\n",
      "+-------+-----------+---------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(spark.read.format(\"delta\")\n",
    " .option(\"versionAsOf\",\"4\")\n",
    " .load(deltaPath)\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Databases fail to fulfill modern use cases \n",
    "* Databases are designed to overcome database limitations\n",
    "* Datalakes lack ACID guarantees provided by Databases\n",
    "* Lakehouses are next generation data solutions - Provide best features of databases and data lakes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark_learn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
