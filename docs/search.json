[
  {
    "objectID": "bayesian_analysis/resources.html",
    "href": "bayesian_analysis/resources.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Resources\n\nBayesian Analysis for Hackers - Book\nBayesian Methods for Hackers - Book Bayesian reasoning and ML Book Good Book on Bayesian Modeling in python"
  },
  {
    "objectID": "bayesian_analysis/Bayesian_analysis.html",
    "href": "bayesian_analysis/Bayesian_analysis.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Bayesian inference is updating your beliefs after considering new evidence.\nWe re-weight our prior beliefs after observing the evience and arrive at posterior probabilities\nAs we gather an infinite amount of evidence, Bayesian results align with frequentist results. For large N, statistical inference is more or less objective. For small N inference is much more unstable, frequentist estimates have more variance and larger confidence intervals.\nBayesian inference preserve the uncertainty that reflects the instability for a small N dataset. \nA good starting point for Bayesian modeling is to think about how the data might have been generated.  \nFor getting the samples from posterior probability Monte Carlo Marko Chain (MCMC) algorithms are used \nIn MCMC only the current location matters (memoryless)\nLaplace Approximations, Variational Bayes methods are also used to approximate the posterior.\nMetropolis sampling is used fElemwiseCategorical for categorical variables.\nA low auto-correlation between posterior samples are preferred\nThinning - If we take every nth sample, we can remove some autocorrelation, with more thinning, the autocorrelation drops quicker. Higher thinning requires more MCMC iterations to achieve the same number of returned samples\nLaw of large numbers - The average of a sequence of random variables from the same distribution converges to the expected value of that distribution. The law of large numbers is only valid as N gets infinitely large:never truly attainable.\nSmall datasets should not be processed using the law of large numbers. We will encounter this problem when try to analyze subset of the data on some specific categories\nIn Bayesian inference are computed using expected values. If we can sample from the posterior distribution directly, we simply need to evaluate averages.If further accuracy is desired, take more samples from the posterior.\nBayesian inference also include loss functions. Instead of returning the average of posterior distribution, we can calculate minimum of the loss function to choose the estimate that minimized the expected loss. Minimum of the expected loss is called ‘Bayes action’\nIn traditional ML it often happens that prediction measure and what frequentist methods are optimizing for are very different. For example in logistic regression trying to optimize cross entropy loss and trying to measure AUC, ROC, Precision etc. On the contrary, Bayes action is equivalent to finding parameters that optimize not parameter accuracy but an arbitary performance measure like loss functions, AUC, ROC, precision / Recall etc.\nPrior selected can be either objective or subjective. Objective prior is following the “principle of indifference” (all values are equally probable). Subjective prior is like adding our belief into the system ex. injecting domain knowledge.\nIf the posterior does not make sense, then it means that the prior may not contain all the information and needs to be updated\nEmpirical Bayes combines frequentist and bayesian inference. Frequentist methods are used to find the prior’s hyperparameters and then proceed with bayesian methods. It was suggested not to use empirical bayes unless you have lots of data \n\n\n\n\nGamma distribution\nWishart distribution - It is a distribution over all positive semi-definite matrices. covariance matrices are positive-definite, hence the Wishart is an appropriate prior for covariance matrices.\nBeta distribution is a generalization of uniform distribution\nA beta prior with binomial observations creates a beta posterior"
  },
  {
    "objectID": "computer_vision/object_detection/introduction.html",
    "href": "computer_vision/object_detection/introduction.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "The system looks at the image and proposes RoIs for further analysis. RoIs are regions that the system believes have a high likelihood of containing an object, called the objectness score\n\n\n\n\n\nExtract features from ROI to determine the class of the image.\nTwo predictions are made for each region\n\nBounding box prediction\nClass prediction\n\n\n\n\n\n\nMultiple detections map happen for the same object. NMS is a technique that makes sure the detection algorithm detects each object only once.\n\n\n\n\n\n\nFrames per second (FPS) to measure detection speed - R-CNN operates at 7 FPS, SSD operates at 59 FPS.\nMean average precision (mAP) - It is a percentage from 0 to 100, and higher values are better.\n\n\n\n\nIt evaluates the overlap between two bounding boxes - Ground truth bounding box and the predicted bounding box\n\n\nIOU = Area of Intersection / Area of the Union IOU should be greater than a threshold which is tunable according to the problem.\nIf the IOU value is above this threshod, the prediction is considered a True Positive (TP) and if it is below the threshold, it is considered a False Positive (FP)\n\n\n\n\n\n\nsteps to calculate mAP\n\n\n\n\n\n\n\n\n\n\nselective search along with bottom up segmentation for region proposal\n\n\n\n\n\nVery slow\nThree separate modules which does not share computation\n\n\n\n\n\n\nThe loss will be multi-task loss - For predicting the class of the objects and the bounding box.\nThe region proposal is coming from a different model. This slow down the algorithm.\n\n\n\nInstead of using a selective search algorithm on the feature map to identify the region proposals, a region proposal network (RPN) is used to predict the region proposals as part of the training process.\nThe predicted region proposals are then reshaped using an RoI pooling layer and used to classify the image within the proposed region and predict the offset values for the bounding boxes\n\nIn general, single-stage detectors (SSD, yolo) tend to be less accurate than two-stage detectors (R-CNN) but are significantly faster.\n\n\n\n\n\n\nSSD Architecture\n\n\n\n\n\n\n\n\nYolo working\n\n\n\n\n\nYolo3 architecture\n\n\n\n\n\nImage classification is the task of predicting the type or class of an object in an image.\nObject detection is the task of predicting the location of objects in an image via bounding boxes and the classes of the located objects.\nThe general framework of object detection systems consists of four main components: region proposals, feature extraction and predictions, non-maximum suppression, and evaluation metrics.\nObject detection algorithms are evaluated using two main metrics: frame per second (FPS) to measure the network’s speed, and mean average precision (mAP) to measure the network’s precision.\nThe three most popular object detection systems are the R-CNN family of networks, SSD, and the YOLO family of networks.\nThe R-CNN family of networks has three main variations: R-CNN, Fast R-CNN, and Faster R-CNN. R-CNN and Fast R-CNN use a selective search algorithm to propose RoIs, whereas Faster R-CNN is an end-to-end DL system that uses a region proposal network to propose RoIs.\nThe YOLO family of networks include YOLOv1, YOLOv2 (or YOLO9000), and YOLOv3.\nR-CNN is a multi-stage detector: it separates the process to predict the objectness score of the bounding box and the object class into two different stages.\nSSD and YOLO are single-stage detectors: the image is passed once through the network to predict the objectness score and the object class.\nIn general, single-stage detectors tend to be less accurate than two-stage detectors but are significantly faster."
  },
  {
    "objectID": "computer_vision/00_Interesting_links_to_read.html",
    "href": "computer_vision/00_Interesting_links_to_read.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Interesting Links\nReceptive Field\nskip connections\nDice loss - segmentation\ncomputer vision Generative Learning GAN Medical NLP Reinforcement learning Unsupervised learning"
  },
  {
    "objectID": "computer_vision/image_segmentation/02_architectures.html",
    "href": "computer_vision/image_segmentation/02_architectures.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "This is a typical encoder-decoder architecture.\nEncoder compresses the information\nDecoder will upsample and use skip connections to come up with orginial image\nThis image is used for segmentation\n\n FCN research paper\n\n\n\n Unet research paper\n\n\n\n  Research paper\n\n\n\n\n\n\nDeeplab\n\n\n\n\n\n  Research paper Conditional Random Fields"
  },
  {
    "objectID": "computer_vision/image_segmentation/01_intro.html",
    "href": "computer_vision/image_segmentation/01_intro.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Different types of segmentation\n\nSemantic segmentation (every pixel is classified)\nInstance segmentation (object level classification)\nPanoptic segmentation (combination of pixel and instance)\n\n\n\n\n\nDifferent segmentation\n\n\n\nAfter convolutions and pooling we will have a lot of feature maps in reduced dimensions. With the help of compressed latent space representation we can do lot of things like classification, upsampling etc \nAuto-encoders\n\n\n\n\nUpsampling from Latent space representation to the original size of the image\nUpsampling happens in decoder\nUpsampling is used for image generation, enhancement, mapping and more\n\nNearest Neighor\nBed of Nails\nBilinear upsampling\nTransposed convolutions\nMax unpooling\n\n\n\n\n\n\n\nNearest Neighbors Upsampling\n\n\n\n\n\n\n\n\nBed of nails\n\n\n\n\n\n\nMost popular\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nnearest_neighbour_out = F.interpolate(input, scale_factor=2, mode='nearest')\n\nbilinear_interp_out = F.interpolate(input, scale_factor=2, mode='bilinear', align_corners=False)\n\n\n\n\nWhen we are decompressing the image, it is difficult for the network to come up with original image size. Lot of information is lost in compression. To help in the process, skip connections exist between encoder and decoder.\n\n\n\n\nskip connections\n\n\n\nThere are two types of skip connections: Additive and concatenating\nResNet is addition\nDenseNet is concatenation\n\n\n\n\nAddition Vs Concat\n\n\nIn segmentation, skip connections are used to pass features from the encoder path to the decoder path in order to recover spatial information lost during downsampling\n\n\n\nskip connection - Gradient Descent\n\n\n * 1x1 convolution is to reduce the number of filters (not sure how it happens)\n\n\n\n1x1 convolutions\n\n\n\n\n\nThe metric used is Intersection over union (i.e Jaccard similarity)\nThe loss function is Focal loss (This is weighted cross entropy loss in addition to ‘gamma’ which will take care of class imbalance)\nDice loss is also used\n\n\n\n\nDifferent types of loss functions"
  },
  {
    "objectID": "computer_vision/applications/surveillance_with_dl.html",
    "href": "computer_vision/applications/surveillance_with_dl.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Video Feed\nQuality is often lowered to maximize storage Scalalbe system should be able to intrepret low quality images Training should happen on low quality images\n\n\nProcessing power\nProcessing on a centralized server Processing on the edge - TensorRT"
  },
  {
    "objectID": "computer_vision/image_classification/satellite_images_classification.html",
    "href": "computer_vision/image_classification/satellite_images_classification.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Images Classification Implementation\n\nImport the required libraries\n\nimport torch\nimport argparse\nimport torch.nn as nn\nimport torch.optim as optim\nimport argparse\nimport cv2\nfrom matplotlib import pyplot as plt\n\n\nimport warnings\nwarnings.filterwarnings(action='ignore')\n\n\nfrom model import build_model\nfrom utils import save_model, save_plots\nfrom datasets import train_loader, valid_loader, dataset\nfrom tqdm.notebook import tqdm\n\nClasses: ['cloudy', 'desert', 'green_area', 'water']\nTotal number of images: 5631\nTotal training images: 4505\nTotal valid_images: 1126\n\n\n\n\nLoad the weights for Reset Model\n\nlr = 0.001\nepochs = 20\ndevice = ('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"computation device: {device}\\n\")\n\ncomputation device: cuda\n\n\n\n\nmodel = build_model(\n    pretrained=True, fine_tune=False, num_classes=len(dataset.classes)).to(device)\n   \n# total parameters and trainable parameters\ntotal_params = sum(p.numel() for p in model.parameters())\nprint(f\"{total_params:,} total parameters.\")\n\ntotal_trainable_params = sum(\n    p.numel() for p in model.parameters() if p.requires_grad)\nprint(f\"{total_trainable_params:,} training parameters.\\n\")\n\n[INFO]: Loading pre-trained weights\n[INFO]: Freezing hidden layers...\n21,286,724 total parameters.\n2,052 training parameters.\n\n\n\n\n# optimizer\noptimizer = optim.Adam(model.parameters(), lr=lr)\n\n# loss function\ncriterion = nn.CrossEntropyLoss()\n\n\n\nTraining and Validation Functions\n\ndef train(model, trainloader, optimizer, criterion):\n    model.train()\n    print('Training')\n    train_running_loss = 0.0\n    train_running_correct = 0\n    counter = 0\n    for i, data in tqdm(enumerate(trainloader), total=len(trainloader)):\n        counter += 1\n        image, labels = data\n        image = image.to(device)\n        labels = labels.to(device)\n        optimizer.zero_grad()\n        # forward pass\n        outputs = model(image)\n        # calculate the loss\n        loss = criterion(outputs, labels)\n        train_running_loss += loss.item()\n        # calculate the accuracy\n        _, preds = torch.max(outputs.data, 1)\n        train_running_correct += (preds == labels).sum().item()\n        # backpropagation\n        loss.backward()\n        # update the optimizer parameters\n        optimizer.step()\n    \n    # loss and accuracy for the complete epoch\n    epoch_loss = train_running_loss / counter\n    epoch_acc = 100. * (train_running_correct / len(trainloader.dataset))\n    return epoch_loss, epoch_acc\n\n\ndef validate(model, testloader, criterion, class_names):\n    model.eval()\n    print('Validation')\n    valid_running_loss = 0.0\n    valid_running_correct = 0\n    counter = 0\n    \n    # we need two lists to keep track of class-wise accuracy\n    class_correct = list(0. for i in range(len(class_names)))\n    class_total = list(0. for i in range(len(class_names)))\n    \n    with torch.no_grad():\n        for i, data in tqdm(enumerate(testloader), total=len(testloader)):\n            counter += 1\n            \n            image, labels = data\n            image = image.to(device)\n            labels = labels.to(device)\n            # forward pass\n            outputs = model(image)\n            # calculate the loss\n            loss = criterion(outputs, labels)\n            valid_running_loss += loss.item()\n            # calculate the accuracy\n            _, preds = torch.max(outputs.data, 1)\n            valid_running_correct += (preds == labels).sum().item()\n            \n            # calculate the accuracy for each class\n            correct  = (preds == labels).squeeze()\n            for i in range(len(preds)):\n                label = labels[i]\n                class_correct[label] += correct[i].item()\n                class_total[label] += 1\n        \n    # loss and accuracy for the complete epoch\n    epoch_loss = valid_running_loss / counter\n    epoch_acc = 100. * (valid_running_correct / len(testloader.dataset))\n    \n    # print the accuracy for each class after every epoch\n    print('\\n')\n    for i in range(len(class_names)):\n        print(f\"Accuracy of class {class_names[i]}: {100*class_correct[i]/class_total[i]}\")\n    print('\\n')\n        \n    return epoch_loss, epoch_acc\n\n\n\nTrain for 20 Epochs\n\n# lists to keep track of losses and accuracies\ntrain_loss, valid_loss = [], []\ntrain_acc, valid_acc = [], []\n# start the training\nfor epoch in range(epochs):\n    #print(f\"[INFO]: Epoch {epoch+1} of {epochs}\")\n    train_epoch_loss, train_epoch_acc = train(model, train_loader, \n                                              optimizer, criterion)\n    valid_epoch_loss, valid_epoch_acc = validate(model, valid_loader,  \n                                                 criterion, dataset.classes)\n    train_loss.append(train_epoch_loss)\n    valid_loss.append(valid_epoch_loss)\n    train_acc.append(train_epoch_acc)\n    valid_acc.append(valid_epoch_acc)\n    print(f\"Training loss: {train_epoch_loss:.3f}, training acc: {train_epoch_acc:.3f}\")\n    print(f\"Validation loss: {valid_epoch_loss:.3f}, validation acc: {valid_epoch_acc:.3f}\")\n    print('-'*50)\n# save the trained model weights\nsave_model(epochs, model, optimizer, criterion)\n# save the loss and accuracy plots\nsave_plots(train_acc, valid_acc, train_loss, valid_loss)\nprint('TRAINING COMPLETE')\n\nTraining\n\n\n\n\n\nValidation\n\n\n\n\n\n\n\nAccuracy of class cloudy: 99.05660377358491\nAccuracy of class desert: 95.57522123893806\nAccuracy of class green_area: 96.53979238754326\nAccuracy of class water: 93.8566552901024\n\n\nTraining loss: 0.057, training acc: 98.180\nValidation loss: 0.142, validation acc: 96.359\n--------------------------------------------------\nTraining\n\n\n\n\n\nValidation\n\n\n\n\n\n\n\nAccuracy of class cloudy: 99.37106918238993\nAccuracy of class desert: 92.92035398230088\nAccuracy of class green_area: 97.57785467128028\nAccuracy of class water: 83.2764505119454\n\n\nTraining loss: 0.049, training acc: 98.557\nValidation loss: 0.208, validation acc: 93.428\n--------------------------------------------------\nTraining\n\n\n\n\n\nValidation\n\n\n\n\n\n\n\nAccuracy of class cloudy: 99.05660377358491\nAccuracy of class desert: 96.90265486725664\nAccuracy of class green_area: 96.88581314878893\nAccuracy of class water: 84.98293515358361\n\n\nTraining loss: 0.048, training acc: 98.424\nValidation loss: 0.189, validation acc: 94.405\n--------------------------------------------------\nTraining\n\n\n\n\n\nValidation\n\n\n\n\n\n\n\nAccuracy of class cloudy: 99.05660377358491\nAccuracy of class desert: 98.23008849557522\nAccuracy of class green_area: 95.50173010380622\nAccuracy of class water: 91.46757679180887\n\n\nTraining loss: 0.047, training acc: 98.690\nValidation loss: 0.162, validation acc: 96.004\n--------------------------------------------------\nTraining\n\n\n\n\n\nValidation\n\n\n\n\n\n\n\nAccuracy of class cloudy: 98.74213836477988\nAccuracy of class desert: 97.78761061946902\nAccuracy of class green_area: 89.96539792387543\nAccuracy of class water: 96.24573378839591\n\n\nTraining loss: 0.060, training acc: 98.091\nValidation loss: 0.161, validation acc: 95.648\n--------------------------------------------------\nTraining\n\n\n\n\n\nValidation\n\n\n\n\n\n\n\nAccuracy of class cloudy: 98.11320754716981\nAccuracy of class desert: 98.67256637168141\nAccuracy of class green_area: 96.88581314878893\nAccuracy of class water: 88.39590443686006\n\n\nTraining loss: 0.049, training acc: 98.313\nValidation loss: 0.159, validation acc: 95.382\n--------------------------------------------------\nTraining\n\n\n\n\n\nValidation\n\n\n\n\n\n\n\nAccuracy of class cloudy: 99.05660377358491\nAccuracy of class desert: 97.34513274336283\nAccuracy of class green_area: 97.92387543252595\nAccuracy of class water: 91.12627986348123\n\n\nTraining loss: 0.049, training acc: 98.402\nValidation loss: 0.143, validation acc: 96.359\n--------------------------------------------------\nTraining\n\n\n\n\n\nValidation\n\n\n\n\n\n\n\nAccuracy of class cloudy: 99.05660377358491\nAccuracy of class desert: 97.78761061946902\nAccuracy of class green_area: 93.77162629757785\nAccuracy of class water: 93.8566552901024\n\n\nTraining loss: 0.040, training acc: 98.912\nValidation loss: 0.161, validation acc: 96.092\n--------------------------------------------------\nTraining\n\n\n\n\n\nValidation\n\n\n\n\n\n\n\nAccuracy of class cloudy: 99.05660377358491\nAccuracy of class desert: 92.47787610619469\nAccuracy of class green_area: 96.19377162629758\nAccuracy of class water: 91.80887372013652\n\n\nTraining loss: 0.039, training acc: 98.690\nValidation loss: 0.163, validation acc: 95.115\n--------------------------------------------------\nTraining\n\n\n\n\n\nValidation\n\n\n\n\n\n\n\nAccuracy of class cloudy: 99.05660377358491\nAccuracy of class desert: 96.90265486725664\nAccuracy of class green_area: 96.19377162629758\nAccuracy of class water: 89.419795221843\n\n\nTraining loss: 0.044, training acc: 98.468\nValidation loss: 0.162, validation acc: 95.382\n--------------------------------------------------\nTraining\n\n\n\n\n\nValidation\n\n\n\n\n\n\n\nAccuracy of class cloudy: 97.48427672955975\nAccuracy of class desert: 98.67256637168141\nAccuracy of class green_area: 87.5432525951557\nAccuracy of class water: 93.51535836177474\n\n\nTraining loss: 0.043, training acc: 98.513\nValidation loss: 0.178, validation acc: 94.139\n--------------------------------------------------\nTraining\n\n\n\n\n\nValidation\n\n\n\n\n\n\n\nAccuracy of class cloudy: 98.42767295597484\nAccuracy of class desert: 98.67256637168141\nAccuracy of class green_area: 88.23529411764706\nAccuracy of class water: 96.24573378839591\n\n\nTraining loss: 0.041, training acc: 98.602\nValidation loss: 0.162, validation acc: 95.293\n--------------------------------------------------\nTraining\n\n\n\n\n\nValidation\n\n\n\n\n\n\n\nAccuracy of class cloudy: 99.05660377358491\nAccuracy of class desert: 98.23008849557522\nAccuracy of class green_area: 86.85121107266436\nAccuracy of class water: 92.83276450511946\n\n\nTraining loss: 0.044, training acc: 98.513\nValidation loss: 0.183, validation acc: 94.139\n--------------------------------------------------\nTraining\n\n\n\n\n\nValidation\n\n\n\n\n\n\n\nAccuracy of class cloudy: 99.05660377358491\nAccuracy of class desert: 97.34513274336283\nAccuracy of class green_area: 94.80968858131487\nAccuracy of class water: 92.15017064846417\n\n\nTraining loss: 0.042, training acc: 98.668\nValidation loss: 0.157, validation acc: 95.826\n--------------------------------------------------\nTraining\n\n\n\n\n\nValidation\n\n\n\n\n\n\n\nAccuracy of class cloudy: 99.05660377358491\nAccuracy of class desert: 98.67256637168141\nAccuracy of class green_area: 92.38754325259515\nAccuracy of class water: 95.56313993174061\n\n\nTraining loss: 0.044, training acc: 98.446\nValidation loss: 0.143, validation acc: 96.359\n--------------------------------------------------\nTraining\n\n\n\n\n\nValidation\n\n\n\n\n\n\n\nAccuracy of class cloudy: 99.05660377358491\nAccuracy of class desert: 97.78761061946902\nAccuracy of class green_area: 96.53979238754326\nAccuracy of class water: 93.51535836177474\n\n\nTraining loss: 0.039, training acc: 98.468\nValidation loss: 0.125, validation acc: 96.714\n--------------------------------------------------\nTraining\n\n\n\n\n\nValidation\n\n\n\n\n\n\n\nAccuracy of class cloudy: 99.05660377358491\nAccuracy of class desert: 96.90265486725664\nAccuracy of class green_area: 95.15570934256056\nAccuracy of class water: 92.83276450511946\n\n\nTraining loss: 0.038, training acc: 98.713\nValidation loss: 0.149, validation acc: 96.004\n--------------------------------------------------\nTraining\n\n\n\n\n\nValidation\n\n\n\n\n\n\n\nAccuracy of class cloudy: 98.74213836477988\nAccuracy of class desert: 98.67256637168141\nAccuracy of class green_area: 89.27335640138408\nAccuracy of class water: 94.53924914675768\n\n\nTraining loss: 0.041, training acc: 98.713\nValidation loss: 0.154, validation acc: 95.204\n--------------------------------------------------\nTraining\n\n\n\n\n\nValidation\n\n\n\n\n\n\n\nAccuracy of class cloudy: 99.37106918238993\nAccuracy of class desert: 97.78761061946902\nAccuracy of class green_area: 96.19377162629758\nAccuracy of class water: 88.73720136518772\n\n\nTraining loss: 0.040, training acc: 98.602\nValidation loss: 0.163, validation acc: 95.471\n--------------------------------------------------\nTraining\n\n\n\n\n\nValidation\n\n\n\n\n\n\n\nAccuracy of class cloudy: 99.05660377358491\nAccuracy of class desert: 97.78761061946902\nAccuracy of class green_area: 97.2318339100346\nAccuracy of class water: 91.46757679180887\n\n\nTraining loss: 0.042, training acc: 98.557\nValidation loss: 0.133, validation acc: 96.359\n--------------------------------------------------\nTRAINING COMPLETE\n\n\n\n\n\n\n\n\n\n\nInference\n\nimport torch\nimport cv2\nimport torchvision.transforms as transforms\nfrom model import build_model\n\n\ndevice = 'cpu'\n\n\n# list containing all the labels\nlabels = ['cloudy', 'desert', 'green_area', 'water']\n# initialize the model and load the trained weights\nmodel = build_model(\n    pretrained=False, fine_tune=False, num_classes=4\n).to(device)\n\nprint('[INFO]: Loading custom-trained weights...')\ncheckpoint = torch.load('outputs/model.pth', map_location=device)\nmodel.load_state_dict(checkpoint['model_state_dict'])\nmodel.eval()\n\n# define preprocess transforms\ntransform = transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.Resize(224),\n    transforms.ToTensor(),\n    transforms.Normalize(\n        mean=[0.485, 0.456, 0.406],\n        std=[0.229, 0.224, 0.225]\n    )\n]) \n\n[INFO]: Not loading pre-trained weights\n[INFO]: Freezing hidden layers...\n[INFO]: Loading custom-trained weights...\n\n\n\ndef inference(input):\n# read and preprocess the image\n    image = cv2.imread(input)\n    # get the ground truth class\n    gt_class = input.split('/')[-1].split('.')[0]\n    orig_image = image.copy()\n    # convert to RGB format\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    image = transform(image)\n    # add batch dimension\n    image = torch.unsqueeze(image, 0)\n    with torch.no_grad():\n        outputs = model(image.to(device))\n    output_label = torch.topk(outputs, 1)\n    pred_class = labels[int(output_label.indices)]\n    cv2.putText(orig_image, \n        f\"GT: {gt_class}\",\n        (10, 25),\n        cv2.FONT_HERSHEY_SIMPLEX, \n        1, (0, 255, 0), 2, cv2.LINE_AA\n    )\n    cv2.putText(orig_image, \n        f\"Pred: {pred_class}\",\n        (10, 55),\n        cv2.FONT_HERSHEY_SIMPLEX, \n        1, (0, 0, 255), 2, cv2.LINE_AA\n    )\n    print(f\"GT: {gt_class}, pred: {pred_class}\")\n    #image = cv2.imshow('Result', orig_image)\n    rgb_image = cv2.cvtColor(orig_image,cv2.COLOR_BGR2RGB)\n    fig = plt.figure()\n    plt.axis('off')\n    plt.grid(b=None)\n    plt.imshow(rgb_image)\n    cv2.imwrite(f\"outputs/{gt_class}.png\",\n        orig_image)\n\n\ninference(input='input/test_data/cloudy.jpeg')\n\nGT: cloudy, pred: cloudy\n\n\n\n\n\n\ninference(input='input/test_data/desert.jpeg')\n\nGT: desert, pred: desert"
  },
  {
    "objectID": "computer_vision/image_classification/01_convolution.html",
    "href": "computer_vision/image_classification/01_convolution.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Architecture for Image Classification\n\nA convolution is a NN way to extract and learn a feature through matrix multiplication\nConvolution filters are used to find features such as edges and corners\nThese filters are learned through backpropagation\n\n\nFiltering\n\nConvolution is a mathematical operation on two functions (f and g) that produces a third function (f * g) expressing how the shape of one is modified by the other.\nA filter (or kernel) is defined and is applied to the image.\nThe region the filter is being applied and is called the receptive field.\nAn element-wise multiplication is done between the region and filter and adds everything up\nDoing a convolution produces an image with a reduced size. The bigger the filter, the smaller the resulting image.\nEvery filter will have as many channels as the image it is convolving. convolving a three-channel filter over a three-channel image still produces a single value.\nWe can have more than one filter. \n\n\n\n\nConvolving three-channel Image\n\n\n\n\n\nImage size after convolution\n\n\n\n\n\nConvolutions with square filter\n\n\nThe actual filter, that is, the square matrix used to perform element-wise multiplication is learned using backpropogation\n\nEven if we have only one channel as input, we can have many channels as output. we can also force a convolutional module to use a particular filter by setting its weights\nThe size of the movement, in pixels, is called a stride. When doing the stride, the filter should not move out of the image (big no-no). The bigger the stride, the smaller the resulting image\n\n\n\n\nShape after a convolution with stride\n\n\n\nwe can use padding if we would like to keep the original image size. we can also add asymmetric padding.\n\n\n\nDifferent Padding modes\n\n\n\n\n\n\nImage size after padding\n\n\n\n\n\nCalculating size of feature map after convolution\n\n\n\n\nPooling\n\n\n\n\nMax pooling\n\n\nThe bigger the pooling kernel, the smaller the resulting image\nA pooling kernel of two-by-two results in an image whose dimensions are half of the original. A pooling kernel of threeby-three makes the resulting image one third the size of the original, and so on.\nThe pooling kernel should not go out of the image\nCommon pooling operations - Max pooling, Average pooling,\nNormally the stride will be equal to the dimensions of the square filter. We can also consider other strides as well. In this case there will be overalaps and the pooling works like strides in the convolution layer\nIt is a technique to downsample the output while keeping the most relevant information\n\n\n\nTypical Architecture\n\nconvolution\nActivation function\npooling\nThe number of channels/filters produced by each block is increased as more blocks are added\nImage gets flattened\n\n\n\n\nconvolution architecture\n\n\n\n\nBatch Norm\n\nBatch normalization is the process of normalizing each layer’s inputs by using the mean and variance of the values in the current mini-batch - Benefits in faster convergence with higher learning rates"
  },
  {
    "objectID": "computer_vision/image_classification/classification.html",
    "href": "computer_vision/image_classification/classification.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "classification with pytorch - Functions to use"
  },
  {
    "objectID": "spatial_data_processing/visualization.html",
    "href": "spatial_data_processing/visualization.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Data Visualization using Folium\n\nimport folium\n\n\nfrom pyproj import crs\nimport geopandas as gpd\nimport matplotlib.pyplot as plt\n\n\nCreate a simple Interactive Map\n\nm = folium.Map(location=[20.59,78.96],zoom_start=5,control_scale=True)\n\n\nm\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\n\n\nChange the Style of the Map\n\nm = folium.Map(\n    location = [20.59,78.96],\n    tiles = \"Stamen Toner\",\n    zoom_start = 5,\n    control_scale = True,\n    prefer_canvas = True\n)\n\nm\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\n\n\nAdding layers to the Map - Pin location\n\n#18.5786832,73.7666697\nm = folium.Map(location=[20.59,78.96],\n                zoom_start=5,control_scale=True)\nfolium.Marker(\n    location = [18.5786832,73.7666697],\n    popup='Sai Eshnaya Apartments',\n    icon = folium.Icon(color='green',icon='ok-sign')\n).add_to(m)\n\nm\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\n\n\nMark good resedential areas in Pune\n\npoints_fp = '../data/addresses.shp'\npoints = gpd.read_file(points_fp)\n\n\npoints.head()\n\n\n\n\n\n  \n    \n      \n      id\n      addr\n      geometry\n    \n  \n  \n    \n      0\n      1000\n      Boat Club Road, 411001, Pune, Maharastra\n      POINT (73.87826 18.53937)\n    \n    \n      1\n      1001\n      Koregaon, 415501, Pune, Maharastra\n      POINT (73.89299 18.53772)\n    \n    \n      2\n      1002\n      Kothrud, 411038, Pune, Maharastra\n      POINT (73.80767 18.50389)\n    \n    \n      3\n      1003\n      Balewadi, 411045, Pune, Maharastra\n      POINT (73.76912 18.57767)\n    \n    \n      4\n      1004\n      Baner, 411047, Pune, Maharastra\n      POINT (73.77686 18.56424)\n    \n  \n\n\n\n\n\npoints_gjson = folium.features.GeoJson(points, name='Good Residential Areas')\n\n\nm = folium.Map(location=[18.5786832,73.7666697], tiles=\"cartodbpositron\",\n                zoom_start=8,\n                control_scale=True)\npoints_gjson.add_to(m)\nfolium.LayerControl().add_to(m)\nm\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\n\n\nCreate a Heatmap of the locations\n\npoints[\"x\"] = points[\"geometry\"].apply(lambda geom: geom.x)\npoints[\"y\"] = points[\"geometry\"].apply(lambda geom: geom.y)\n\n# Create a list of coordinate pairs\nlocations = list(zip(points[\"y\"], points[\"x\"]))\n\n\nfrom folium.plugins import HeatMap\n\n# Create a Map instance\nm = folium.Map(\n    location=[18.5786832,73.7666697], tiles=\"stamentoner\", zoom_start=10, control_scale=True\n)\n\n# Add heatmap to map instance\n# Available parameters: HeatMap(data, name=None, min_opacity=0.5, max_zoom=18, max_val=1.0, radius=25, blur=15, gradient=None, overlay=True, control=True, show=True)\nHeatMap(locations).add_to(m)\n\n# Alternative syntax:\n# m.add_child(HeatMap(points_array, radius=15))\n\n# Show map\nm\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\n\n\nCreate a Clustered point Map\n\nfrom folium.plugins import MarkerCluster\n\n\n# Create a Map instance\nm = folium.Map(\n    location=[18.5786832,73.7666697], tiles=\"cartodbpositron\", zoom_start=12, control_scale=True\n)\n\n\n# Get x and y coordinates for each point\npoints[\"x\"] = points[\"geometry\"].apply(lambda geom: geom.x)\npoints[\"y\"] = points[\"geometry\"].apply(lambda geom: geom.y)\n\n# Create a list of coordinate pairs\nlocations = list(zip(points[\"y\"], points[\"x\"]))\n\n\n# Create a folium marker cluster\nmarker_cluster = MarkerCluster(locations)\n\n# Add marker cluster to map\nmarker_cluster.add_to(m)\n\n# Show map\nm\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\n\n\nCreate a Choropleth Map\n\nimport geopandas as gpd\nfrom pyproj import CRS\nimport requests\nimport geojson\n\n# Specify the url for web feature service\nurl = \"https://kartta.hsy.fi/geoserver/wfs\"\n\n# Specify parameters (read data in json format).\n# Available feature types in this particular data source: http://geo.stat.fi/geoserver/vaestoruutu/wfs?service=wfs&version=2.0.0&request=describeFeatureType\nparams = dict(\n    service=\"WFS\",\n    version=\"2.0.0\",\n    request=\"GetFeature\",\n    typeName=\"asuminen_ja_maankaytto:Vaestotietoruudukko_2018\",\n    outputFormat=\"json\",\n)\n\n# Fetch data from WFS using requests\nr = requests.get(url, params=params)\n\n# Create GeoDataFrame from geojson\ndata = gpd.GeoDataFrame.from_features(geojson.loads(r.content))\n\n# Check the data\ndata.head()\n\n\n\n\n\n  \n    \n      \n      geometry\n      index\n      asukkaita\n      asvaljyys\n      ika0_9\n      ika10_19\n      ika20_29\n      ika30_39\n      ika40_49\n      ika50_59\n      ika60_69\n      ika70_79\n      ika_yli80\n    \n  \n  \n    \n      0\n      POLYGON ((25472499.995 6689749.005, 25472499.9...\n      688\n      9\n      28.0\n      99\n      99\n      99\n      99\n      99\n      99\n      99\n      99\n      99\n    \n    \n      1\n      POLYGON ((25472499.995 6685998.998, 25472499.9...\n      703\n      5\n      51.0\n      99\n      99\n      99\n      99\n      99\n      99\n      99\n      99\n      99\n    \n    \n      2\n      POLYGON ((25472499.995 6684249.004, 25472499.9...\n      710\n      8\n      44.0\n      99\n      99\n      99\n      99\n      99\n      99\n      99\n      99\n      99\n    \n    \n      3\n      POLYGON ((25472499.995 6683999.005, 25472499.9...\n      711\n      5\n      90.0\n      99\n      99\n      99\n      99\n      99\n      99\n      99\n      99\n      99\n    \n    \n      4\n      POLYGON ((25472499.995 6682998.998, 25472499.9...\n      715\n      11\n      41.0\n      99\n      99\n      99\n      99\n      99\n      99\n      99\n      99\n      99\n    \n  \n\n\n\n\n\nfrom pyproj import CRS\n\n# Define crs\ndata.crs = CRS.from_epsg(3879)\n\n\n# Re-project to WGS84\ndata = data.to_crs(epsg=4326)\n\n# Check layer crs definition\nprint(data.crs)\n\nEPSG:4326\n\n\n\n# Change the name of a column\ndata = data.rename(columns={\"asukkaita\": \"pop18\"})\n\n\ndata[\"geoid\"] = data.index.astype(str)\n\n\n# Select only needed columns\ndata = data[[\"geoid\", \"pop18\", \"geometry\"]]\n\n# Convert to geojson (not needed for the simple coropleth map!)\n# pop_json = data.to_json()\n\n# check data\ndata.head()\n\n\n\n\n\n  \n    \n      \n      geoid\n      pop18\n      geometry\n    \n  \n  \n    \n      0\n      0\n      9\n      POLYGON ((24.50236 60.31928, 24.50233 60.32152...\n    \n    \n      1\n      1\n      5\n      POLYGON ((24.50287 60.28562, 24.50284 60.28787...\n    \n    \n      2\n      2\n      8\n      POLYGON ((24.50311 60.26992, 24.50308 60.27216...\n    \n    \n      3\n      3\n      5\n      POLYGON ((24.50315 60.26767, 24.50311 60.26992...\n    \n    \n      4\n      4\n      11\n      POLYGON ((24.50328 60.25870, 24.50325 60.26094...\n    \n  \n\n\n\n\n\nm = folium.Map(\n    location=[60.25, 24.8], tiles=\"cartodbpositron\", zoom_start=10, control_scale=True\n)\n\n# Plot a choropleth map\n# Notice: 'geoid' column that we created earlier needs to be assigned always as the first column\nfolium.Choropleth(\n    geo_data=data,\n    name=\"Population in 2018\",\n    data=data,\n    columns=[\"geoid\", \"pop18\"],\n    key_on=\"feature.id\",\n    fill_color=\"YlOrRd\",\n    fill_opacity=0.7,\n    line_opacity=0.2,\n    line_color=\"white\",\n    line_weight=0,\n    highlight=False,\n    smooth_factor=1.0,\n    # threshold_scale=[100, 250, 500, 1000, 2000],\n    legend_name=\"Population in Helsinki\",\n).add_to(m)\n\n# Show map\nm\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\n\n\nCreate Choropleth Map with Interaction\n\n# Convert points to GeoJson\nfolium.features.GeoJson(\n    data,\n    name=\"Labels\",\n    style_function=lambda x: {\n        \"color\": \"transparent\",\n        \"fillColor\": \"transparent\",\n        \"weight\": 0,\n    },\n    tooltip=folium.features.GeoJsonTooltip(\n        fields=[\"pop18\"], aliases=[\"Population\"], labels=True, sticky=False\n    ),\n).add_to(m)\n\nm\n\n\nMake this Notebook Trusted to load map: File -> Trust Notebook"
  },
  {
    "objectID": "spatial_data_processing/plot_buildings_with_area.html",
    "href": "spatial_data_processing/plot_buildings_with_area.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Visualizing Buildings in a location along with its Area\n\nImport the required libraries\n\nimport osmnx as ox\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport geopandas as gpd\nimport plotly.express as px\nimport keplergl\nimport warnings\nwarnings.filterwarnings(action='ignore')\n\n\n\nPlot the map for Pune\n\npune = ox.geocode_to_gdf(\"Pune, India\")\npune.plot(edgecolor=\"0.2\")\nplt.title(\"Pune\")\n\nText(0.5, 1.0, 'Pune')\n\n\n\n\n\n\n\nPlot your location on the Map\n\nmy_location = pd.DataFrame(\n    {\"location\":[\"Baner\"],\n    \"Longitude\":[73.7747862],\n    \"Latitude\":[18.578686]}\n)\n\n\nmy_location\n\n\n\n\n\n  \n    \n      \n      location\n      Longitude\n      Latitude\n    \n  \n  \n    \n      0\n      Baner\n      73.774786\n      18.578686\n    \n  \n\n\n\n\n\nmy_location = gpd.GeoDataFrame(my_location,\n                    crs = \"EPSG:4326\",\n                    geometry=gpd.points_from_xy(my_location[\"Longitude\"],my_location[\"Latitude\"]))\n\n\nmy_location\n\n\n\n\n\n  \n    \n      \n      location\n      Longitude\n      Latitude\n      geometry\n    \n  \n  \n    \n      0\n      Baner\n      73.774786\n      18.578686\n      POINT (73.77479 18.57869)\n    \n  \n\n\n\n\n\nax = pune.plot(edgecolor=\"0.2\")\nmy_location.plot(ax=ax,markersize=60,edgecolor=\"0.2\",color='red')\nplt.title(\"My Location in Pune\")\n\nText(0.5, 1.0, 'My Location in Pune')\n\n\n\n\n\n\n\nGet the Bike Routes for your location\n\nbike_network = ox.graph_from_point(center_point=(18.5584546,73.7852182),dist=400,network_type='bike')\nbike_network\n\n<networkx.classes.multidigraph.MultiDiGraph at 0x7f3d9ae77f40>\n\n\n\nbike_network = (ox.graph_to_gdfs(bike_network, nodes=False)\n                  .reset_index(drop=True)\n                  .loc[:, [\"name\", \"length\", \"geometry\"]]\n               )\nbike_network\n\n\n\n\n\n  \n    \n      \n      name\n      length\n      geometry\n    \n  \n  \n    \n      0\n      Gopal Hari Deshmukh Marg\n      52.331\n      LINESTRING (73.78688 18.56143, 73.78638 18.56145)\n    \n    \n      1\n      NaN\n      127.050\n      LINESTRING (73.78688 18.56143, 73.78683 18.560...\n    \n    \n      2\n      Pancard Clubs Road\n      8.998\n      LINESTRING (73.78638 18.56153, 73.78638 18.56145)\n    \n    \n      3\n      Gopal Hari Deshmukh Marg\n      58.852\n      LINESTRING (73.78638 18.56153, 73.78689 18.561...\n    \n    \n      4\n      Pancard Clubs Road\n      8.998\n      LINESTRING (73.78638 18.56145, 73.78638 18.56153)\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      201\n      NaN\n      32.554\n      LINESTRING (73.78206 18.55943, 73.78175 18.55946)\n    \n    \n      202\n      NaN\n      47.738\n      LINESTRING (73.78206 18.55943, 73.78204 18.55986)\n    \n    \n      203\n      NaN\n      19.034\n      LINESTRING (73.78206 18.55943, 73.78224 18.55944)\n    \n    \n      204\n      NaN\n      32.554\n      LINESTRING (73.78175 18.55946, 73.78206 18.55943)\n    \n    \n      205\n      NaN\n      19.034\n      LINESTRING (73.78224 18.55944, 73.78206 18.55943)\n    \n  \n\n206 rows × 3 columns\n\n\n\n\nbike_network.plot()\n\n<AxesSubplot: >\n\n\n\n\n\n\ntotal_length = bike_network[\"length\"].sum()\nprint(f\"Total bike lane length: {total_length / 1000:.0f}km\")\n\nTotal bike lane length: 16km\n\n\n\n\nPlot Bike routes on the Map\n\nimport contextily as ctx\n\nax = (bike_network.to_crs(\"EPSG:3857\")\n         .plot(figsize=(10, 8), legend=True,\n               edgecolor=\"0.2\", markersize=200, cmap=\"rainbow\")\n     )\nctx.add_basemap(ax, source=ctx.providers.OpenStreetMap.Mapnik)  # I'm using OSM as the source. See all provides with ctx.providers\nplt.axis(\"off\")\nplt.title(\"Baner\")\n\nText(0.5, 1.0, 'Baner')\n\n\n\n\n\n\n\nGet the building details in your area\n\ntags = {'building':True}\n\n\nbaner_buildings = ox.geometries_from_point(center_point=(18.5584546,73.7852182),dist=400,tags=tags)\n\n\nbaner_buildings = baner_buildings.assign(label='Building Footprints').reset_index()\n\n\n(baner_buildings.head(10).T)\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n      9\n    \n  \n  \n    \n      element_type\n      node\n      way\n      way\n      way\n      way\n      way\n      way\n      way\n      way\n      way\n    \n    \n      osmid\n      1432130601\n      264286363\n      359568513\n      359568520\n      359568545\n      359568551\n      359568562\n      359684077\n      359684097\n      359684099\n    \n    \n      building\n      yes\n      yes\n      yes\n      yes\n      yes\n      yes\n      yes\n      yes\n      yes\n      yes\n    \n    \n      name\n      UBICS\n      Baneshwar Temple\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      geometry\n      POINT (73.7860015 18.5612441)\n      POLYGON ((73.7869388 18.5589314, 73.7869469 18...\n      POLYGON ((73.7889321 18.5605767, 73.7890453 18...\n      POLYGON ((73.7876847 18.56149, 73.7877554 18.5...\n      POLYGON ((73.78786 18.5613415, 73.7880387 18.5...\n      POLYGON ((73.7881551 18.559907, 73.7882733 18....\n      POLYGON ((73.7828324 18.5618882, 73.782974 18....\n      POLYGON ((73.7813494 18.5612504, 73.7814597 18...\n      POLYGON ((73.781994 18.5613356, 73.7821481 18....\n      POLYGON ((73.7814951 18.5606079, 73.7817434 18...\n    \n    \n      nodes\n      NaN\n      [2699768401, 2699768402, 2699768403, 269976840...\n      [3642483644, 3642483643, 3642483641, 364248364...\n      [3642483654, 3642483653, 3642483648, 364248365...\n      [3642483649, 3642483647, 3642483645, 364248364...\n      [3642483640, 3642483639, 3642483637, 364248363...\n      [3642483660, 3642483659, 3642483657, 364248365...\n      [3643589874, 3643589877, 3643589867, 364358986...\n      [3643589882, 3643589883, 3643589873, 364358987...\n      [3643589826, 3643589830, 3643589824, 364358981...\n    \n    \n      amenity\n      NaN\n      place_of_worship\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      religion\n      NaN\n      hindu\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      addr:city\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      addr:postcode\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      addr:street\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      ways\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      type\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      label\n      Building Footprints\n      Building Footprints\n      Building Footprints\n      Building Footprints\n      Building Footprints\n      Building Footprints\n      Building Footprints\n      Building Footprints\n      Building Footprints\n      Building Footprints\n    \n  \n\n\n\n\n\nbaner_buildings.name.fillna(value='not_known',inplace=True)\n\n\nbaner_buildings.shape\n\n(255, 14)\n\n\n\n\nVisualize the buildings on the map\n\nax = (baner_buildings.to_crs(\"EPSG:3857\")\n         .plot(figsize=(10, 12),column=\"name\",legend=True,\n               edgecolor=\"0.2\", markersize=200, cmap=\"rainbow\")\n     )\nctx.add_basemap(ax, source=ctx.providers.OpenStreetMap.Mapnik)  # I'm using OSM as the source. See all provides with ctx.providers\nplt.axis(\"off\")\nplt.title(\"Baner\")\n\nText(0.5, 1.0, 'Baner')\n\n\n\n\n\n\nbaner_buildings = baner_buildings.to_crs(epsg=3347)\nbaner_buildings = baner_buildings.assign(area=baner_buildings.area)\n\n\nbaner_buildings= baner_buildings[['geometry','area']]\n\n\n\nPlot the buildings on the Map along with Area\n\nbaner_map = keplergl.KeplerGl(height=500)\nbaner_map.add_data(data=baner_buildings.copy(), name=\"Building area\")\n#baner_map.add_data(data=baner_buildings.copy(), name=\"height\")\n#baner_map\n\nUser Guide: https://docs.kepler.gl/docs/keplergl-jupyter\n\n\n\nbaner_map.save_to_html(file_name='first_map.html')\n\nMap saved to first_map.html!\n\n\n\n%%html\n<iframe src=\"first_map.html\" width=\"80%\" height=\"500\"></iframe>\n\n\n\n\n\n#from IPython.display import IFrame\n#IFrame(src='first_map.html', width=700, height=600)"
  },
  {
    "objectID": "spatial_data_processing/osm_processing copy.html",
    "href": "spatial_data_processing/osm_processing copy.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "OpenStreetMap\n\nIt is an crowd-sourced dataset\nIt contains data about streets, buildings, services, landuse etc.\nOSMnx is a package used to retrieve, construct, analyze and visualize street networks from OpenStreetMap and also retrieve data about points of interest such as restaurants, schools and lots of different kind of services.\nIt is also easy to conduct network routing based on walking, cycling or driving by combining OSMnx functionalities with a package called NetworkX\n\n\nimport osmnx as ox\nimport matplotlib.pyplot as plt\n\n\n#place_name = \"Togo, Africa\"\nplace_name = {18.5786832,73.7666697}\n\n\ngraph = ox.graph_from_point(place_name,dist=750,dist_type='bbox',network_type=\"drive\")\n\nEmptyOverpassResponse: There are no data elements in the response JSON\n\n\n\ntype(graph)\n\nnetworkx.classes.multidigraph.MultiDiGraph\n\n\n\nfig, ax = ox.plot_graph(graph)\n\n\n\n\n\nnodes, edges = ox.graph_to_gdfs(graph)\n\n\nnodes.head()\n\n\n\n\n\n  \n    \n      \n      y\n      x\n      street_count\n      geometry\n    \n    \n      osmid\n      \n      \n      \n      \n    \n  \n  \n    \n      652724178\n      18.574935\n      73.763832\n      4\n      POINT (73.76383 18.57493)\n    \n    \n      652724182\n      18.574981\n      73.764610\n      3\n      POINT (73.76461 18.57498)\n    \n    \n      763423062\n      18.571967\n      73.764768\n      3\n      POINT (73.76477 18.57197)\n    \n    \n      871491336\n      18.574828\n      73.770821\n      4\n      POINT (73.77082 18.57483)\n    \n    \n      1377773005\n      18.574932\n      73.763664\n      3\n      POINT (73.76366 18.57493)\n    \n  \n\n\n\n\n\nedges.head()\n\n\n\n\n\n  \n    \n      \n      \n      \n      osmid\n      oneway\n      highway\n      reversed\n      length\n      name\n      geometry\n      access\n      lanes\n      ref\n      maxspeed\n    \n    \n      u\n      v\n      key\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      652724178\n      7984103956\n      0\n      669050753\n      False\n      primary\n      False\n      13.448\n      NaN\n      LINESTRING (73.76383 18.57493, 73.76387 18.57482)\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      6262990166\n      0\n      669050753\n      False\n      primary\n      True\n      60.618\n      NaN\n      LINESTRING (73.76383 18.57493, 73.76364 18.57545)\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      6305085563\n      0\n      73533877\n      True\n      secondary\n      False\n      24.596\n      Moze College Road\n      LINESTRING (73.76383 18.57493, 73.76402 18.574...\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      652724182\n      4676484316\n      0\n      73533877\n      True\n      secondary\n      False\n      69.380\n      Moze College Road\n      LINESTRING (73.76461 18.57498, 73.76493 18.575...\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      7983557257\n      0\n      [250171874, 223437750]\n      False\n      residential\n      [False, True]\n      306.812\n      Echinus Court Road\n      LINESTRING (73.76461 18.57498, 73.76460 18.575...\n      yes\n      NaN\n      NaN\n      NaN\n    \n  \n\n\n\n\n\narea = ox.geocode_to_gdf(place_name)\n\nValueError: each query must be a dict or a string\n\n\n\ntype(area)\n\nNameError: name 'area' is not defined\n\n\n\narea\n\n\n\n\n\n  \n    \n      \n      geometry\n      bbox_north\n      bbox_south\n      bbox_east\n      bbox_west\n      place_id\n      osm_type\n      osm_id\n      lat\n      lon\n      display_name\n      class\n      type\n      importance\n    \n  \n  \n    \n      0\n      POLYGON ((74.91434 26.83563, 74.91534 26.83465...\n      27.860562\n      26.440461\n      76.285428\n      74.914344\n      298175590\n      relation\n      1950062\n      27.150677\n      75.747016\n      Jaipur, Rajasthan, India\n      boundary\n      administrative\n      0.671968\n    \n  \n\n\n\n\n\narea.plot()\n\n<AxesSubplot: >\n\n\n\n\n\n\ntags = {\"building\":True}\n\n\nbuildings = ox.geometries_from_place(place_name,tags)\n\nTypeError: query must be dict, string, or list of strings\n\n\n\nlen(buildings)\n\n32708\n\n\n\nbuildings.head()\n\n\n\n\n\n  \n    \n      \n      \n      nodes\n      building\n      geometry\n      area\n      barrier\n      currency:INR\n      layer\n      name\n      payment:cash\n      payment:fasttag\n      ...\n      name:tg\n      name:fr\n      motor_vehicle\n      architect\n      historic:civilization\n      outdoor_seating\n      location\n      parking\n      changing_table\n      toilets:disposal\n    \n    \n      element_type\n      osmid\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      way\n      383032803\n      [3862350688, 3862350689, 3862350690, 386235069...\n      residential\n      POLYGON ((75.82021 26.78322, 75.82020 26.78289...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      383032804\n      [3862350692, 3862350693, 3862350694, 386235069...\n      residential\n      POLYGON ((75.82045 26.78350, 75.82042 26.78332...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      383032805\n      [3862350696, 3862350697, 3862350698, 386235069...\n      residential\n      POLYGON ((75.82068 26.78322, 75.82084 26.78320...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      383032806\n      [3862350700, 3862350701, 3862350702, 386235070...\n      residential\n      POLYGON ((75.82069 26.78321, 75.82085 26.78319...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      383032807\n      [3862350704, 3862350705, 3862350706, 386235070...\n      residential\n      POLYGON ((75.82068 26.78297, 75.82079 26.78295...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n  \n\n5 rows × 132 columns\n\n\n\n\nbuildings.shape\n\n(32708, 132)\n\n\n\n# List key-value pairs for tags\ntags = {\"amenity\":\"restaurant\"}\n\n\n# Retrieve restaurants\nrestaurants = ox.geometries_from_place(place_name, tags)\n\n# How many restaurants do we have?\nlen(restaurants)\n\n127\n\n\n\nfig, ax = plt.subplots(figsize=(12, 8))\n\n# Plot the footprint\narea.plot(ax=ax, facecolor=\"black\")\n\n# Plot street edges\nedges.plot(ax=ax, linewidth=1, edgecolor=\"dimgray\",alpha=0.9)\n\n# Plot buildings\nbuildings.plot(ax=ax, facecolor=\"yellow\", markersize=20)\n\n# Plot restaurants\nrestaurants.plot(ax=ax, color=\"red\", markersize=20)\nplt.tight_layout()"
  },
  {
    "objectID": "spatial_data_processing/spatial_modelling.html",
    "href": "spatial_data_processing/spatial_modelling.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "import warnings\nimport keplergl\nimport numpy as np\nimport osmnx as ox\nimport pandas as pd\nimport geopandas as gpd\nimport plotly.express as px\nfrom skgstat import Variogram\nimport matplotlib.pyplot as plt\nfrom shapely.geometry import Point\nfrom pykrige.ok import OrdinaryKriging\nfrom scipy.interpolate import NearestNDInterpolator\nfrom tobler.area_weighted import area_interpolate\n# Custom functions\nfrom scripts.utils import pixel2poly\n# Plotting defaults\nplt.style.use('ggplot')\npx.defaults.height = 400; px.defaults.width = 620\nplt.rcParams.update({'font.size': 16, 'axes.labelweight': 'bold', 'figure.figsize': (6, 6), 'axes.grid': False})"
  },
  {
    "objectID": "spatial_data_processing/geographic_data_formats.html",
    "href": "spatial_data_processing/geographic_data_formats.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Geographic information can be represented in two forms vector or raster\nVector representations are constructed from points in geographical space which are connected to each other forming lines and polygons\nRasters are constructed from rectangular cells that form a uniform grid. Each cell of the grid contains a value representing some information such as elevation, temperature or presence / absence\nSpatio-temporal data incorporates time as additional dimension to the geographic dimension\n\n\n\n\nGeometric objects like points, lines and polygons are used\n\n\n\n\nVector data representation\n\n\n\n\n\nAttribute data is typically attached to the geometries that describe the given entity with various possible characteristics. Attributes are always linked to the geometries in one way or another.\n\n\n\n\n\nGDAL (Geospatial Data Abstraction Library) is a library for reading and writing raster and vector data formats which is used by most of the software libraries\n\n\n\n\nIntroduced by ESRI\nFilename extension is .shp\nIt is made of multiple separate files\nA valid shapefile dataset consist of:\n\n.shp - Feature geometries\n.shx - Positional index for the feature geometries\n.dbf - Attribute information\n.prj - Information about CRS of the dataset\n\n\n\n\n\n\nOpen format for encoding variety of geographic data structures along with their attribute information\nFilename extension is .geojson\nFile is not compressed\nAn example of GeoJSON data\n\n{\"type\": \"FeatureCollection\", \n    \"features\": [\n        {\"type\": \"Feature\", \"properties\": {\"id\": 75553155, \"timestamp\": 1494181812},\n        \"geometry\": {\"type\": \"MultiLineString\", \"coordinates\": [[[26.938, 60.520], [26.938, 60.520]], [[26.937, 60.521], [26.937, 60.521]], [[26.937, 60.521], [26.936, 60.522]]]}\n        }, \n        {\"type\": \"Feature\", \"properties\": {\"id\": 424099695, \"timestamp\": 1465572910}, \n        \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [[[26.935, 60.521], [26.935, 60.521], [26.935, 60.521], [26.935, 60.521], [26.935, 60.521]]]}\n        }\n    ]\n}\n\n\n\n\nIt uses SQLite database container to store the data\nFilename extension is .gpkg\n\n\n\n\n\nGeography Markup Language (GML) is an XML based format\nIt serves as a modeling language for geographic systems as well as an open interchange format for geographic transactions on the Internet\nFile extension is .gml\n\n\n\n\n\n\n\nData is represented as arrays of cells (called pixels) to represent real-world objects or continuous phenomena Ex- Digital photos with RGB channels\nWe can store other information to pixels, such as elevation or temperature data or more detailed spectral information that capture how the light reflects from objects on earth at different wave-lengths\n\n\n\n\nRaster data representation\n\n\n\n\n\nRaster Bit Depth\n\n\n\n\n\nxarray Data Format\n\n\n\n\n\n\n\nBased on TIFF format developed by NASA\nFile extension is .tif\n\n\n\n\n\nCloud Optimized GeoTIFF (COG)\nFile extension is .tif\n\n\n\n\n\nNetwork Common Data Form\nVariables stored in NetCDF are often measured multiple times per day over large (e.g. continental) areas\nThe file extension of NetCDF is .nc4\n\n\n\n\n\nUsed to transfer Raster files between applications\nThe file extension of ASCII Raster File is .asc\n\n\n\n\n\nThe ERDAS Imagine file format (IMG) is proprietary file format that was originally created by an image processing software company called ERDAS. The file can be accompanied with an .xml file which stores metadata information about the raster layer\nThe file extension of Imagine file format is .img\n\n\n\n\n\n\n\nNetworkx is used to store graph objects\npysal rely on sparse adjacency matrix"
  },
  {
    "objectID": "spatial_data_processing/osm_processing.html",
    "href": "spatial_data_processing/osm_processing.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "OpenStreetMap\n\nIt is an crowd-sourced dataset\nIt contains data about streets, buildings, services, landuse etc.\nOSMnx is a package used to retrieve, construct, analyze and visualize street networks from OpenStreetMap and also retrieve data about points of interest such as restaurants, schools and lots of different kind of services.\nIt is also easy to conduct network routing based on walking, cycling or driving by combining OSMnx functionalities with a package called NetworkX\n\n\nGet Street Network Graph for Tirupathi\n\nimport osmnx as ox\nimport matplotlib.pyplot as plt\n\n\nplace_name = \"Tirupathi, Andhra Pradesh, India\"\n\n\ngraph = ox.graph_from_place(place_name)\n\n\ntype(graph)\n\nnetworkx.classes.multidigraph.MultiDiGraph\n\n\n\nfig, ax = ox.plot_graph(graph)\n\n\n\n\n\nnodes, edges = ox.graph_to_gdfs(graph)\n\n\nnodes.head()\n\n\n\n\n\n  \n    \n      \n      y\n      x\n      street_count\n      geometry\n    \n    \n      osmid\n      \n      \n      \n      \n    \n  \n  \n    \n      3726004217\n      13.626082\n      79.391887\n      3\n      POINT (79.39189 13.62608)\n    \n    \n      3726082024\n      13.624080\n      79.381771\n      3\n      POINT (79.38177 13.62408)\n    \n    \n      3726082625\n      13.624315\n      79.383015\n      3\n      POINT (79.38302 13.62431)\n    \n    \n      3726082626\n      13.624330\n      79.383098\n      3\n      POINT (79.38310 13.62433)\n    \n    \n      3726082627\n      13.624499\n      79.393360\n      3\n      POINT (79.39336 13.62450)\n    \n  \n\n\n\n\n\nedges.head()\n\n\n\n\n\n  \n    \n      \n      \n      \n      osmid\n      highway\n      oneway\n      reversed\n      length\n      geometry\n      tunnel\n      bridge\n    \n    \n      u\n      v\n      key\n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      3726004217\n      3727759169\n      0\n      368755785\n      service\n      False\n      True\n      102.448\n      LINESTRING (79.39189 13.62608, 79.39096 13.62589)\n      NaN\n      NaN\n    \n    \n      3726082692\n      0\n      368765668\n      service\n      True\n      False\n      11.398\n      LINESTRING (79.39189 13.62608, 79.39189 13.626...\n      NaN\n      NaN\n    \n    \n      3726082024\n      3726082625\n      0\n      368755785\n      service\n      False\n      False\n      136.948\n      LINESTRING (79.38177 13.62408, 79.38302 13.62431)\n      NaN\n      NaN\n    \n    \n      3726082653\n      0\n      368765683\n      service\n      False\n      False\n      247.024\n      LINESTRING (79.38177 13.62408, 79.38146 13.625...\n      NaN\n      NaN\n    \n    \n      3726082625\n      3726082626\n      0\n      368755785\n      service\n      False\n      False\n      9.070\n      LINESTRING (79.38302 13.62431, 79.38310 13.62433)\n      NaN\n      NaN\n    \n  \n\n\n\n\n\narea = ox.geocode_to_gdf(place_name)\n\n\ntype(area)\n\ngeopandas.geodataframe.GeoDataFrame\n\n\n\narea\n\n\n\n\n\n  \n    \n      \n      geometry\n      bbox_north\n      bbox_south\n      bbox_east\n      bbox_west\n      place_id\n      osm_type\n      osm_id\n      lat\n      lon\n      display_name\n      class\n      type\n      importance\n    \n  \n  \n    \n      0\n      POLYGON ((79.37901 13.62928, 79.38167 13.62409...\n      13.634066\n      13.623569\n      79.39373\n      79.379014\n      191306545\n      way\n      369041142\n      13.626914\n      79.386643\n      Sri Venkateshwara Veterinary University, Tirup...\n      amenity\n      university\n      0.718072\n    \n  \n\n\n\n\n\narea.plot()\n\n<AxesSubplot: >\n\n\n\n\n\n\n\nGet Building information\n\ntags = {\"building\":True}\n\n\nbuildings = ox.geometries_from_place(place_name,tags)\n\n\nlen(buildings)\n\n103\n\n\n\nbuildings.head()\n\n\n\n\n\n  \n    \n      \n      \n      nodes\n      building\n      geometry\n      layer\n      name\n      ways\n      type\n    \n    \n      element_type\n      osmid\n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      way\n      368754138\n      [3725992120, 3725992118, 3725992325, 372599232...\n      yes\n      POLYGON ((79.38556 13.62655, 79.38560 13.62650...\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      368765639\n      [3727711369, 3727711372, 3727711373, 372771137...\n      yes\n      POLYGON ((79.38762 13.62865, 79.38763 13.62865...\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      368765640\n      [3726082631, 3726082641, 3726082638, 372608263...\n      yes\n      POLYGON ((79.38655 13.62537, 79.38662 13.62548...\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      368765641\n      [3726082662, 3726082681, 3726082697, 372608269...\n      yes\n      POLYGON ((79.38255 13.62598, 79.38244 13.62610...\n      1\n      Admin Office (Dr. Y.S.R. Bhavan)\n      NaN\n      NaN\n    \n    \n      368765644\n      [3726083018, 3726083020, 3726082991, 372608298...\n      yes\n      POLYGON ((79.38190 13.62782, 79.38196 13.62783...\n      NaN\n      NaN\n      NaN\n      NaN\n    \n  \n\n\n\n\n\nbuildings.shape\n\n(103, 7)\n\n\n\n# List key-value pairs for tags\ntags = {\"railway\":True}\n\n\n# Retrieve restaurants\nrailway = ox.geometries_from_place(place_name, tags)\n\n# How many restaurants do we have?\nlen(railway)\n\n1\n\n\n\nfig, ax = plt.subplots(figsize=(12, 8))\n\n# Plot the footprint\narea.plot(ax=ax, facecolor=\"black\")\n\n# Plot street edges\nedges.plot(ax=ax, linewidth=1, edgecolor=\"dimgray\")\n\n# Plot buildings\nbuildings.plot(ax=ax, facecolor=\"silver\", alpha=0.7)\n\n# Plot restaurants\nrailway.plot(ax=ax, color=\"red\", alpha=0.7, markersize=20)\nplt.tight_layout()\n\n\n\n\n\n\nGet Park Information\n\ntags = {\"leisure\": \"park\", \"landuse\": \"grass\"}\n\n\nparks = ox.geometries_from_place(place_name, tags)\nprint(\"Retrieved\", len(parks), \"objects\")\n\nRetrieved 5 objects\n\n\n\nparks.head(3)\n\n\n\n\n\n  \n    \n      \n      \n      nodes\n      landuse\n      geometry\n    \n    \n      element_type\n      osmid\n      \n      \n      \n    \n  \n  \n    \n      way\n      368765686\n      [3726082943, 3726082942, 3726082954, 372608295...\n      grass\n      POLYGON ((79.38144 13.62756, 79.38141 13.62755...\n    \n    \n      368765687\n      [3726082995, 3726082981, 3726082971, 372608299...\n      grass\n      POLYGON ((79.38149 13.62773, 79.38150 13.62770...\n    \n    \n      368765688\n      [3726083023, 3726083010, 3726082983, 372608300...\n      grass\n      POLYGON ((79.38150 13.62785, 79.38152 13.62778...\n    \n  \n\n\n\n\n\nparks.plot(color='green')\n\n<AxesSubplot: >\n\n\n\n\n\n\nfig, ax = plt.subplots(figsize=(12, 8))\n\n# Plot the footprint\narea.plot(ax=ax, facecolor=\"black\")\n\n# Plot the parks\nparks.plot(ax=ax, facecolor=\"green\")\n\n# Plot street edges\nedges.plot(ax=ax, linewidth=1, edgecolor=\"dimgray\")\n\n# Plot buildings\nbuildings.plot(ax=ax, facecolor=\"silver\", alpha=0.7)\n\n# Plot restaurants\nrailway.plot(ax=ax, color=\"red\", alpha=0.7, markersize=20)\nplt.tight_layout()"
  },
  {
    "objectID": "spatial_data_processing/spatial_analysis.html",
    "href": "spatial_data_processing/spatial_analysis.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "import geopandas as gpd\nfrom pathlib import Path\n\n\ninput_path = '/home/thulasiram/personal/going_deep_and_wide/togo/togo-targeting-replication/data/shapefiles/cantons.geojson'\ndata = gpd.read_file(input_path)\n\n\ntype(data)\n\ngeopandas.geodataframe.GeoDataFrame\n\n\n\ndata.head()\n\n\n\n\n\n  \n    \n      \n      canton\n      poverty\n      geometry\n    \n  \n  \n    \n      0\n      1\n      3.738084\n      MULTIPOLYGON (((0.75228 6.83786, 0.75137 6.840...\n    \n    \n      1\n      2\n      7.096286\n      MULTIPOLYGON (((0.69026 6.80602, 0.69627 6.806...\n    \n    \n      2\n      3\n      0.824586\n      MULTIPOLYGON (((0.63102 6.74430, 0.63295 6.747...\n    \n    \n      3\n      4\n      3.983729\n      MULTIPOLYGON (((0.67259 6.85123, 0.67714 6.849...\n    \n    \n      4\n      5\n      7.708810\n      MULTIPOLYGON (((0.75269 6.84116, 0.75137 6.840...\n    \n  \n\n\n\n\n\nprint(\"Number of rows\",len(data[\"canton\"]))\nprint(\"Number of classes\",data[\"canton\"].nunique())\n\nNumber of rows 387\nNumber of classes 387\n\n\n\n\n\n\ndata.plot()\n\n<AxesSubplot: >\n\n\n\n\n\nChecking the shape and area of the first Multipolygon in the data\n\ndata.at[0,\"geometry\"]\n\n\n\n\n\n# Calculating area with lat and long is wrong, to calculate \n# area correctly we need to change the co-ordinate reference system\nround(data.at[0,\"geometry\"].area,3)\n\n0.014\n\n\n\n\n\n\ndata.plot(\"poverty\",legend=True)\n\n<AxesSubplot: >\n\n\n\n\n\n\ndata.explore(\"poverty\",legend=True)\n\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\n\n\n\nGeocoding is the process of transforming place names or addresses into coordinates\n\n# Import necessary modules\nimport pandas as pd\nimport geopandas as gpd\nfrom shapely.geometry import Point\n\n\n# Filepath\nfp = '../data/addresses.txt'\n\n# Read the data\ndata = pd.read_csv(fp,sep=\";\")\n\n\nlen(data)\n\n5\n\n\n\ndata.head()\n\n\n\n\n\n  \n    \n      \n      id\n      addr\n    \n  \n  \n    \n      0\n      1000\n      Boat Club Road, 411001, Pune, Maharastra\n    \n    \n      1\n      1001\n      Koregaon, 415501, Pune, Maharastra\n    \n    \n      2\n      1002\n      Kothrud, 411038, Pune, Maharastra\n    \n    \n      3\n      1003\n      Balewadi, 411045, Pune, Maharastra\n    \n    \n      4\n      1004\n      Baner, 411047, Pune, Maharastra\n    \n  \n\n\n\n\n\n# Import the geocoding tool\nfrom geopandas.tools import geocode\n\n# Geocode addresses using Nominatim. Remember to provide a custom \"application name\" in the user_agent parameter!\ngeo = geocode(data[\"addr\"], provider=\"nominatim\", user_agent=\"autogis_xx\", timeout=4)\n\n\ngeo.head()\n\n\n\n\n\n  \n    \n      \n      geometry\n      address\n    \n  \n  \n    \n      0\n      POINT (73.87826 18.53937)\n      Boat Club Road, Pune City, Pune, Maharashtra, ...\n    \n    \n      1\n      POINT (73.89299 18.53772)\n      Koregaon Park, Suyojan Society, Ghorpuri, Pune...\n    \n    \n      2\n      POINT (73.80767 18.50389)\n      Kothrud, Pune City, Maharashtra, 411038, India\n    \n    \n      3\n      POINT (73.76912 18.57767)\n      Prakashgad Society, Balewadi, Perfect 10 Inter...\n    \n    \n      4\n      POINT (73.77686 18.56424)\n      Baner, Pune City, Maharashtra, 511045, India\n    \n  \n\n\n\n\n\n# Joining the original dataframe with geoencoded dataframe\njoin = geo.join(data)\njoin = join.drop(columns=['address'])\n\n\njoin.head()\n\n\n\n\n\n  \n    \n      \n      geometry\n      id\n      addr\n    \n  \n  \n    \n      0\n      POINT (73.87826 18.53937)\n      1000\n      Boat Club Road, 411001, Pune, Maharastra\n    \n    \n      1\n      POINT (73.89299 18.53772)\n      1001\n      Koregaon, 415501, Pune, Maharastra\n    \n    \n      2\n      POINT (73.80767 18.50389)\n      1002\n      Kothrud, 411038, Pune, Maharastra\n    \n    \n      3\n      POINT (73.76912 18.57767)\n      1003\n      Balewadi, 411045, Pune, Maharastra\n    \n    \n      4\n      POINT (73.77686 18.56424)\n      1004\n      Baner, 411047, Pune, Maharastra\n    \n  \n\n\n\n\n\n\n\n\nfrom shapely.geometry import box\n\nminx = 73.76\nminy = 18.537\nmaxx = 73.89\nmaxy = 18.56\ngeom = box(minx, miny, maxx, maxy)\nclipping_gdf = gpd.GeoDataFrame({\"geometry\": [geom]}, index=[0], crs=\"epsg:4326\")\n\n# Explore the extent on a map\nclipping_gdf.explore()\n\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\n\n# Write the data to shape file\noutfp = r\"../data/addresses.shp\"\njoin.to_file(outfp)"
  },
  {
    "objectID": "spatial_data_processing/crs.html",
    "href": "spatial_data_processing/crs.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Coordinate Reference Systems (CRS)\nA CRS tells python how coordinates are related to places on the Earth. A map projection (or a projected coordinate system) is a systematic transformation of the latitudes and longitudes into a plain surface where units are quite commonly represented as meters (instead of decimal degrees). This transformation is used to represent the three dimensional earth on a flat, two dimensional map.\nThere is no perfect projection and we should know the strength and weaknesses of projection systems and choose a projection system that best fits our purpose.\nWe can reproject the geometries from crs to another using to_crs() function from GeoPandas.\nWe can define the coordinate system in different formats using pyproj CRS\n\nImport and view the data\n\nimport geopandas as gpd\n\n\n# Read the data\nworld = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n\n\nworld.head(4)\n\n\n\n\n\n  \n    \n      \n      pop_est\n      continent\n      name\n      iso_a3\n      gdp_md_est\n      geometry\n    \n  \n  \n    \n      0\n      889953.0\n      Oceania\n      Fiji\n      FJI\n      5496\n      MULTIPOLYGON (((180.00000 -16.06713, 180.00000...\n    \n    \n      1\n      58005463.0\n      Africa\n      Tanzania\n      TZA\n      63177\n      POLYGON ((33.90371 -0.95000, 34.07262 -1.05982...\n    \n    \n      2\n      603253.0\n      Africa\n      W. Sahara\n      ESH\n      907\n      POLYGON ((-8.66559 27.65643, -8.66512 27.58948...\n    \n    \n      3\n      37589262.0\n      North America\n      Canada\n      CAN\n      1736425\n      MULTIPOLYGON (((-122.84000 49.00000, -122.9742...\n    \n  \n\n\n\n\n\n\nView the CRS of the data\n\n# Check the CRS of the data.\n# Lat Long data should have EPSG 4326 and WGS 84\nworld.crs\n\n<Geographic 2D CRS: EPSG:4326>\nName: WGS 84\nAxis Info [ellipsoidal]:\n- Lat[north]: Geodetic latitude (degree)\n- Lon[east]: Geodetic longitude (degree)\nArea of Use:\n- name: World.\n- bounds: (-180.0, -90.0, 180.0, 90.0)\nDatum: World Geodetic System 1984 ensemble\n- Ellipsoid: WGS 84\n- Prime Meridian: Greenwich\n\n\n\n\nChange the CRS and visualize the data\n\nax = world.plot()\nax.set_title(\"WGS84 (lat/lon)\")\nworld = world[(world.name != \"Antarctica\") & (world.name != \"Fr. S. Antarctic Lands\")]\n# Data in Mercator Projection\nworld = world.to_crs(\"EPSG:3395\")\nax = world.plot()\nax.set_title(\"Mercator\")\n\nText(0.5, 1.0, 'Mercator')\n\n\n\n\n\n\n\n\n\n\nOrthographic Projection\n\n# Orthographic projection\nfrom pyproj import CRS\n\n\n# Define an orthographic projection, from: http://www.statsmapsnpix.com/2019/09/globe-projections-and-insets-in-qgis.html\northo = CRS.from_proj4(\n    \"+proj=ortho +lat_0=60.00 +lon_0=23.0000 +x_0=0 +y_0=0 +a=6370997 +b=6370997 +units=m +no_defs\"\n)\n\n# Re-project and plot\nax = world.to_crs(ortho).plot()\n\n# Remove x and y axis\nax.axis(\"off\")\nax.set_title(\"Orthographic\")\n\nText(0.5, 1.0, 'Orthographic')"
  },
  {
    "objectID": "spatial_data_processing/gee_timelapse.html",
    "href": "spatial_data_processing/gee_timelapse.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Steps to create a Landsat timelapse:\n\nPan and zoom to your area of interest, or click the globe icon at the upper left corner to search for a location.\nUse the drawing tool to draw a rectangle anywhere on the map.\nAdjust the parameters (e.g., start year, end year, title) if needed.\nClick the Create timelapse button to create a timelapse.\nOnce the timelapse has been added to the map, click the hyperlink at the end if you want to download the GIF.\n\n\nimport os\nimport ee\nimport geemap\nimport ipywidgets as widgets\n\n\nMap = geemap.Map()\nMap.add_basemap('HYBRID')\nMap\n\n\n\n\nTraitError: The 'value' trait of a Label instance expected a unicode string, not the int 10.\n\n\nTraitError: The 'value' trait of a Label instance expected a unicode string, not the int 1984.\n\n\nTraitError: The 'value' trait of a Label instance expected a unicode string, not the int 2020.\n\n\nTraitError: The 'value' trait of a Label instance expected a unicode string, not the int 5.\n\n\nTraitError: The 'value' trait of a Label instance expected a unicode string, not the int 10.\n\n\nTraitError: The 'value' trait of a Label instance expected a unicode string, not the int 30.\n\n\nTraitError: The 'value' trait of a Label instance expected a unicode string, not the int 0.\n\n\n\n\n\nGenerating URL...\nDownloading GIF image from https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/videoThumbnails/8557d6ed3c0cdb0ffa673666569b4260-0e20ead6bdef0b1d973c2259323cb508:getPixels\nPlease wait ...\nAn error occurred while downloading.\nUser memory limit exceeded.\nThe input gif file does not exist.\nThe input gif file does not exist.\nAdding GIF to the map ...\nThe provided file does not exist.\nThe timelapse has been added to the map.\n\n\nGenerating URL...\nDownloading GIF image from https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/videoThumbnails/8557d6ed3c0cdb0ffa673666569b4260-0ed6f7095071ae8dea998b8fcb895851:getPixels\nPlease wait ...\nAn error occurred while downloading.\nUser memory limit exceeded.\nThe input gif file does not exist.\nThe input gif file does not exist.\nAdding GIF to the map ...\nThe provided file does not exist.\nThe timelapse has been added to the map.\n\n\nGenerating URL...\nDownloading GIF image from https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/videoThumbnails/8557d6ed3c0cdb0ffa673666569b4260-be14a4d9b69711ca1550db81d8fda1a8:getPixels\nPlease wait ...\nAn error occurred while downloading.\nUser memory limit exceeded.\nThe input gif file does not exist.\nThe input gif file does not exist.\nAdding GIF to the map ...\nThe provided file does not exist.\nThe timelapse has been added to the map."
  },
  {
    "objectID": "spatial_data_processing/raster_data_processing.html",
    "href": "spatial_data_processing/raster_data_processing.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Converting Data from Raster to Tabular (Geometry) format\n\nImport the libraries\n\nimport pandas\nimport osmnx\nimport geopandas \nimport rioxarray\nimport xarray\nimport datashader as ds\nimport contextily as cx\nfrom shapely import geometry\nimport matplotlib.pyplot as plt\nimport folium\n\n\nimport warnings\nwarnings.filterwarnings(action='ignore')\n\n\n\nDownload Geopackage\n\n# URL for the geopackage\nurl = (\"https://jeodpp.jrc.ec.europa.eu/ftp/\"\\\n       \"jrc-opendata/GHSL/\"\\\n       \"GHS_FUA_UCDB2015_GLOBE_R2019A/V1-0/\"\\\n       \"GHS_FUA_UCDB2015_GLOBE_R2019A_54009_1K_V1_0.zip\"\n      )\nurl\n\n'https://jeodpp.jrc.ec.europa.eu/ftp/jrc-opendata/GHSL/GHS_FUA_UCDB2015_GLOBE_R2019A/V1-0/GHS_FUA_UCDB2015_GLOBE_R2019A_54009_1K_V1_0.zip'\n\n\n\n\nVisualize the map\n\n# Visualize the Map for Sao Paulo\np = f\"zip+{url}!GHS_FUA_UCDB2015_GLOBE_R2019A_54009_1K_V1_0.gpkg\"\nfuas = geopandas.read_file(p)\nsao_paulo = fuas.query(\"eFUA_name == 'São Paulo'\").to_crs(\"EPSG:4326\")\n\n\nax = sao_paulo.plot(alpha=0.5, figsize=(9, 9))\ncx.add_basemap(ax, crs=sao_paulo.crs);\n\n\n\n\n\n\nDownload the population data\n\nurl = (\"https://cidportal.jrc.ec.europa.eu/ftp/\"\\\n       \"jrc-opendata/GHSL/GHS_POP_MT_GLOBE_R2019A/\"\\\n       \"GHS_POP_E2015_GLOBE_R2019A_54009_250/V1-0/\"\\\n       \"tiles/\"\\\n       \"GHS_POP_E2015_GLOBE_R2019A_54009_250_V1_0_13_11.zip\"\n      )\nurl\n\n'https://cidportal.jrc.ec.europa.eu/ftp/jrc-opendata/GHSL/GHS_POP_MT_GLOBE_R2019A/GHS_POP_E2015_GLOBE_R2019A_54009_250/V1-0/tiles/GHS_POP_E2015_GLOBE_R2019A_54009_250_V1_0_13_11.zip'\n\n\n\n# Population data in raster format\n%%time\np = f\"zip+{url}!GHS_POP_E2015_GLOBE_R2019A_54009_250_V1_0_13_11.tif\"\nghsl = rioxarray.open_rasterio(p)\nghsl\n\nCPU times: user 35.6 ms, sys: 4.12 ms, total: 39.8 ms\nWall time: 7.12 s\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.DataArray (band: 1, y: 4000, x: 4000)>\n[16000000 values with dtype=float32]\nCoordinates:\n  * band         (band) int64 1\n  * x            (x) float64 -5.041e+06 -5.041e+06 ... -4.041e+06 -4.041e+06\n  * y            (y) float64 -2e+06 -2e+06 -2.001e+06 ... -3e+06 -3e+06\n    spatial_ref  int64 0\nAttributes:\n    AREA_OR_POINT:  Area\n    _FillValue:     -200.0\n    scale_factor:   1.0\n    add_offset:     0.0xarray.DataArrayband: 1y: 4000x: 4000...[16000000 values with dtype=float32]Coordinates: (4)band(band)int641array([1])x(x)float64-5.041e+06 ... -4.041e+06array([-5040875., -5040625., -5040375., ..., -4041625., -4041375., -4041125.])y(y)float64-2e+06 -2e+06 ... -3e+06 -3e+06array([-2000125., -2000375., -2000625., ..., -2999375., -2999625., -2999875.])spatial_ref()int640crs_wkt :PROJCS[\"World_Mollweide\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0],UNIT[\"Degree\",0.0174532925199433]],PROJECTION[\"Mollweide\"],PARAMETER[\"central_meridian\",0],PARAMETER[\"false_easting\",0],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]spatial_ref :PROJCS[\"World_Mollweide\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0],UNIT[\"Degree\",0.0174532925199433]],PROJECTION[\"Mollweide\"],PARAMETER[\"central_meridian\",0],PARAMETER[\"false_easting\",0],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]GeoTransform :-5041000.0 250.0 0.0 -2000000.0 0.0 -250.0array(0)Indexes: (3)bandPandasIndexPandasIndex(Int64Index([1], dtype='int64', name='band'))xPandasIndexPandasIndex(Float64Index([-5040875.0, -5040625.0, -5040375.0, -5040125.0, -5039875.0,\n              -5039625.0, -5039375.0, -5039125.0, -5038875.0, -5038625.0,\n              ...\n              -4043375.0, -4043125.0, -4042875.0, -4042625.0, -4042375.0,\n              -4042125.0, -4041875.0, -4041625.0, -4041375.0, -4041125.0],\n             dtype='float64', name='x', length=4000))yPandasIndexPandasIndex(Float64Index([-2000125.0, -2000375.0, -2000625.0, -2000875.0, -2001125.0,\n              -2001375.0, -2001625.0, -2001875.0, -2002125.0, -2002375.0,\n              ...\n              -2997625.0, -2997875.0, -2998125.0, -2998375.0, -2998625.0,\n              -2998875.0, -2999125.0, -2999375.0, -2999625.0, -2999875.0],\n             dtype='float64', name='y', length=4000))Attributes: (4)AREA_OR_POINT :Area_FillValue :-200.0scale_factor :1.0add_offset :0.0\n\n\n\n\nVisualize the population on raster data\n\ncvs = ds.Canvas(plot_width=600, plot_height=600)\nagg = cvs.raster(ghsl.where(ghsl>0).sel(band=1))\n\n\nf, ax = plt.subplots(1, figsize=(9, 7))\nagg.plot.imshow(ax=ax, alpha=0.5, cmap=\"cividis_r\")\ncx.add_basemap(\n    ax, \n    crs=ghsl.rio.crs, \n    zorder=-1, \n    source=cx.providers.CartoDB.Voyager\n)\n\n\n\n\n\n# Clip the data for Sao Paulo\nghsl_sp = ghsl.rio.clip(sao_paulo.to_crs(ghsl.rio.crs).geometry.iloc[0])\nghsl_sp\n\n/home/thulasiram/miniconda3/envs/geopy/lib/python3.9/site-packages/rasterio/features.py:290: ShapelyDeprecationWarning: Iteration over multi-part geometries is deprecated and will be removed in Shapely 2.0. Use the `geoms` property to access the constituent parts of a multi-part geometry.\n  for index, item in enumerate(shapes):\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.DataArray (band: 1, y: 416, x: 468)>\narray([[[-200., -200., -200., ..., -200., -200., -200.],\n        [-200., -200., -200., ..., -200., -200., -200.],\n        [-200., -200., -200., ..., -200., -200., -200.],\n        ...,\n        [-200., -200., -200., ..., -200., -200., -200.],\n        [-200., -200., -200., ..., -200., -200., -200.],\n        [-200., -200., -200., ..., -200., -200., -200.]]], dtype=float32)\nCoordinates:\n  * band         (band) int64 1\n  * x            (x) float64 -4.482e+06 -4.482e+06 ... -4.365e+06 -4.365e+06\n  * y            (y) float64 -2.822e+06 -2.822e+06 ... -2.926e+06 -2.926e+06\n    spatial_ref  int64 0\nAttributes:\n    AREA_OR_POINT:  Area\n    scale_factor:   1.0\n    add_offset:     0.0\n    _FillValue:     -200.0xarray.DataArrayband: 1y: 416x: 468-200.0 -200.0 -200.0 -200.0 -200.0 ... -200.0 -200.0 -200.0 -200.0array([[[-200., -200., -200., ..., -200., -200., -200.],\n        [-200., -200., -200., ..., -200., -200., -200.],\n        [-200., -200., -200., ..., -200., -200., -200.],\n        ...,\n        [-200., -200., -200., ..., -200., -200., -200.],\n        [-200., -200., -200., ..., -200., -200., -200.],\n        [-200., -200., -200., ..., -200., -200., -200.]]], dtype=float32)Coordinates: (4)band(band)int641array([1])x(x)float64-4.482e+06 ... -4.365e+06axis :Xlong_name :x coordinate of projectionstandard_name :projection_x_coordinateunits :metrearray([-4481875., -4481625., -4481375., ..., -4365625., -4365375., -4365125.])y(y)float64-2.822e+06 ... -2.926e+06axis :Ylong_name :y coordinate of projectionstandard_name :projection_y_coordinateunits :metrearray([-2822125., -2822375., -2822625., ..., -2925375., -2925625., -2925875.])spatial_ref()int640crs_wkt :PROJCS[\"World_Mollweide\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0],UNIT[\"Degree\",0.0174532925199433]],PROJECTION[\"Mollweide\"],PARAMETER[\"central_meridian\",0],PARAMETER[\"false_easting\",0],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]spatial_ref :PROJCS[\"World_Mollweide\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0],UNIT[\"Degree\",0.0174532925199433]],PROJECTION[\"Mollweide\"],PARAMETER[\"central_meridian\",0],PARAMETER[\"false_easting\",0],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]GeoTransform :-4482000.0 250.0 0.0 -2822000.0 0.0 -250.0array(0)Indexes: (3)bandPandasIndexPandasIndex(Int64Index([1], dtype='int64', name='band'))xPandasIndexPandasIndex(Float64Index([-4481875.0, -4481625.0, -4481375.0, -4481125.0, -4480875.0,\n              -4480625.0, -4480375.0, -4480125.0, -4479875.0, -4479625.0,\n              ...\n              -4367375.0, -4367125.0, -4366875.0, -4366625.0, -4366375.0,\n              -4366125.0, -4365875.0, -4365625.0, -4365375.0, -4365125.0],\n             dtype='float64', name='x', length=468))yPandasIndexPandasIndex(Float64Index([-2822125.0, -2822375.0, -2822625.0, -2822875.0, -2823125.0,\n              -2823375.0, -2823625.0, -2823875.0, -2824125.0, -2824375.0,\n              ...\n              -2923625.0, -2923875.0, -2924125.0, -2924375.0, -2924625.0,\n              -2924875.0, -2925125.0, -2925375.0, -2925625.0, -2925875.0],\n             dtype='float64', name='y', length=416))Attributes: (4)AREA_OR_POINT :Areascale_factor :1.0add_offset :0.0_FillValue :-200.0\n\n\nout_p = “../data/ghsl_sao_paulo.tif” ! rm $out_p ghsl_sp.rio.to_raster(out_p)\n\n\nConvert Raster to geometry\n\n# Read the raster data\nsurface = xarray.open_rasterio(\"../data/ghsl_sao_paulo.tif\")\n\n\n# Convert raster to geometry\nt_surface = surface.to_series()\n\n\nt_surface.head()\n\nband  y           x         \n1     -2822125.0  -4481875.0   -200.0\n                  -4481625.0   -200.0\n                  -4481375.0   -200.0\n                  -4481125.0   -200.0\n                  -4480875.0   -200.0\ndtype: float32\n\n\n\nt_surface = t_surface.reset_index().rename(columns={0: \"Value\"})\n\n\nt_surface.query(\"Value > 1000\").info()\n\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 7734 entries, 3785 to 181296\nData columns (total 4 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   band    7734 non-null   int64  \n 1   y       7734 non-null   float64\n 2   x       7734 non-null   float64\n 3   Value   7734 non-null   float32\ndtypes: float32(1), float64(2), int64(1)\nmemory usage: 271.9 KB\n\n\n\ntype(t_surface)\n\npandas.core.frame.DataFrame\n\n\n\n# Calculate the polygon based on resolution values\ndef row2cell(row, res_xy):\n    res_x, res_y = res_xy  # Extract resolution for each dimension\n    # XY Coordinates are centered on the pixel\n    minX = row[\"x\"] - (res_x / 2)\n    maxX = row[\"x\"] + (res_x / 2)\n    minY = row[\"y\"] + (res_y / 2)\n    maxY = row[\"y\"] - (res_y / 2)\n    poly = geometry.box(\n        minX, minY, maxX, maxY\n    )  # Build squared polygon\n    return poly\n\n\n# Get the polygons\nmax_polys = (\n    t_surface.query(\n        \"Value > 1000\"\n    )  # Keep only cells with more than 1k people\n    .apply(  # Build polygons for selected cells\n        row2cell, res_xy=surface.attrs[\"res\"], axis=1\n    )\n    .pipe(  # Pipe result from apply to convert into a GeoSeries\n        geopandas.GeoSeries, crs=surface.attrs[\"crs\"]\n    )\n)\n\n\n# Plot polygons on the map\nax = max_polys.plot(edgecolor=\"red\", figsize=(9, 9))\n# Add basemap\ncx.add_basemap(\n    ax, crs=surface.attrs[\"crs\"], source=cx.providers.CartoDB.Voyager\n)\n\n\n\n\n\n\nConvert Geometry to Raster\n\nnew_da = xarray.DataArray.from_series(\n    t_surface.set_index([\"band\", \"y\", \"x\"])[\"Value\"]\n)\nnew_da\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.DataArray 'Value' (band: 1, y: 416, x: 468)>\narray([[[-200., -200., -200., ..., -200., -200., -200.],\n        [-200., -200., -200., ..., -200., -200., -200.],\n        [-200., -200., -200., ..., -200., -200., -200.],\n        ...,\n        [-200., -200., -200., ..., -200., -200., -200.],\n        [-200., -200., -200., ..., -200., -200., -200.],\n        [-200., -200., -200., ..., -200., -200., -200.]]], dtype=float32)\nCoordinates:\n  * band     (band) int64 1\n  * y        (y) float64 -2.926e+06 -2.926e+06 ... -2.822e+06 -2.822e+06\n  * x        (x) float64 -4.482e+06 -4.482e+06 ... -4.365e+06 -4.365e+06xarray.DataArray'Value'band: 1y: 416x: 468-200.0 -200.0 -200.0 -200.0 -200.0 ... -200.0 -200.0 -200.0 -200.0array([[[-200., -200., -200., ..., -200., -200., -200.],\n        [-200., -200., -200., ..., -200., -200., -200.],\n        [-200., -200., -200., ..., -200., -200., -200.],\n        ...,\n        [-200., -200., -200., ..., -200., -200., -200.],\n        [-200., -200., -200., ..., -200., -200., -200.],\n        [-200., -200., -200., ..., -200., -200., -200.]]], dtype=float32)Coordinates: (3)band(band)int641array([1])y(y)float64-2.926e+06 ... -2.822e+06array([-2925875., -2925625., -2925375., ..., -2822625., -2822375., -2822125.])x(x)float64-4.482e+06 ... -4.365e+06array([-4481875., -4481625., -4481375., ..., -4365625., -4365375., -4365125.])Indexes: (3)bandPandasIndexPandasIndex(Int64Index([1], dtype='int64', name='band'))yPandasIndexPandasIndex(Float64Index([-2925875.0, -2925625.0, -2925375.0, -2925125.0, -2924875.0,\n              -2924625.0, -2924375.0, -2924125.0, -2923875.0, -2923625.0,\n              ...\n              -2824375.0, -2824125.0, -2823875.0, -2823625.0, -2823375.0,\n              -2823125.0, -2822875.0, -2822625.0, -2822375.0, -2822125.0],\n             dtype='float64', name='y', length=416))xPandasIndexPandasIndex(Float64Index([-4481875.0, -4481625.0, -4481375.0, -4481125.0, -4480875.0,\n              -4480625.0, -4480375.0, -4480125.0, -4479875.0, -4479625.0,\n              ...\n              -4367375.0, -4367125.0, -4366875.0, -4366625.0, -4366375.0,\n              -4366125.0, -4365875.0, -4365625.0, -4365375.0, -4365125.0],\n             dtype='float64', name='x', length=468))Attributes: (0)"
  },
  {
    "objectID": "3D_deep_learning/02_coordination_systems.html",
    "href": "3D_deep_learning/02_coordination_systems.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "The first coordination system we frequently use is called the world coordination system. This coordinate system is a 3D coordination system chosen with respect to all the 3D objects, such that the locations of the 3D objects can be easy to determine. Usually, the axis of the world coordination system does not agree with the object orientation or camera orientation.\n\n\n\nIn PyTorch3D, the camera view coordination system is defined such that the origin is at the projection point of the camera, the x axis points to the left, the y axis points upward, and the z axis points to the front.\n\n\n\ncamera view coordination system in pytorch3d\n\n\n\n\n\nThe normalized device coordinate (NDC) confines the volume that a camera can render. The x coordinate values in the NDC space range from -1 to +1, as do the y coordinate values. The z coordinate values range from znear to zfar, where znear is the nearest depth and zfar is the farthest depth. Any object out of this znear to zfar range would not be rendered by the camera.\n\n\n\nIt is defined in terms of how the rendered images are shown on our screens. The coordinate system contains the x coordinate as the columns of the pixels, the y coordinate as the rows of the pixels, and the z coordinate corresponding to the depth of the object.\n\n\n\nCamera models are to create a correspondence between 2D space and the 3D world. In pytorch3D there are two major camera models\n\n\n\nPytorch3D camera projections\n\n\n\n\nThe orthographic projections map objects to 2D images, disregarding the object depth. For example, just as shown in the figure, two objects with the same geometric size at different depths would be mapped to 2D images of the same size.\n\n\n\nIf an object moved far away from the camera, it would be mapped to a smaller size on the 2D images."
  },
  {
    "objectID": "3D_deep_learning/resources.html",
    "href": "3D_deep_learning/resources.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Resources\n\nMega Github resource\npytorch3d\nmeshrcnn\n3D Deep learning youtube videos\nNvidia Kaolin"
  },
  {
    "objectID": "3D_deep_learning/01_data_formats.html",
    "href": "3D_deep_learning/01_data_formats.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Depth images\nPoint clouds\nVoxels\nMeshes\n\n\n\nDepth images contain the depth values of a scene in the form of distance from the camera in meters for each pixel in the image frame.\n\n\n\nA Point cloud is a collection of three-dimensional points distributed in a 3D space. Each of these 3D points has a deterministic position denoted by a certain (x, y, z) coordinate along with other attributes like RGB colour values. Unlike depth images, point cloud representation preserves more high-quality geometric information of the three-dimensional space without any discretization.\nPoint clouds do not have grid-like structures, thus convolutions cannot be directly used for them. They are one of the unordered and irregular data types. There are no clear and regular definitions for neighboring points for each point in a point cloud.special types of deep learning models need to be used for processing point clouds, such as PointNet\nAnother issue for point clouds as training data for 3D deep learning is the heterogeneous data issue – that is, for one training dataset, different point clouds may contain different numbers of 3D points.\n\n\n\nA voxel is just a pixel in a three-dimensional space. A voxel is defined by dividing a 3D cube into smaller-sized cubes and each cube is called one voxel.\nVoxel usually use Truncated Signed Distance Functions (TSDFs) to represent 3D surfaces.\nA Signed Distance Function (SDF) can be defined at each voxel as the (signed) distance between the center of the voxel to the closest point on the surface. A positive sign in an SDF indicates that the voxel center is outside an object. The only difference between a TSDF and an SDF is that the values of a TSDF are truncated, such that the values of a TSDF always range from -1 to +1.\n#### Polygon Mesh A Polygon Mesh is a collection of edges, vertices and faces that together defines the shape and volume of a polyhedral object.\nEach mesh contains a set of 3D points called vertices. Each mesh also contains a set of polygons called faces, which are defined on vertices.\nMeshes also have heterogeneous data issues.\n\n\n\n\nply\nobj\n\n\n\n\n\nLink to Blog post\nBook - 3D deep learning with python"
  },
  {
    "objectID": "3D_deep_learning/07_gan_based_image_synthesis.html",
    "href": "3D_deep_learning/07_gan_based_image_synthesis.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Compositional 3D Aware Image Synthesis"
  },
  {
    "objectID": "3D_deep_learning/06_Differentiable_volumetric_rendering.html",
    "href": "3D_deep_learning/06_Differentiable_volumetric_rendering.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Volumetric rendering is a collection of techniques used to generate a 2D view of discrete 3D data.The projections generated from this method are without any explicit conversion to a geometric representation. Volumetric rendering is typically used when generating surfaces is difficult or can lead to errors. It can also be used when the content (and not just the geometry and surface) of the volume is important. It is typically used for data visualization.\n\n\n\nVolumetric Rendering\n\n\n\n\nTo determine the RGB values at each pixel, a ray is generated from the projection center going through each image pixel of the cameras. We need to check the probability of occupancy or opacity and colors along this ray to determine RGB values for the pixel. Note there are infinitely many points on each such ray. Thus, we need to have a sampling scheme to select a certain number of points along this ray. This sampling operation is called ray sampling.\n\n\n\nwe have densities and colors defined on the nodes of the volume but not on the points on the rays. Thus, we need to have a way to convert densities and colors of volumes to points on rays. This operation is called volume sampling.\nThe points defined in the ray sampling step might not fall exactly on a point. The nodes of the volume grids and points on rays typically have different spatial locations. We need to use an interpolation scheme to interpolate the densities and colors at points of rays from the densities and colors at volumes.\n\n\n\nFrom the densities and colors of the rays, we need to determine the RGB values of each pixel. In this process, we need to compute how many incident lights can arrive at each point along the ray and how many lights are reflected to the image pixel. We call this process ray marching.\n\n\n\nWhile standard volumetric rendering is used to render 2D projections of 3D data, differentiable volume rendering is used to do the opposite: construct 3D data from 2D images.\n\n\nWe will have ground-truth images. From the intial 3D model, 2D images are rendered and compared to the ground truth images. As this process is differentiable, we run an optimization algorithm to get the final resulting volumetric model. This model can be used to render images from new angles.\nwe represent the shape and texture of the object as a parametric function. This function can be used to generate 2D projections. But, given 2D projections (this is typically multiple views of the 3D scene), we can optimize the parameters of these implicit shape and texture functions so that its projections are the multi-view 2D images. This optimization is possible since the rendering process is completely differentiable, and the implicit functions used are also differentiable."
  },
  {
    "objectID": "3D_deep_learning/05_differentiable_rendering.html",
    "href": "3D_deep_learning/05_differentiable_rendering.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "The physical process of image formation is a mapping from 3D models to 2D images.\n\n\n\nThe image formation process is a mapping from the 3D models to 2D images\n\n\nMany 3D computer vision problems are a reversal of image formation. In these problems, we are usually given 2D images and need to estimate the 3D models from the 2D images.\n\n\n\nMany 3D computer vision problems are based on 2D images given to estimate 3D models\n\n\nwe can cast this as an optimization problem. We can minimize the distance between rendered image and the original image. To do this the rendering should be differentiable.\n\n\n\n\n\nBy considering a weighted average of all the relevant mesh faces instead of single mesh face per ray we can make the rendering differentiable"
  },
  {
    "objectID": "3D_deep_learning/04_fitting_deformable_mesh_to_point.html",
    "href": "3D_deep_learning/04_fitting_deformable_mesh_to_point.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Real-world depth cameras, such as LiDAR, time-of-flight cameras, and stereo vision cameras, usually output either depth images or point clouds. These devices do not give any direct measurements of surfaces. However, in many real-world applications, surface information is sought.\n\n\nThe cost function should be chosen such that it is a good measurement of how similar the point cloud is to the mesh. One such cost function is Chamfer set distance.\n\n\n\nChamfer distance\n\n\nFor fitting meshes to point clouds, we first randomly sample some points from a mesh model and then optimize the Chamfer distances between the sampled points from the mesh model and the input point cloud.\n\n\n\nThere may exist multiple mesh models that can be good fits to the same point cloud.These mesh models that are good fits may include some mesh models that are far away from smooth meshes.\nRegularization methods:- * Mesh laplacian smoothing loss * Mesh normal consistency loss * Mesh edge loss\n\n\n\n\n\nMesh Laplacian Smoothing loss\n\n\nIn the preceding definition, the Laplacian at the i-th vertex is just a sum of differences, where each difference is between the coordinates of the current vertex and those of a neighboring vertex.\nThe Laplacian is a measurement for smoothness. If the i-th vertex and its neighbors lie all within one plane, then the Laplacian should be zero.\n\n\n\nThe mesh normal consistency loss is a loss function for penalizing the distances between adjacent normal vectors on the mesh.\n\n\n\n\nIt is sum of all the edge lengths in the mesh. Longer edge lenghts will not capture the fine details of slowly varying surfaces.\n\n\n\n\nAn Example of 3D point cloud of a Pedestrian\n\n\n\n\n\nOptimized deformed mesh model. We have far more points than the original input point cloud. All the different losses are weighted"
  },
  {
    "objectID": "3D_deep_learning/03_rendering.html",
    "href": "3D_deep_learning/03_rendering.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Rendering is the process of projecting 3D physical models into 2D images\nIn the below example, the world model contains one 3D sphere, which is represented by a mesh model. To form the image of the 3D sphere, for each image pixel, we generate one ray, starting from the camera origin and going through the image pixel. If one ray intersects with one mesh face, then we know the mesh face can project its color to the image pixel. We also need to trace the depth of each intersection because a face with a smaller depth would occlude faces with larger depths.\n\n\n\nRendering\n\n\nRendering usually is divided into two stages * Rasterization * Shading\n\n\nIt is the process of finding relevant geometric objects for each image pixel\n\n\n\nIt is the process of taking the outputs of the rasterization and computing the pixel value for each image pixel.\n\n\nLambertian surfaces are types of objects that are not shiny at all, such as paper, unfinished wood, and unpolished stones.\n\n\n\nLambertian shading model\n\n\nOne basic idea of the Lambertian cosine law is that for Lambertian surfaces, the amplitude of the reflected light does not depend on the viewer’s angle, but only depends on the angle between the surface normal and the direction of the incident light.\n\n\n\nThe Phong lighting model is a frequently used model for these glossy components.\nOne basic principle of the Phong lighting model is that the shiny light component should be strongest in the direction of reflection of the incoming light. The component would become weaker as the angle between the direction of reflection and the viewing angle becomes larger.\n\n\n\nPhong model"
  },
  {
    "objectID": "3D_deep_learning/06_nerf.html",
    "href": "3D_deep_learning/06_nerf.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Differentiable volume Rendering will require a lot of storage. This is undesirable if we want to transmit this information over the network.\nNeRF is one of the first techniques to model a 3D scene that requires less disk space and also captures the fine geometry and texture of complex scenes.\n\n\nThe challenge is to synthesize new views of a 3D scene using a small number of available 2D snapshots of the scene. The challenge is to construct complete information about the world given incomplete and noisy information.\n## Radiance Fields Radiance is the intensity of a point in space when viewed in a particular direction. When capturing this information in RGB, the radiance will have three components corresponding to the colors Red, Green, and Blue. The radiance of a point in space can depend on many factors, including the following:\n\nLight sources illuminating that point\nThe existence of a surface (or volume density) that can reflect light at that point\nThe texture properties of the surface\n\n\n\n\nRadiance (r,g,b) at a point (x,y,z) when viewed from certain viewing angles\n\n\nIf we know the radiance of all the points in a scene in all directions then it constitutes a radiance field.\n\n\n\nNeRF uses a neural network to represent a volumetric scene function. This neural network takes a 5-dimensional input. These are the three spatial locations (x, y, z) and two viewing angles (θ, ∅). Its output is the volume density σ at (x, y, z) and the emitted color (r, g, b) of the point (x, y, z) when viewed from the viewing angle (θ, ∅).\nThe model therefore maps any point in the 3D scene and a viewing angle to the volume density and radiance at that point. You can then use this model to synthesize views by querying the 5D coordinates along camera rays and using the volume rendering technique to project the output colors and volume densities into an image.\n\n\n\nNeRF Architecture\n\n\nA single NeRF model is optimized on a set of images from a single scene. Therefore, each model only knows the scene on which it is optimized. NeRF is optimized to generalize unseen viewpoints well for a particular scene."
  },
  {
    "objectID": "about_me/why_me.html",
    "href": "about_me/why_me.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "The International Social Service (ISS) is an international NGO founded in 1924 which assists children and families confronted with complex social problems as a result of migration. This project was undertaken in collaboration with 30 data scientists around the world.\n\n\n\n\nA Goodwill project I did for The International Social Service\n\n\n\n\n\n\n\n\nI am a team player\n\n\n\nAn example - I initiated and built a data discovery platform understanding the difficulties faced by other data scientists in the team\nI initiated and built a automated system for video moderation and tagging after knowing about the challenges faced by a different team in the company\n\n\n\n\n\nAfter serving in military for 20 years, I pivoted to a corporate environment and a different industry\nI love making small upgrades on a daily basis\n\n\n\nI upgrade my version everyday\n\n\n\n\n\n\n\nBooks on ML I read the previous year \nCourse I am doing currently"
  },
  {
    "objectID": "about_me/my_experience.html",
    "href": "about_me/my_experience.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "# My Data Science Experience\n\nI got 6 years of experience as a Data Science Manager and Data Scientist. In my Data Science career I worked on a variety of data science projects like Building a Recommendation engine for a social media app, building look-a-like models and deploying them on scale (100’s of them) and automating the whole process, Content moderation and tagging using computer vision, building churn prediction models, creating a data lake, building a data discovery platform, automating dashboards etc. I have skills in various data science topics like Computer vision, NLP, Interpretable Machine learning, Causal Inference, Probabilistic Programming, Privacy preserving ML among other things. I love to work with data, derive insights and build models to drive business decisions."
  },
  {
    "objectID": "about_me/experience.html",
    "href": "about_me/experience.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "# Experience\n\nI got 6 years of experience as a Data Science Manager and Data Scientist. In my Data Science career I worked on a variety of data science projects like Building a Recommendation engine for a social media app, building look-a-like models and deploying them on scale (100’s of them) and automating the whole process, Content moderation and tagging using computer vision, building churn prediction models, creating a data lake, building a data discovery platform, automating dashboards etc. I have skills in various data science topics like Computer vision, NLP, Interpretable Machine learning, Causal Inference, Probabilistic Programming, Privacy preserving ML among other things. I love to work with data, derive insights and build models to drive business decisions."
  },
  {
    "objectID": "causal_inference/resources.html",
    "href": "causal_inference/resources.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "causal Inference Book\nBrady Neal course website\nBrady Neal causal Book\nBrady Neal youtube course\nJoshua Angrist videos\nwhat If - Book\nwhat If Python code\nMixtape - Book\nstatistical Rethinking book code in python and pymc"
  },
  {
    "objectID": "causal_inference/resources.html#packages",
    "href": "causal_inference/resources.html#packages",
    "title": "My Datascience Journey",
    "section": "Packages",
    "text": "Packages\n\nCausalinference\nCausallib\nCausalimpact\nDoWhy\nEconml"
  },
  {
    "objectID": "causal_inference/03_stats_revision.html",
    "href": "causal_inference/03_stats_revision.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Stats Revisited\n\nWith standard error, we can create an interval that will contain the true mean 95% of the time (error is for estimating true mean from the mean of the means of experiment).\nConfidence interval is calculated when we don’t have the luxury of simulating the same experiment with multiple datasets.To calculate confidence interval, we use the central limit theorem.As 95% of the mass of normal distribution is between 1.96 (close to 2) standard errors, we multiply standard error by 2 and add and subtract it from the mean of one of our experiments, we will construct a 95% confidence interval for the true mean. (In practice we multiple by Z which is a cumulative density function instead of 2). If sample size is small, the larger the standard error and wider the confidence interval.\nThe sum or difference of 2 independent normal distributions is also normal distribution. The resulting mean will be the sum or difference between the two distributions, while the variance will always be the sum of the variance. \nP-values - It measures how unlikely is the measurement given that null hypothesis is true. p-values is P(data|Null Hypothesis is True) \n\n\nCode to do AB Testing in python\ndef AB_test(test: pd.Series, control: pd.Series, confidence=0.95, h0=0):\n    mu1, mu2 = test.mean(), control.mean()\n    se1, se2 = test.std() / np.sqrt(len(test)), control.std() / np.sqrt(len(control))\n    \n    diff = mu1 - mu2\n    se_diff = np.sqrt(test.var()/len(test) + control.var()/len(control))\n    \n    z_stats = (diff-h0)/se_diff\n    p_value = stats.norm.cdf(z_stats)\n    \n    def critial(se): return -se*stats.norm.ppf((1 - confidence)/2)\n    \n    print(f\"Test {confidence*100}% CI: {mu1} +- {critial(se1)}\")\n    print(f\"Control {confidence*100}% CI: {mu2} +- {critial(se2)}\")\n    print(f\"Test-Control {confidence*100}% CI: {diff} +- {critial(se_diff)}\")\n    print(f\"Z Statistic {z_stats}\")\n    print(f\"P-Value {p_value}\")"
  },
  {
    "objectID": "causal_inference/01_intro.html",
    "href": "causal_inference/01_intro.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Average Treatment Effect and Average Treatment Effect on the treated\n\n\n\n\n\nAssociation Vs ATT\n\n\nIn the above equation the first term is association. If there is no Bias, then association will be causation. Bias is given by how treated and control group differ before the treatment, in case neither of them has received the treatment.\nCausal Inference is all about finding clever ways to removing bias and making the treated and untreated comparable so that all the difference we see is only the average treatment effect.\n\n\n\nIt happens because of unequal weights given to different conditions for different treatments. If we stratify the condition then we will know the causal impact\nCorrleation is not causation becuase correlation = confounding effect + causal effect."
  },
  {
    "objectID": "causal_inference/02_randomised_exp.html",
    "href": "causal_inference/02_randomised_exp.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Randomised Experiments\n\nRandomised experiments can make bias vanish\nThey tend to be very expensive or plain unethical. Sometimes it is not practical to control the assignment.\nRandomised experiments are the simplest and most effective way to uncover causal impact. It does this by making the treatment and control groups comparable."
  },
  {
    "objectID": "causal_inference/05_potential_outcomes.html",
    "href": "causal_inference/05_potential_outcomes.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Intro - Potential Outcomes"
  },
  {
    "objectID": "causal_inference/04_graphical_causal_models.html",
    "href": "causal_inference/04_graphical_causal_models.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Graphical Causal Models\n\nMy understanding of conditioning in causality is filtering on a value of the variable. \n\n\nDirect path\n\n\n\nDirect Path\n\n\nIf we are conditioning on the intermediary variable i,e problem solving, then the dependence between causal knowledge and promotion is broken - They become independent.\n\n\n\nDirect Path Rule\n\n\n\n\nFork\nIn the case of a fork, the dependence flows backward through the arrows, and we have a Backdoor path\n\n\n\nFork\n\n\nWe can close the backdoor path and shut down dependence by conditioning on the common cause. Two variables that share a common cause are dependent, but independent when we condition on the common cause.\n\n\n\nFork Rule\n\n\n\n\nCollider\n\nA collider is when two arrows collide on a single variable. Both the causes share a common effect.\n\n\n\n\ncollider\n\n\nConditioning on the collider opens the dependence path. Not conditioning on it leaves it closed.\n\n\n\ncollider rule\n\n\n\nBased on the above, we can have a more general rule. A path is blocked if and only if:\n\nIt contains a non-collider that has been conditioned on\nIt contains a collider and that has not been conditioned on and has no descendants that have been conditioned on\n\n\n\n\n\ncausality rules\n\n\n\n\nDescendent\n\n\n\nDescendent\n\n\n\nExcercises for causal rules\n \n\n\nImportant points\n\n\n\nUnderstanding the causal effects\n\n\n\n\n\nConfounding\nIt is caused when the treatment and the outcome share a common cause. If we close all the backdoor paths between the treatment and the outcome, we can identify the causal effect.\n\n\n\nconfounding Bias\n\n\nTo fix confounding bias, we need to control all common causes of the treatment and the outcome\nSometimes confounders are not measurable. we may have other measured variables that can act as a proxy for the confounder. controlling for them will help lower the bias. Those variables are sometimes referred to as surrogate confounders. Controlling for the surrogate variables is not sufficient to eliminate bias, but it helps\n\n\nSelection Bias\nSelection bias arises when we control for more variables than we should. It might be the case that the treatment and the potential outcome are marginally independent but become dependent once we condition on a collider.\nSelection bias can also happen due to excessive controlling of mediator variables. This excessive controlling could lead to bias even if the treatment was randomly assigned.\nSelection bias can often be fixed by simply doing nothing, which is why it is dangerous. Since we are biased toward action, we tend to see ideas that control things as clever when they can be doing more harm than good."
  },
  {
    "objectID": "telecom_churn_prediction/telecom_churn_prediction.html",
    "href": "telecom_churn_prediction/telecom_churn_prediction.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Telecom Churn Prediction\nThe objectives of this project are:-\n1. Perform exploratory analysis and extract insights from the dataset.\n2. Split the dataset into train/test sets and explain your reasoning.\n3. Build a predictive model to predict which customers are going to churn and discuss the reason why you choose a particular algorithm.\n4. Establish metrics to evaluate model performance.\n5. Discuss the potential issues with deploying the model into production\n\nImport the required libraries\n\n# python version # 3.8.2\nimport pandas as pd \nimport numpy as np \nfrom pandas_profiling import ProfileReport \nfrom pycaret.classification import * \nfrom sklearn import metrics \nimport os \nfrom sklearn.model_selection import train_test_split \n\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n# option to display all columns\npd.set_option('display.max_columns', None)\n\n\n# Read the data\ntelecom_churn = pd.read_csv('data science challenge.csv')\n\n\ntelecom_churn.head(10)\n\n\n\n\n\n  \n    \n      \n      state\n      account length\n      area code\n      phone number\n      international plan\n      voice mail plan\n      number vmail messages\n      total day minutes\n      total day calls\n      total day charge\n      total eve minutes\n      total eve calls\n      total eve charge\n      total night minutes\n      total night calls\n      total night charge\n      total intl minutes\n      total intl calls\n      total intl charge\n      customer service calls\n      churn\n    \n  \n  \n    \n      0\n      KS\n      128\n      415\n      382-4657\n      no\n      yes\n      25\n      265.1\n      110\n      45.07\n      197.4\n      99\n      16.78\n      244.7\n      91\n      11.01\n      10.0\n      3\n      2.70\n      1\n      False\n    \n    \n      1\n      OH\n      107\n      415\n      371-7191\n      no\n      yes\n      26\n      161.6\n      123\n      27.47\n      195.5\n      103\n      16.62\n      254.4\n      103\n      11.45\n      13.7\n      3\n      3.70\n      1\n      False\n    \n    \n      2\n      NJ\n      137\n      415\n      358-1921\n      no\n      no\n      0\n      243.4\n      114\n      41.38\n      121.2\n      110\n      10.30\n      162.6\n      104\n      7.32\n      12.2\n      5\n      3.29\n      0\n      False\n    \n    \n      3\n      OH\n      84\n      408\n      375-9999\n      yes\n      no\n      0\n      299.4\n      71\n      50.90\n      61.9\n      88\n      5.26\n      196.9\n      89\n      8.86\n      6.6\n      7\n      1.78\n      2\n      False\n    \n    \n      4\n      OK\n      75\n      415\n      330-6626\n      yes\n      no\n      0\n      166.7\n      113\n      28.34\n      148.3\n      122\n      12.61\n      186.9\n      121\n      8.41\n      10.1\n      3\n      2.73\n      3\n      False\n    \n    \n      5\n      AL\n      118\n      510\n      391-8027\n      yes\n      no\n      0\n      223.4\n      98\n      37.98\n      220.6\n      101\n      18.75\n      203.9\n      118\n      9.18\n      6.3\n      6\n      1.70\n      0\n      False\n    \n    \n      6\n      MA\n      121\n      510\n      355-9993\n      no\n      yes\n      24\n      218.2\n      88\n      37.09\n      348.5\n      108\n      29.62\n      212.6\n      118\n      9.57\n      7.5\n      7\n      2.03\n      3\n      False\n    \n    \n      7\n      MO\n      147\n      415\n      329-9001\n      yes\n      no\n      0\n      157.0\n      79\n      26.69\n      103.1\n      94\n      8.76\n      211.8\n      96\n      9.53\n      7.1\n      6\n      1.92\n      0\n      False\n    \n    \n      8\n      LA\n      117\n      408\n      335-4719\n      no\n      no\n      0\n      184.5\n      97\n      31.37\n      351.6\n      80\n      29.89\n      215.8\n      90\n      9.71\n      8.7\n      4\n      2.35\n      1\n      False\n    \n    \n      9\n      WV\n      141\n      415\n      330-8173\n      yes\n      yes\n      37\n      258.6\n      84\n      43.96\n      222.0\n      111\n      18.87\n      326.4\n      97\n      14.69\n      11.2\n      5\n      3.02\n      0\n      False\n    \n  \n\n\n\n\n\n\nCheck the Shape and Column types of the Dataframe\n\ntelecom_churn.shape\n\n(3333, 21)\n\n\n\ntelecom_churn.dtypes\n\nstate                      object\naccount length              int64\narea code                   int64\nphone number               object\ninternational plan         object\nvoice mail plan            object\nnumber vmail messages       int64\ntotal day minutes         float64\ntotal day calls             int64\ntotal day charge          float64\ntotal eve minutes         float64\ntotal eve calls             int64\ntotal eve charge          float64\ntotal night minutes       float64\ntotal night calls           int64\ntotal night charge        float64\ntotal intl minutes        float64\ntotal intl calls            int64\ntotal intl charge         float64\ncustomer service calls      int64\nchurn                        bool\ndtype: object\n\n\n\n\nExploratory Analysis\n\n# No missing values in the data.\n# Scaling of numeric columns is required\ntelecom_churn.describe()\n\n\n\n\n\n  \n    \n      \n      account length\n      area code\n      number vmail messages\n      total day minutes\n      total day calls\n      total day charge\n      total eve minutes\n      total eve calls\n      total eve charge\n      total night minutes\n      total night calls\n      total night charge\n      total intl minutes\n      total intl calls\n      total intl charge\n      customer service calls\n    \n  \n  \n    \n      count\n      3333.000000\n      3333.000000\n      3333.000000\n      3333.000000\n      3333.000000\n      3333.000000\n      3333.000000\n      3333.000000\n      3333.000000\n      3333.000000\n      3333.000000\n      3333.000000\n      3333.000000\n      3333.000000\n      3333.000000\n      3333.000000\n    \n    \n      mean\n      101.064806\n      437.182418\n      8.099010\n      179.775098\n      100.435644\n      30.562307\n      200.980348\n      100.114311\n      17.083540\n      200.872037\n      100.107711\n      9.039325\n      10.237294\n      4.479448\n      2.764581\n      1.562856\n    \n    \n      std\n      39.822106\n      42.371290\n      13.688365\n      54.467389\n      20.069084\n      9.259435\n      50.713844\n      19.922625\n      4.310668\n      50.573847\n      19.568609\n      2.275873\n      2.791840\n      2.461214\n      0.753773\n      1.315491\n    \n    \n      min\n      1.000000\n      408.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      23.200000\n      33.000000\n      1.040000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n    \n    \n      25%\n      74.000000\n      408.000000\n      0.000000\n      143.700000\n      87.000000\n      24.430000\n      166.600000\n      87.000000\n      14.160000\n      167.000000\n      87.000000\n      7.520000\n      8.500000\n      3.000000\n      2.300000\n      1.000000\n    \n    \n      50%\n      101.000000\n      415.000000\n      0.000000\n      179.400000\n      101.000000\n      30.500000\n      201.400000\n      100.000000\n      17.120000\n      201.200000\n      100.000000\n      9.050000\n      10.300000\n      4.000000\n      2.780000\n      1.000000\n    \n    \n      75%\n      127.000000\n      510.000000\n      20.000000\n      216.400000\n      114.000000\n      36.790000\n      235.300000\n      114.000000\n      20.000000\n      235.300000\n      113.000000\n      10.590000\n      12.100000\n      6.000000\n      3.270000\n      2.000000\n    \n    \n      max\n      243.000000\n      510.000000\n      51.000000\n      350.800000\n      165.000000\n      59.640000\n      363.700000\n      170.000000\n      30.910000\n      395.000000\n      175.000000\n      17.770000\n      20.000000\n      20.000000\n      5.400000\n      9.000000\n    \n  \n\n\n\n\n\n# Format the column names, remove space and special characters in column names\ntelecom_churn.columns =  telecom_churn.columns.str.strip().str.lower().str.replace(' ', '_').str.replace('(', '').str.replace(')', '')\n\n\ntelecom_churn\n\n\n\n\n\n  \n    \n      \n      state\n      account_length\n      area_code\n      phone_number\n      international_plan\n      voice_mail_plan\n      number_vmail_messages\n      total_day_minutes\n      total_day_calls\n      total_day_charge\n      total_eve_minutes\n      total_eve_calls\n      total_eve_charge\n      total_night_minutes\n      total_night_calls\n      total_night_charge\n      total_intl_minutes\n      total_intl_calls\n      total_intl_charge\n      customer_service_calls\n      churn\n    \n  \n  \n    \n      0\n      KS\n      128\n      415\n      382-4657\n      no\n      yes\n      25\n      265.1\n      110\n      45.07\n      197.4\n      99\n      16.78\n      244.7\n      91\n      11.01\n      10.0\n      3\n      2.70\n      1\n      False\n    \n    \n      1\n      OH\n      107\n      415\n      371-7191\n      no\n      yes\n      26\n      161.6\n      123\n      27.47\n      195.5\n      103\n      16.62\n      254.4\n      103\n      11.45\n      13.7\n      3\n      3.70\n      1\n      False\n    \n    \n      2\n      NJ\n      137\n      415\n      358-1921\n      no\n      no\n      0\n      243.4\n      114\n      41.38\n      121.2\n      110\n      10.30\n      162.6\n      104\n      7.32\n      12.2\n      5\n      3.29\n      0\n      False\n    \n    \n      3\n      OH\n      84\n      408\n      375-9999\n      yes\n      no\n      0\n      299.4\n      71\n      50.90\n      61.9\n      88\n      5.26\n      196.9\n      89\n      8.86\n      6.6\n      7\n      1.78\n      2\n      False\n    \n    \n      4\n      OK\n      75\n      415\n      330-6626\n      yes\n      no\n      0\n      166.7\n      113\n      28.34\n      148.3\n      122\n      12.61\n      186.9\n      121\n      8.41\n      10.1\n      3\n      2.73\n      3\n      False\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      3328\n      AZ\n      192\n      415\n      414-4276\n      no\n      yes\n      36\n      156.2\n      77\n      26.55\n      215.5\n      126\n      18.32\n      279.1\n      83\n      12.56\n      9.9\n      6\n      2.67\n      2\n      False\n    \n    \n      3329\n      WV\n      68\n      415\n      370-3271\n      no\n      no\n      0\n      231.1\n      57\n      39.29\n      153.4\n      55\n      13.04\n      191.3\n      123\n      8.61\n      9.6\n      4\n      2.59\n      3\n      False\n    \n    \n      3330\n      RI\n      28\n      510\n      328-8230\n      no\n      no\n      0\n      180.8\n      109\n      30.74\n      288.8\n      58\n      24.55\n      191.9\n      91\n      8.64\n      14.1\n      6\n      3.81\n      2\n      False\n    \n    \n      3331\n      CT\n      184\n      510\n      364-6381\n      yes\n      no\n      0\n      213.8\n      105\n      36.35\n      159.6\n      84\n      13.57\n      139.2\n      137\n      6.26\n      5.0\n      10\n      1.35\n      2\n      False\n    \n    \n      3332\n      TN\n      74\n      415\n      400-4344\n      no\n      yes\n      25\n      234.4\n      113\n      39.85\n      265.9\n      82\n      22.60\n      241.4\n      77\n      10.86\n      13.7\n      4\n      3.70\n      0\n      False\n    \n  \n\n3333 rows × 21 columns\n\n\n\n\n#telecom_churn[\"area_code\"] = telecom_churn[\"area_code\"].astype('category')\n\n\nprofile = ProfileReport(telecom_churn, title = \"Telecom Churn Report\")\n\n\n# create report for EDA\nprofile.to_widgets()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#save the profile report\nprofile.to_file(\"telecom_churn_eda.html\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n[telecom_churn.churn.value_counts()]\n\n[False    2850\n True      483\n Name: churn, dtype: int64]\n\n\n\npd.crosstab(telecom_churn.churn, telecom_churn. customer_service_calls,margins=True, margins_name=\"Total\")\n\n\n\n\n\n  \n    \n      customer_service_calls\n      0\n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n      9\n      Total\n    \n    \n      churn\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      False\n      605\n      1059\n      672\n      385\n      90\n      26\n      8\n      4\n      1\n      0\n      2850\n    \n    \n      True\n      92\n      122\n      87\n      44\n      76\n      40\n      14\n      5\n      1\n      2\n      483\n    \n    \n      Total\n      697\n      1181\n      759\n      429\n      166\n      66\n      22\n      9\n      2\n      2\n      3333\n    \n  \n\n\n\n\n\nimport matplotlib.pyplot as plt\nct = pd.crosstab(telecom_churn.churn, telecom_churn.customer_service_calls)\nct.plot.bar(stacked=True)\nplt.legend(title='churn vs Number of calls')\nplt.show()\n\n\n\n\n\ntelecom_churn['area_code'].value_counts().plot.bar()\n\n<AxesSubplot:>\n\n\n\n\n\n\ntelecom_churn['state'].value_counts().head(10).plot.bar()\n\n<AxesSubplot:>\n\n\n\n\n\n\ntelecom_churn['international_plan'].value_counts().plot.bar()\n\n<AxesSubplot:>\n\n\n\n\n\n\ntelecom_churn['voice_mail_plan'].value_counts().plot.bar()\n\n<AxesSubplot:>\n\n\n\n\n\n\n\nObservations from EDA are:-\n\nDataset is imbalanced - 85.5% customers did not churn and 14.5% customers churned\nState consist of 51 distinct values with high cardinality\nNumeric variables are in different ranges and needs to be scaled\nThree distinct area codes - Area code ‘415’ is 49.7%, rest two area codes are equally distributed\nDistinct values of phone number is equal to the length of the dataset. This will be equivalent to primary key of the dataset. Not be included in modelling\n72.3% customers did not activate their voicemail plan. This is verified by equal number of customers with zero number of voice messages\nTotal international calls and customer service calls data is skewed. This is verified by high kurtosis and skewness.\nAll the other numeric variables are following normal distribution as verified by kurtosis / skewness values and histogram\n\n\n\nSplit the Data for Training and Testing\nThe Machine learning algorithm should not be exposed to test data. The performance of the learning algorithm can only be measured by testing on unseen data. To achieve the same a train and test split with 95% and 5% is created. It is ensured that the sampling is stratified so that the proportion of churn and not churn customers are equal in train and test data. As the amount of data is very less, only 5% of the data is kept aside for testing.Further the train data is further split into train and validation set with 90% and 10%. Validation set is required for hyperparameter tuning.As validation set is also exposed to training algorithm, it is also should not be used for model validation. Model validation is done on test set only.\n\n# convert the target value to integers. \ntelecom_churn['churn'] = telecom_churn['churn'] * 1\n\n\ntrain, test = train_test_split(telecom_churn, test_size = 0.05, stratify = telecom_churn['churn']) \nprint('Data for Modeling: ' + str(train.shape))\nprint('Unseen Data For Predictions: ' + str(test.shape))\n\nData for Modeling: (3166, 21)\nUnseen Data For Predictions: (167, 21)\n\n\n\n# Test the proportion of churn in train and test sets\ntrain.churn.value_counts()\n\n0    2707\n1     459\nName: churn, dtype: int64\n\n\n\n# 16.5% of the customers churned in train data\n(459/2707)*100\n\n16.956039896564462\n\n\n\ntest.churn.value_counts()\n\n0    143\n1     24\nName: churn, dtype: int64\n\n\n\n# 16.7% of the customers churned in test data\n(24/143)*100\n# customers churned proportionally from train and test data\n\n16.783216783216783\n\n\n\n\nModelling with Pycaret\n\nTrain and validation sets are created with 90 % and 10 % data.\nThe random seed selected for the modeling is 786\nIn this step we are normalizing the data, ignoring the variable ‘phone number’ for analysis\nFixing the imbalance in the data using SMOTE method\nWe are transforming the features - Changing the distribution of variables to a normal or approximate normal distribution\nIgnorning features with low variance - This will ignore variables (multi level categorical) where a single level dominates and there is not much variation in the information provided by the feature\nThe setup is inferring the customer_service_calls as numeric (as there are only ten distinct values). Hence explicitly mentioning it as numeric\n\n\nexp_clf =    setup(data = train, target = 'churn', session_id = 786, \n                   train_size = 0.9,\n                   normalize = True,\n                   transformation = True,\n                   ignore_low_variance = True,               \n                   ignore_features = ['phone_number'],\n                   fix_imbalance = True,\n                   high_cardinality_features = ['state'],\n                   numeric_features = ['customer_service_calls'])               \n\nSetup Succesfully Completed!\n\n\n\n                    Description        Value    \n                \n                        0\n                        session_id\n                        786\n            \n            \n                        1\n                        Target Type\n                        Binary\n            \n            \n                        2\n                        Label Encoded\n                        None\n            \n            \n                        3\n                        Original Data\n                        (3166, 21)\n            \n            \n                        4\n                        Missing Values \n                        False\n            \n            \n                        5\n                        Numeric Features \n                        15\n            \n            \n                        6\n                        Categorical Features \n                        5\n            \n            \n                        7\n                        Ordinal Features \n                        False\n            \n            \n                        8\n                        High Cardinality Features \n                        True\n            \n            \n                        9\n                        High Cardinality Method \n                        frequency\n            \n            \n                        10\n                        Sampled Data\n                        (3166, 21)\n            \n            \n                        11\n                        Transformed Train Set\n                        (2849, 22)\n            \n            \n                        12\n                        Transformed Test Set\n                        (317, 22)\n            \n            \n                        13\n                        Numeric Imputer \n                        mean\n            \n            \n                        14\n                        Categorical Imputer \n                        constant\n            \n            \n                        15\n                        Normalize \n                        True\n            \n            \n                        16\n                        Normalize Method \n                        zscore\n            \n            \n                        17\n                        Transformation \n                        True\n            \n            \n                        18\n                        Transformation Method \n                        yeo-johnson\n            \n            \n                        19\n                        PCA \n                        False\n            \n            \n                        20\n                        PCA Method \n                        None\n            \n            \n                        21\n                        PCA Components \n                        None\n            \n            \n                        22\n                        Ignore Low Variance \n                        True\n            \n            \n                        23\n                        Combine Rare Levels \n                        False\n            \n            \n                        24\n                        Rare Level Threshold \n                        None\n            \n            \n                        25\n                        Numeric Binning \n                        False\n            \n            \n                        26\n                        Remove Outliers \n                        False\n            \n            \n                        27\n                        Outliers Threshold \n                        None\n            \n            \n                        28\n                        Remove Multicollinearity \n                        False\n            \n            \n                        29\n                        Multicollinearity Threshold \n                        None\n            \n            \n                        30\n                        Clustering \n                        False\n            \n            \n                        31\n                        Clustering Iteration \n                        None\n            \n            \n                        32\n                        Polynomial Features \n                        False\n            \n            \n                        33\n                        Polynomial Degree \n                        None\n            \n            \n                        34\n                        Trignometry Features \n                        False\n            \n            \n                        35\n                        Polynomial Threshold \n                        None\n            \n            \n                        36\n                        Group Features \n                        False\n            \n            \n                        37\n                        Feature Selection \n                        False\n            \n            \n                        38\n                        Features Selection Threshold \n                        None\n            \n            \n                        39\n                        Feature Interaction \n                        False\n            \n            \n                        40\n                        Feature Ratio \n                        False\n            \n            \n                        41\n                        Interaction Threshold \n                        None\n            \n            \n                        42\n                        Fix Imbalance\n                        True\n            \n            \n                        43\n                        Fix Imbalance Method\n                        SMOTE\n            \n    \n\n\n\ncompare_models(fold = 5)\n\n\n                    Model        Accuracy        AUC        Recall        Prec.        F1        Kappa        MCC        TT (Sec)    \n                \n                        0\n                        Light Gradient Boosting Machine\n                        0.9512\n                        0.9127\n                        0.7771\n                        0.8732\n                        0.8213\n                        0.7932\n                        0.7957\n                        0.1908\n            \n            \n                        1\n                        Extreme Gradient Boosting\n                        0.9488\n                        0.9145\n                        0.7675\n                        0.8652\n                        0.8124\n                        0.7829\n                        0.7854\n                        0.2130\n            \n            \n                        2\n                        CatBoost Classifier\n                        0.9466\n                        0.9140\n                        0.7577\n                        0.8583\n                        0.8041\n                        0.7734\n                        0.7759\n                        4.3314\n            \n            \n                        3\n                        Extra Trees Classifier\n                        0.9333\n                        0.9032\n                        0.6535\n                        0.8544\n                        0.7392\n                        0.7018\n                        0.7109\n                        0.1456\n            \n            \n                        4\n                        Random Forest Classifier\n                        0.9330\n                        0.9082\n                        0.7045\n                        0.8147\n                        0.7529\n                        0.7145\n                        0.7186\n                        0.0366\n            \n            \n                        5\n                        Gradient Boosting Classifier\n                        0.9308\n                        0.9078\n                        0.7674\n                        0.7618\n                        0.7635\n                        0.7230\n                        0.7238\n                        1.1674\n            \n            \n                        6\n                        Decision Tree Classifier\n                        0.8929\n                        0.8227\n                        0.7238\n                        0.6106\n                        0.6616\n                        0.5986\n                        0.6022\n                        0.0356\n            \n            \n                        7\n                        Ada Boost Classifier\n                        0.8645\n                        0.8464\n                        0.6003\n                        0.5325\n                        0.5625\n                        0.4829\n                        0.4853\n                        0.2996\n            \n            \n                        8\n                        Naive Bayes\n                        0.8284\n                        0.7894\n                        0.6439\n                        0.4430\n                        0.5233\n                        0.4237\n                        0.4355\n                        0.0028\n            \n            \n                        9\n                        K Neighbors Classifier\n                        0.7715\n                        0.7897\n                        0.6878\n                        0.3531\n                        0.4664\n                        0.3398\n                        0.3705\n                        0.0160\n            \n            \n                        10\n                        Logistic Regression\n                        0.7375\n                        0.7886\n                        0.6852\n                        0.3152\n                        0.4314\n                        0.2904\n                        0.3273\n                        0.0316\n            \n            \n                        11\n                        Ridge Classifier\n                        0.7371\n                        0.0000\n                        0.6852\n                        0.3149\n                        0.4311\n                        0.2899\n                        0.3270\n                        0.0082\n            \n            \n                        12\n                        Linear Discriminant Analysis\n                        0.7336\n                        0.7844\n                        0.6779\n                        0.3103\n                        0.4252\n                        0.2824\n                        0.3190\n                        0.0144\n            \n            \n                        13\n                        SVM - Linear Kernel\n                        0.7329\n                        0.0000\n                        0.6079\n                        0.3163\n                        0.4021\n                        0.2621\n                        0.2910\n                        0.0198\n            \n            \n                        14\n                        Quadratic Discriminant Analysis\n                        0.5907\n                        0.6505\n                        0.6373\n                        0.2089\n                        0.3106\n                        0.1201\n                        0.1596\n                        0.0110\n            \n    \n\n\nLGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n               importance_type='split', learning_rate=0.1, max_depth=-1,\n               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,\n               random_state=786, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)\n\n\n\n\nCreating the models for top performing algorithms based on Precision and AUC. Tree based models are performing well on this dataset\n\nlightgbm = create_model('lightgbm', fold =5)\n\n\n                    Accuracy        AUC        Recall        Prec.        F1        Kappa        MCC    \n                \n                        0\n                        0.9526\n                        0.9008\n                        0.8171\n                        0.8481\n                        0.8323\n                        0.8047\n                        0.8049\n            \n            \n                        1\n                        0.9561\n                        0.9271\n                        0.8193\n                        0.8718\n                        0.8447\n                        0.8192\n                        0.8197\n            \n            \n                        2\n                        0.9544\n                        0.9241\n                        0.8072\n                        0.8701\n                        0.8375\n                        0.8110\n                        0.8118\n            \n            \n                        3\n                        0.9544\n                        0.9278\n                        0.7470\n                        0.9254\n                        0.8267\n                        0.8007\n                        0.8068\n            \n            \n                        4\n                        0.9385\n                        0.8836\n                        0.6951\n                        0.8507\n                        0.7651\n                        0.7301\n                        0.7351\n            \n            \n                        Mean\n                        0.9512\n                        0.9127\n                        0.7771\n                        0.8732\n                        0.8213\n                        0.7932\n                        0.7957\n            \n            \n                        SD\n                        0.0065\n                        0.0176\n                        0.0488\n                        0.0278\n                        0.0287\n                        0.0321\n                        0.0307\n            \n    \n\n\n\ncatboost = create_model('catboost', fold =5)\n\n\n                    Accuracy        AUC        Recall        Prec.        F1        Kappa        MCC    \n                \n                        0\n                        0.9421\n                        0.9161\n                        0.7439\n                        0.8356\n                        0.7871\n                        0.7537\n                        0.7554\n            \n            \n                        1\n                        0.9456\n                        0.9254\n                        0.7711\n                        0.8421\n                        0.8050\n                        0.7735\n                        0.7745\n            \n            \n                        2\n                        0.9561\n                        0.9198\n                        0.8313\n                        0.8625\n                        0.8466\n                        0.8210\n                        0.8212\n            \n            \n                        3\n                        0.9544\n                        0.9314\n                        0.7470\n                        0.9254\n                        0.8267\n                        0.8007\n                        0.8068\n            \n            \n                        4\n                        0.9350\n                        0.8775\n                        0.6951\n                        0.8261\n                        0.7550\n                        0.7178\n                        0.7214\n            \n            \n                        Mean\n                        0.9466\n                        0.9140\n                        0.7577\n                        0.8583\n                        0.8041\n                        0.7734\n                        0.7759\n            \n            \n                        SD\n                        0.0078\n                        0.0190\n                        0.0443\n                        0.0356\n                        0.0317\n                        0.0360\n                        0.0358\n            \n    \n\n\n\nxgboost = create_model('xgboost', fold = 5)\n\n\n                    Accuracy        AUC        Recall        Prec.        F1        Kappa        MCC    \n                \n                        0\n                        0.9474\n                        0.9080\n                        0.8171\n                        0.8171\n                        0.8171\n                        0.7863\n                        0.7863\n            \n            \n                        1\n                        0.9561\n                        0.9270\n                        0.8072\n                        0.8816\n                        0.8428\n                        0.8173\n                        0.8184\n            \n            \n                        2\n                        0.9474\n                        0.9231\n                        0.7590\n                        0.8630\n                        0.8077\n                        0.7773\n                        0.7795\n            \n            \n                        3\n                        0.9509\n                        0.9393\n                        0.7470\n                        0.8986\n                        0.8158\n                        0.7877\n                        0.7922\n            \n            \n                        4\n                        0.9420\n                        0.8752\n                        0.7073\n                        0.8657\n                        0.7785\n                        0.7455\n                        0.7506\n            \n            \n                        Mean\n                        0.9488\n                        0.9145\n                        0.7675\n                        0.8652\n                        0.8124\n                        0.7829\n                        0.7854\n            \n            \n                        SD\n                        0.0047\n                        0.0220\n                        0.0404\n                        0.0272\n                        0.0206\n                        0.0230\n                        0.0218\n            \n    \n\n\n\nrf = create_model('rf', fold =5)\n\n\n                    Accuracy        AUC        Recall        Prec.        F1        Kappa        MCC    \n                \n                        0\n                        0.9298\n                        0.8986\n                        0.7317\n                        0.7692\n                        0.7500\n                        0.7092\n                        0.7095\n            \n            \n                        1\n                        0.9263\n                        0.9227\n                        0.7349\n                        0.7531\n                        0.7439\n                        0.7009\n                        0.7009\n            \n            \n                        2\n                        0.9281\n                        0.9091\n                        0.6867\n                        0.7917\n                        0.7355\n                        0.6941\n                        0.6965\n            \n            \n                        3\n                        0.9421\n                        0.9291\n                        0.7349\n                        0.8472\n                        0.7871\n                        0.7538\n                        0.7563\n            \n            \n                        4\n                        0.9385\n                        0.8817\n                        0.6341\n                        0.9123\n                        0.7482\n                        0.7145\n                        0.7298\n            \n            \n                        Mean\n                        0.9330\n                        0.9082\n                        0.7045\n                        0.8147\n                        0.7529\n                        0.7145\n                        0.7186\n            \n            \n                        SD\n                        0.0062\n                        0.0170\n                        0.0396\n                        0.0583\n                        0.0178\n                        0.0208\n                        0.0221\n            \n    \n\n\n\net = create_model('et', fold =5)\n\n\n                    Accuracy        AUC        Recall        Prec.        F1        Kappa        MCC    \n                \n                        0\n                        0.9175\n                        0.8940\n                        0.6463\n                        0.7465\n                        0.6928\n                        0.6455\n                        0.6477\n            \n            \n                        1\n                        0.9404\n                        0.9064\n                        0.6988\n                        0.8657\n                        0.7733\n                        0.7394\n                        0.7451\n            \n            \n                        2\n                        0.9404\n                        0.9096\n                        0.6747\n                        0.8889\n                        0.7671\n                        0.7337\n                        0.7428\n            \n            \n                        3\n                        0.9456\n                        0.9307\n                        0.6867\n                        0.9194\n                        0.7862\n                        0.7558\n                        0.7664\n            \n            \n                        4\n                        0.9227\n                        0.8754\n                        0.5610\n                        0.8519\n                        0.6765\n                        0.6347\n                        0.6525\n            \n            \n                        Mean\n                        0.9333\n                        0.9032\n                        0.6535\n                        0.8544\n                        0.7392\n                        0.7018\n                        0.7109\n            \n            \n                        SD\n                        0.0111\n                        0.0183\n                        0.0494\n                        0.0586\n                        0.0453\n                        0.0510\n                        0.0503\n            \n    \n\n\n\n\nTune the created models for selecting the best hyperparameters\n\ntuned_lightgbm = tune_model(lightgbm, optimize = 'F1', n_iter = 30)\n\n\n                    Accuracy        AUC        Recall        Prec.        F1        Kappa        MCC    \n                \n                        0\n                        0.9404\n                        0.9034\n                        0.7805\n                        0.8000\n                        0.7901\n                        0.7554\n                        0.7554\n            \n            \n                        1\n                        0.9439\n                        0.8894\n                        0.8049\n                        0.8049\n                        0.8049\n                        0.7721\n                        0.7721\n            \n            \n                        2\n                        0.9579\n                        0.9525\n                        0.8537\n                        0.8537\n                        0.8537\n                        0.8291\n                        0.8291\n            \n            \n                        3\n                        0.9649\n                        0.9000\n                        0.8049\n                        0.9429\n                        0.8684\n                        0.8483\n                        0.8519\n            \n            \n                        4\n                        0.9474\n                        0.9161\n                        0.8049\n                        0.8250\n                        0.8148\n                        0.7841\n                        0.7842\n            \n            \n                        5\n                        0.9614\n                        0.9198\n                        0.8049\n                        0.9167\n                        0.8571\n                        0.8349\n                        0.8373\n            \n            \n                        6\n                        0.9684\n                        0.9594\n                        0.8810\n                        0.9024\n                        0.8916\n                        0.8731\n                        0.8732\n            \n            \n                        7\n                        0.9649\n                        0.9162\n                        0.7619\n                        1.0000\n                        0.8649\n                        0.8451\n                        0.8554\n            \n            \n                        8\n                        0.9333\n                        0.8673\n                        0.7381\n                        0.7949\n                        0.7654\n                        0.7266\n                        0.7273\n            \n            \n                        9\n                        0.9437\n                        0.9092\n                        0.6829\n                        0.9032\n                        0.7778\n                        0.7462\n                        0.7558\n            \n            \n                        Mean\n                        0.9526\n                        0.9134\n                        0.7918\n                        0.8744\n                        0.8289\n                        0.8015\n                        0.8042\n            \n            \n                        SD\n                        0.0117\n                        0.0259\n                        0.0531\n                        0.0659\n                        0.0414\n                        0.0480\n                        0.0484\n            \n    \n\n\n\ntuned_catboost = tune_model(catboost, optimize = 'F1', n_iter = 30)\n\n\n                    Accuracy        AUC        Recall        Prec.        F1        Kappa        MCC    \n                \n                        0\n                        0.9368\n                        0.8919\n                        0.7317\n                        0.8108\n                        0.7692\n                        0.7328\n                        0.7341\n            \n            \n                        1\n                        0.9474\n                        0.9042\n                        0.7805\n                        0.8421\n                        0.8101\n                        0.7796\n                        0.7804\n            \n            \n                        2\n                        0.9509\n                        0.9510\n                        0.8049\n                        0.8462\n                        0.8250\n                        0.7964\n                        0.7968\n            \n            \n                        3\n                        0.9544\n                        0.8872\n                        0.7561\n                        0.9118\n                        0.8267\n                        0.8007\n                        0.8053\n            \n            \n                        4\n                        0.9474\n                        0.9258\n                        0.8049\n                        0.8250\n                        0.8148\n                        0.7841\n                        0.7842\n            \n            \n                        5\n                        0.9544\n                        0.9058\n                        0.8049\n                        0.8684\n                        0.8354\n                        0.8090\n                        0.8098\n            \n            \n                        6\n                        0.9579\n                        0.9607\n                        0.7619\n                        0.9412\n                        0.8421\n                        0.8181\n                        0.8242\n            \n            \n                        7\n                        0.9439\n                        0.9133\n                        0.6667\n                        0.9333\n                        0.7778\n                        0.7467\n                        0.7605\n            \n            \n                        8\n                        0.9193\n                        0.8823\n                        0.7143\n                        0.7317\n                        0.7229\n                        0.6757\n                        0.6757\n            \n            \n                        9\n                        0.9437\n                        0.8767\n                        0.7073\n                        0.8788\n                        0.7838\n                        0.7518\n                        0.7577\n            \n            \n                        Mean\n                        0.9456\n                        0.9099\n                        0.7533\n                        0.8589\n                        0.8008\n                        0.7695\n                        0.7729\n            \n            \n                        SD\n                        0.0106\n                        0.0270\n                        0.0452\n                        0.0597\n                        0.0351\n                        0.0411\n                        0.0414\n            \n    \n\n\n\ntuned_xgboost = tune_model(xgboost, optimize = 'F1', n_iter = 30)\n\n\n                    Accuracy        AUC        Recall        Prec.        F1        Kappa        MCC    \n                \n                        0\n                        0.9474\n                        0.8786\n                        0.7805\n                        0.8421\n                        0.8101\n                        0.7796\n                        0.7804\n            \n            \n                        1\n                        0.9474\n                        0.8737\n                        0.7805\n                        0.8421\n                        0.8101\n                        0.7796\n                        0.7804\n            \n            \n                        2\n                        0.9509\n                        0.9396\n                        0.8293\n                        0.8293\n                        0.8293\n                        0.8006\n                        0.8006\n            \n            \n                        3\n                        0.9684\n                        0.9017\n                        0.8049\n                        0.9706\n                        0.8800\n                        0.8620\n                        0.8670\n            \n            \n                        4\n                        0.9509\n                        0.9386\n                        0.7805\n                        0.8649\n                        0.8205\n                        0.7921\n                        0.7935\n            \n            \n                        5\n                        0.9544\n                        0.9125\n                        0.7561\n                        0.9118\n                        0.8267\n                        0.8007\n                        0.8053\n            \n            \n                        6\n                        0.9439\n                        0.9686\n                        0.7381\n                        0.8611\n                        0.7949\n                        0.7626\n                        0.7656\n            \n            \n                        7\n                        0.9614\n                        0.9216\n                        0.7857\n                        0.9429\n                        0.8571\n                        0.8350\n                        0.8397\n            \n            \n                        8\n                        0.9263\n                        0.8634\n                        0.6905\n                        0.7838\n                        0.7342\n                        0.6916\n                        0.6935\n            \n            \n                        9\n                        0.9366\n                        0.9118\n                        0.6829\n                        0.8485\n                        0.7568\n                        0.7208\n                        0.7264\n            \n            \n                        Mean\n                        0.9487\n                        0.9110\n                        0.7629\n                        0.8697\n                        0.8120\n                        0.7825\n                        0.7852\n            \n            \n                        SD\n                        0.0113\n                        0.0313\n                        0.0446\n                        0.0533\n                        0.0408\n                        0.0472\n                        0.0476\n            \n    \n\n\n\ntuned_rf = tune_model(rf, optimize = 'F1', n_iter = 30)\n\n\n                    Accuracy        AUC        Recall        Prec.        F1        Kappa        MCC    \n                \n                        0\n                        0.9368\n                        0.8803\n                        0.7561\n                        0.7949\n                        0.7750\n                        0.7383\n                        0.7386\n            \n            \n                        1\n                        0.9368\n                        0.9106\n                        0.8049\n                        0.7674\n                        0.7857\n                        0.7487\n                        0.7490\n            \n            \n                        2\n                        0.9439\n                        0.9287\n                        0.8049\n                        0.8049\n                        0.8049\n                        0.7721\n                        0.7721\n            \n            \n                        3\n                        0.9614\n                        0.9113\n                        0.7805\n                        0.9412\n                        0.8533\n                        0.8313\n                        0.8362\n            \n            \n                        4\n                        0.9439\n                        0.9427\n                        0.8049\n                        0.8049\n                        0.8049\n                        0.7721\n                        0.7721\n            \n            \n                        5\n                        0.9368\n                        0.8999\n                        0.7561\n                        0.7949\n                        0.7750\n                        0.7383\n                        0.7386\n            \n            \n                        6\n                        0.9439\n                        0.9418\n                        0.7143\n                        0.8824\n                        0.7895\n                        0.7575\n                        0.7631\n            \n            \n                        7\n                        0.9509\n                        0.9113\n                        0.7143\n                        0.9375\n                        0.8108\n                        0.7832\n                        0.7927\n            \n            \n                        8\n                        0.9263\n                        0.8532\n                        0.6905\n                        0.7838\n                        0.7342\n                        0.6916\n                        0.6935\n            \n            \n                        9\n                        0.9437\n                        0.8886\n                        0.6829\n                        0.9032\n                        0.7778\n                        0.7462\n                        0.7558\n            \n            \n                        Mean\n                        0.9424\n                        0.9068\n                        0.7509\n                        0.8415\n                        0.7911\n                        0.7579\n                        0.7612\n            \n            \n                        SD\n                        0.0089\n                        0.0265\n                        0.0454\n                        0.0636\n                        0.0293\n                        0.0344\n                        0.0356\n            \n    \n\n\n\ntuned_et = tune_model(et, optimize = 'F1' , n_iter = 30)\n\n\n                    Accuracy        AUC        Recall        Prec.        F1        Kappa        MCC    \n                \n                        0\n                        0.9158\n                        0.8828\n                        0.6341\n                        0.7429\n                        0.6842\n                        0.6360\n                        0.6386\n            \n            \n                        1\n                        0.9298\n                        0.8971\n                        0.6829\n                        0.8000\n                        0.7368\n                        0.6966\n                        0.6996\n            \n            \n                        2\n                        0.9263\n                        0.9369\n                        0.6585\n                        0.7941\n                        0.7200\n                        0.6780\n                        0.6819\n            \n            \n                        3\n                        0.9509\n                        0.8955\n                        0.7317\n                        0.9091\n                        0.8108\n                        0.7830\n                        0.7891\n            \n            \n                        4\n                        0.9368\n                        0.9228\n                        0.6829\n                        0.8485\n                        0.7568\n                        0.7210\n                        0.7266\n            \n            \n                        5\n                        0.9439\n                        0.8852\n                        0.7073\n                        0.8788\n                        0.7838\n                        0.7520\n                        0.7578\n            \n            \n                        6\n                        0.9474\n                        0.9410\n                        0.7381\n                        0.8857\n                        0.8052\n                        0.7751\n                        0.7794\n            \n            \n                        7\n                        0.9439\n                        0.9394\n                        0.6429\n                        0.9643\n                        0.7714\n                        0.7409\n                        0.7607\n            \n            \n                        8\n                        0.9123\n                        0.8558\n                        0.5476\n                        0.7931\n                        0.6479\n                        0.5997\n                        0.6131\n            \n            \n                        9\n                        0.9120\n                        0.8913\n                        0.4878\n                        0.8333\n                        0.6154\n                        0.5695\n                        0.5956\n            \n            \n                        Mean\n                        0.9319\n                        0.9048\n                        0.6514\n                        0.8450\n                        0.7332\n                        0.6952\n                        0.7042\n            \n            \n                        SD\n                        0.0141\n                        0.0273\n                        0.0755\n                        0.0625\n                        0.0629\n                        0.0698\n                        0.0666\n            \n    \n\n\n\n\nCreate an Ensemble, Blended and Stack model to see the performance\n\ndt = create_model('dt' , fold = 5)\n\n\n                    Accuracy        AUC        Recall        Prec.        F1        Kappa        MCC    \n                \n                        0\n                        0.8895\n                        0.8289\n                        0.7439\n                        0.5922\n                        0.6595\n                        0.5945\n                        0.6000\n            \n            \n                        1\n                        0.8912\n                        0.8314\n                        0.7470\n                        0.6019\n                        0.6667\n                        0.6026\n                        0.6076\n            \n            \n                        2\n                        0.8877\n                        0.8094\n                        0.6988\n                        0.5979\n                        0.6444\n                        0.5783\n                        0.5807\n            \n            \n                        3\n                        0.9035\n                        0.8536\n                        0.7831\n                        0.6373\n                        0.7027\n                        0.6458\n                        0.6507\n            \n            \n                        4\n                        0.8928\n                        0.7903\n                        0.6463\n                        0.6235\n                        0.6347\n                        0.5719\n                        0.5721\n            \n            \n                        Mean\n                        0.8929\n                        0.8227\n                        0.7238\n                        0.6106\n                        0.6616\n                        0.5986\n                        0.6022\n            \n            \n                        SD\n                        0.0055\n                        0.0214\n                        0.0471\n                        0.0170\n                        0.0234\n                        0.0260\n                        0.0274\n            \n    \n\n\n\ntuned_dt = tune_model(dt, optimize = 'F1', n_iter = 30)\n\n\n                    Accuracy        AUC        Recall        Prec.        F1        Kappa        MCC    \n                \n                        0\n                        0.9018\n                        0.8782\n                        0.7805\n                        0.6275\n                        0.6957\n                        0.6379\n                        0.6433\n            \n            \n                        1\n                        0.8842\n                        0.9037\n                        0.8049\n                        0.5690\n                        0.6667\n                        0.5991\n                        0.6123\n            \n            \n                        2\n                        0.8982\n                        0.9072\n                        0.8780\n                        0.6000\n                        0.7129\n                        0.6537\n                        0.6712\n            \n            \n                        3\n                        0.9053\n                        0.8717\n                        0.7805\n                        0.6400\n                        0.7033\n                        0.6476\n                        0.6521\n            \n            \n                        4\n                        0.9193\n                        0.8988\n                        0.8049\n                        0.6875\n                        0.7416\n                        0.6941\n                        0.6971\n            \n            \n                        5\n                        0.9018\n                        0.9168\n                        0.8537\n                        0.6140\n                        0.7143\n                        0.6569\n                        0.6699\n            \n            \n                        6\n                        0.9439\n                        0.9188\n                        0.8333\n                        0.7955\n                        0.8140\n                        0.7809\n                        0.7812\n            \n            \n                        7\n                        0.9298\n                        0.8626\n                        0.7381\n                        0.7750\n                        0.7561\n                        0.7151\n                        0.7154\n            \n            \n                        8\n                        0.9053\n                        0.8125\n                        0.6667\n                        0.6829\n                        0.6747\n                        0.6193\n                        0.6193\n            \n            \n                        9\n                        0.9190\n                        0.8680\n                        0.7073\n                        0.7250\n                        0.7160\n                        0.6688\n                        0.6689\n            \n            \n                        Mean\n                        0.9108\n                        0.8838\n                        0.7848\n                        0.6716\n                        0.7195\n                        0.6673\n                        0.6731\n            \n            \n                        SD\n                        0.0164\n                        0.0307\n                        0.0623\n                        0.0715\n                        0.0406\n                        0.0494\n                        0.0469\n            \n    \n\n\n\nbagged_dt = ensemble_model(tuned_dt, n_estimators = 200, optimize = 'F1')\n\n\n                    Accuracy        AUC        Recall        Prec.        F1        Kappa        MCC    \n                \n                        0\n                        0.8807\n                        0.8698\n                        0.7805\n                        0.5614\n                        0.6531\n                        0.5833\n                        0.5949\n            \n            \n                        1\n                        0.9298\n                        0.9043\n                        0.8293\n                        0.7234\n                        0.7727\n                        0.7315\n                        0.7338\n            \n            \n                        2\n                        0.9053\n                        0.9175\n                        0.9024\n                        0.6167\n                        0.7327\n                        0.6776\n                        0.6957\n            \n            \n                        3\n                        0.9404\n                        0.9077\n                        0.8049\n                        0.7857\n                        0.7952\n                        0.7603\n                        0.7604\n            \n            \n                        4\n                        0.9298\n                        0.9073\n                        0.8537\n                        0.7143\n                        0.7778\n                        0.7365\n                        0.7406\n            \n            \n                        5\n                        0.9123\n                        0.9220\n                        0.8293\n                        0.6538\n                        0.7312\n                        0.6796\n                        0.6865\n            \n            \n                        6\n                        0.9509\n                        0.9436\n                        0.8810\n                        0.8043\n                        0.8409\n                        0.8119\n                        0.8131\n            \n            \n                        7\n                        0.9509\n                        0.9128\n                        0.7857\n                        0.8684\n                        0.8250\n                        0.7965\n                        0.7979\n            \n            \n                        8\n                        0.9088\n                        0.8489\n                        0.7381\n                        0.6739\n                        0.7045\n                        0.6507\n                        0.6517\n            \n            \n                        9\n                        0.9401\n                        0.8935\n                        0.7561\n                        0.8158\n                        0.7848\n                        0.7501\n                        0.7508\n            \n            \n                        Mean\n                        0.9249\n                        0.9027\n                        0.8161\n                        0.7218\n                        0.7618\n                        0.7178\n                        0.7225\n            \n            \n                        SD\n                        0.0215\n                        0.0254\n                        0.0504\n                        0.0922\n                        0.0540\n                        0.0664\n                        0.0632\n            \n    \n\n\n\nboosted_dt = ensemble_model(tuned_dt, method = 'Boosting', n_estimators = 50, optimize = 'F1')\n\n\n                    Accuracy        AUC        Recall        Prec.        F1        Kappa        MCC    \n                \n                        0\n                        0.8877\n                        0.8717\n                        0.5610\n                        0.6216\n                        0.5897\n                        0.5249\n                        0.5258\n            \n            \n                        1\n                        0.9123\n                        0.8352\n                        0.6341\n                        0.7222\n                        0.6753\n                        0.6249\n                        0.6266\n            \n            \n                        2\n                        0.9333\n                        0.9076\n                        0.7073\n                        0.8056\n                        0.7532\n                        0.7149\n                        0.7169\n            \n            \n                        3\n                        0.9404\n                        0.8308\n                        0.6829\n                        0.8750\n                        0.7671\n                        0.7335\n                        0.7409\n            \n            \n                        4\n                        0.9053\n                        0.8260\n                        0.6098\n                        0.6944\n                        0.6494\n                        0.5949\n                        0.5965\n            \n            \n                        5\n                        0.9333\n                        0.9336\n                        0.7317\n                        0.7895\n                        0.7595\n                        0.7209\n                        0.7216\n            \n            \n                        6\n                        0.9228\n                        0.8915\n                        0.6429\n                        0.7941\n                        0.7105\n                        0.6666\n                        0.6715\n            \n            \n                        7\n                        0.9018\n                        0.8379\n                        0.4524\n                        0.7917\n                        0.5758\n                        0.5248\n                        0.5512\n            \n            \n                        8\n                        0.9263\n                        0.8179\n                        0.6190\n                        0.8387\n                        0.7123\n                        0.6712\n                        0.6814\n            \n            \n                        9\n                        0.9085\n                        0.8018\n                        0.5610\n                        0.7419\n                        0.6389\n                        0.5876\n                        0.5952\n            \n            \n                        Mean\n                        0.9172\n                        0.8554\n                        0.6202\n                        0.7675\n                        0.6832\n                        0.6364\n                        0.6428\n            \n            \n                        SD\n                        0.0159\n                        0.0411\n                        0.0774\n                        0.0701\n                        0.0654\n                        0.0735\n                        0.0710\n            \n    \n\n\n\n# Train a voting classifier with all models in the library\nblender = blend_models()\n\n\n                    Accuracy        AUC        Recall        Prec.        F1        Kappa        MCC    \n                \n                        0\n                        0.8912\n                        0.0000\n                        0.7561\n                        0.5962\n                        0.6667\n                        0.6028\n                        0.6088\n            \n            \n                        1\n                        0.9404\n                        0.0000\n                        0.7805\n                        0.8000\n                        0.7901\n                        0.7554\n                        0.7554\n            \n            \n                        2\n                        0.9158\n                        0.0000\n                        0.6829\n                        0.7179\n                        0.7000\n                        0.6511\n                        0.6513\n            \n            \n                        3\n                        0.9474\n                        0.0000\n                        0.7805\n                        0.8421\n                        0.8101\n                        0.7796\n                        0.7804\n            \n            \n                        4\n                        0.9263\n                        0.0000\n                        0.7561\n                        0.7381\n                        0.7470\n                        0.7039\n                        0.7039\n            \n            \n                        5\n                        0.9158\n                        0.0000\n                        0.7073\n                        0.7073\n                        0.7073\n                        0.6581\n                        0.6581\n            \n            \n                        6\n                        0.9404\n                        0.0000\n                        0.7857\n                        0.8049\n                        0.7952\n                        0.7603\n                        0.7604\n            \n            \n                        7\n                        0.9474\n                        0.0000\n                        0.7619\n                        0.8649\n                        0.8101\n                        0.7797\n                        0.7818\n            \n            \n                        8\n                        0.9018\n                        0.0000\n                        0.7143\n                        0.6522\n                        0.6818\n                        0.6239\n                        0.6248\n            \n            \n                        9\n                        0.9190\n                        0.0000\n                        0.7317\n                        0.7143\n                        0.7229\n                        0.6755\n                        0.6755\n            \n            \n                        Mean\n                        0.9245\n                        0.0000\n                        0.7457\n                        0.7438\n                        0.7431\n                        0.6990\n                        0.7001\n            \n            \n                        SD\n                        0.0183\n                        0.0000\n                        0.0333\n                        0.0802\n                        0.0520\n                        0.0628\n                        0.0621\n            \n    \n\n\n\nblender_specific = blend_models(estimator_list = [tuned_lightgbm,tuned_xgboost,\n                                                 tuned_rf, tuned_et, tuned_dt], method = 'soft')\n\n\n                    Accuracy        AUC        Recall        Prec.        F1        Kappa        MCC    \n                \n                        0\n                        0.9404\n                        0.8833\n                        0.7805\n                        0.8000\n                        0.7901\n                        0.7554\n                        0.7554\n            \n            \n                        1\n                        0.9404\n                        0.9079\n                        0.7805\n                        0.8000\n                        0.7901\n                        0.7554\n                        0.7554\n            \n            \n                        2\n                        0.9474\n                        0.9345\n                        0.8293\n                        0.8095\n                        0.8193\n                        0.7885\n                        0.7886\n            \n            \n                        3\n                        0.9684\n                        0.8996\n                        0.8049\n                        0.9706\n                        0.8800\n                        0.8620\n                        0.8670\n            \n            \n                        4\n                        0.9509\n                        0.9361\n                        0.8049\n                        0.8462\n                        0.8250\n                        0.7964\n                        0.7968\n            \n            \n                        5\n                        0.9474\n                        0.9149\n                        0.8293\n                        0.8095\n                        0.8193\n                        0.7885\n                        0.7886\n            \n            \n                        6\n                        0.9544\n                        0.9541\n                        0.7857\n                        0.8919\n                        0.8354\n                        0.8091\n                        0.8113\n            \n            \n                        7\n                        0.9579\n                        0.9318\n                        0.7381\n                        0.9688\n                        0.8378\n                        0.8142\n                        0.8241\n            \n            \n                        8\n                        0.9298\n                        0.8511\n                        0.6905\n                        0.8056\n                        0.7436\n                        0.7032\n                        0.7060\n            \n            \n                        9\n                        0.9437\n                        0.8926\n                        0.7073\n                        0.8788\n                        0.7838\n                        0.7518\n                        0.7577\n            \n            \n                        Mean\n                        0.9481\n                        0.9106\n                        0.7751\n                        0.8581\n                        0.8124\n                        0.7824\n                        0.7851\n            \n            \n                        SD\n                        0.0102\n                        0.0289\n                        0.0458\n                        0.0639\n                        0.0354\n                        0.0412\n                        0.0422\n            \n    \n\n\n\nstacked_models = stack_models(estimator_list = [tuned_lightgbm,tuned_catboost,tuned_xgboost,\n                                                 tuned_rf, tuned_et, tuned_dt], meta_model = None, optimize = 'F1', fold = 5)\n\n\n                    Accuracy        AUC        Recall        Prec.        F1        Kappa        MCC    \n                \n                        0\n                        0.9368\n                        0.9035\n                        0.8293\n                        0.7556\n                        0.7907\n                        0.7536\n                        0.7547\n            \n            \n                        1\n                        0.9579\n                        0.9200\n                        0.8434\n                        0.8642\n                        0.8537\n                        0.8291\n                        0.8292\n            \n            \n                        2\n                        0.9509\n                        0.9184\n                        0.8434\n                        0.8235\n                        0.8333\n                        0.8045\n                        0.8046\n            \n            \n                        3\n                        0.9614\n                        0.9347\n                        0.8434\n                        0.8861\n                        0.8642\n                        0.8417\n                        0.8421\n            \n            \n                        4\n                        0.9315\n                        0.8787\n                        0.7561\n                        0.7654\n                        0.7607\n                        0.7207\n                        0.7208\n            \n            \n                        Mean\n                        0.9477\n                        0.9111\n                        0.8231\n                        0.8190\n                        0.8205\n                        0.7899\n                        0.7903\n            \n            \n                        SD\n                        0.0117\n                        0.0189\n                        0.0339\n                        0.0519\n                        0.0391\n                        0.0459\n                        0.0458\n            \n    \n\n\n\n\nOut of all the models created tuned_lightgbm is performing better on validation data\n\nevaluate_model(tuned_lightgbm)\n\n\n\n\n\n\nTest the model on the test data and choose the best performing model\n\n\nEvaluate Model\n\n# create funtion to return evaluation metrics\ndef evaluation_metrics(model):\n    check_model = predict_model(model, data = test)\n    print(metrics.confusion_matrix(check_model.churn,check_model.Label))\n    tn, fp, fn, tp = metrics.confusion_matrix(check_model.churn,check_model.Label).ravel()\n    Accuracy = round((tp+tn)/(tp+tn+fp+fn),3)\n    precision = round(tp/(tp+fp),3)\n    specificity = round(tn/(tn+fp),3)\n    recall = round(tp/(tp+fn),3)\n    print( f\"Accuracy:{Accuracy} , Specificity:{specificity}, Precision:{precision} , Recall:{recall}\")\n\n\ncheck tuned_lightgbm\n\nevaluation_metrics(tuned_lightgbm)\n\n[[142   1]\n [  5  19]]\nAccuracy:0.964 , Specificity:0.993, Precision:0.95 , Recall:0.792\n\n\n\n\ncheck tuned_catboost\n\nevaluation_metrics(tuned_catboost)\n\n[[141   2]\n [  5  19]]\nAccuracy:0.958 , Specificity:0.986, Precision:0.905 , Recall:0.792\n\n\n\n\ncheck tuned_xgboost\n\nevaluation_metrics(tuned_xgboost)\n\n[[141   2]\n [  5  19]]\nAccuracy:0.958 , Specificity:0.986, Precision:0.905 , Recall:0.792\n\n\n\n\ncheck tuned_rf\n\nevaluation_metrics(tuned_rf)\n\n[[141   2]\n [  6  18]]\nAccuracy:0.952 , Specificity:0.986, Precision:0.9 , Recall:0.75\n\n\n\n\ncheck tuned_et\n\nevaluation_metrics(tuned_et)\n\n[[143   0]\n [ 11  13]]\nAccuracy:0.934 , Specificity:1.0, Precision:1.0 , Recall:0.542\n\n\n\n\ncheck tuned_dt\n\nevaluation_metrics(tuned_dt)\n\n[[141   2]\n [  6  18]]\nAccuracy:0.952 , Specificity:0.986, Precision:0.9 , Recall:0.75\n\n\n\n\ncheck boosted_dt\n\nevaluation_metrics(boosted_dt)\n\n[[141   2]\n [ 10  14]]\nAccuracy:0.928 , Specificity:0.986, Precision:0.875 , Recall:0.583\n\n\n\n\ncheck bagged_dt\n\nevaluation_metrics(bagged_dt)\n\n[[139   4]\n [  6  18]]\nAccuracy:0.94 , Specificity:0.972, Precision:0.818 , Recall:0.75\n\n\n\n\ncheck blender\n\nevaluation_metrics(blender)\n\n[[143   0]\n [ 11  13]]\nAccuracy:0.934 , Specificity:1.0, Precision:1.0 , Recall:0.542\n\n\n\n\ncheck blender_specific\n\nevaluation_metrics(blender_specific)\n\n[[142   1]\n [  5  19]]\nAccuracy:0.964 , Specificity:0.993, Precision:0.95 , Recall:0.792\n\n\n\n\ncheck stacked_models\n\nevaluation_metrics(stacked_models)\n\n[[139   4]\n [  4  20]]\nAccuracy:0.952 , Specificity:0.972, Precision:0.833 , Recall:0.833\n\n\n\n\n\nFinalizing the Model and Metrics\n\nCompared multiple models to examine which algorithm is suitable for this dataset\nChose the five best performing algorithms and created models for them\nHyper parameter tuning was done to further improve the model performance\nEnsemble of models were created to check their performance on test data\nAll the tuned and esemble models were tested out on unseen data to finalize a model\n\n\n\nModel finalization\nMy recommendation for the final model is tuned_lightgbm. This is because the models predictions for churned customers is very high. From my experience in media Industry, it was observed that business users usually request for model explainability. This model’s Recall is slightly lower than stacked_models. stacked_model was not selected because it does not provide model interpretation. Also given similar performance it is better to go for simpler model.\n\n# Finalize the model\nfinal_model = finalize_model(tuned_lightgbm)\n\n\n# Feature importance using decision tree models\nplot_model(final_model, plot = 'feature')\n\n\n\n\n\n# Feature importance using Shap\ninterpret_model(final_model, plot = 'summary')\n\n\n\n\n\n# local interpretation\ninterpret_model(final_model, plot = 'reason', observation = 14)\n\n\n\n\n\n\n\n\n  Visualization omitted, Javascript library not loaded!\n  Have you run `initjs()` in this notebook? If this notebook was from another\n  user you must also trust this notebook (File -> Trust notebook). If you are viewing\n  this notebook on github the Javascript has been stripped for security. If you are using\n  JupyterLab this error is because a JupyterLab extension has not yet been written.\n\n \n\n\n\n# save the model\nsave_model(final_model,'tuned_lightgbm_dt_save_20201017')\n\nTransformation Pipeline and Model Succesfully Saved\n\n\n\n# load the model\nloaded_model = load_model('bagged_dt_save_20201017')\n\nTransformation Pipeline and Model Sucessfully Loaded\n\n\n\n\nPotential issues with deploying the model into production are:-\n\nData and model versioning As the amount of data is increasing every day, a mechanism to version data along with code needs to be established. This needs to be done without any cost overhead for storing multiple copies of the same data.\nTracking and storing of experiment results and artifacts efficiently. Data scientist should be able to tell which version of model is presently in production, data used for training, what are the evaluation metrics of the model at any given time.\nMonitoring of models in production for data drift - The behaviour of incoming data may change and will may differ from the data on which it was trained\nTaking care of CI/CD in production - As soon as a better performing model is finalized and commited, it should go to production in an automated fashion\nInstead of full-deployment of the models - Canary or Blue-Green deployment should be done. If model should be exposed to 10% to 15% of the population. If it performs well on a small population, then it should be rolled out for everyone.\nFeedback loops - The data used by the model for prediction going again into the training set\n\nThe following tools which can be used for the production issues mentioned above:- 1. Data and model versioning - Data Version Control (DVC) and MLOps by DVC 2. Tracking experiments - mlflow python package 3. Monitoring of models in production - Bi tools 4. Containerization - Docker and Kubernetes 5. CI/CD - Github, CircleCI, MLops 6. Deployment - Seldon core, Heroku 7. Canary / Bluegreen Deployment - AWS Sagemaker\n\n\nAppendix\nIf the business priorirty is to predict both churners and non-churners accurately then Accuracy. If the business priority is to identify churners then precision and recall. If the business priority is to predict non-churners (which is a very rare scenario) then it is specificity. The final model will be chosen as per business priority. If the business priority is :- 1. Predicting both churn and non-churning customers accurately then the model with highest accuracy will be chosen i.e - tuned_dt 2. Maximizing the proportion of churner identifications which are actually correct, then model with highest precision - tuned_dt 3. Identifying the Maximum proportion of actual churners then model with highest recall - bagged_dt (as this is simpler than blender_specific)\n\n# Data for DOE\ndoe = predict_model(loaded_model, data = telecom_churn)\nprint(metrics.confusion_matrix(doe.churn,doe.Label))\n\n[[2821   29]\n [ 135  348]]\n\n\n\ntn, fp, fn, tp = metrics.confusion_matrix(doe.churn,doe.Label).ravel()\nAccuracy = round((tp+tn)/(tp+tn+fp+fn),3)\nprecision = round(tp/(tp+fp),3)\nspecificity = round(tn/(tn+fp),3)\nrecall = round(tp/(tp+fn),3)\nprint( f\"Accuracy:{Accuracy} , Specificity:{specificity}, Precision:{precision} , Recall:{recall}\")\n\nAccuracy:0.951 , Specificity:0.99, Precision:0.923 , Recall:0.72\n\n\n\ndoe.shape\n\n(3333, 23)\n\n\n\ndoe.churn.value_counts()\n\n0    2850\n1     483\nName: churn, dtype: int64\n\n\n\n93/483\n\n0.19254658385093168"
  },
  {
    "objectID": "telecom_churn_prediction/telecom_churn_eda.html",
    "href": "telecom_churn_prediction/telecom_churn_eda.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "EDA on Telecom Churn Data\nThe objectives of this project are:-\n1. Perform exploratory analysis and extract insights from the dataset.\n2. Split the dataset into train/test sets and explain your reasoning.\n3. Build a predictive model to predict which customers are going to churn and discuss the reason why you choose a particular algorithm.\n4. Establish metrics to evaluate model performance.\n5. Discuss the potential issues with deploying the model into production\n\nImport the required libraries\n\n# python version # 3.8.2\nimport pandas as pd \nimport numpy as np \nimport os \nfrom pandas_profiling import ProfileReport\n\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n# option to display all columns\npd.set_option('display.max_columns', None)\n\n\n# Read the data\ntelecom_churn = pd.read_csv('../data/telecom_data/telecom.csv')\n\n\ntelecom_churn.head(10)\n\n\n\n\n\n  \n    \n      \n      state\n      account length\n      area code\n      phone number\n      international plan\n      voice mail plan\n      number vmail messages\n      total day minutes\n      total day calls\n      total day charge\n      total eve minutes\n      total eve calls\n      total eve charge\n      total night minutes\n      total night calls\n      total night charge\n      total intl minutes\n      total intl calls\n      total intl charge\n      customer service calls\n      churn\n    \n  \n  \n    \n      0\n      KS\n      128\n      415\n      382-4657\n      no\n      yes\n      25\n      265.1\n      110\n      45.07\n      197.4\n      99\n      16.78\n      244.7\n      91\n      11.01\n      10.0\n      3\n      2.70\n      1\n      False\n    \n    \n      1\n      OH\n      107\n      415\n      371-7191\n      no\n      yes\n      26\n      161.6\n      123\n      27.47\n      195.5\n      103\n      16.62\n      254.4\n      103\n      11.45\n      13.7\n      3\n      3.70\n      1\n      False\n    \n    \n      2\n      NJ\n      137\n      415\n      358-1921\n      no\n      no\n      0\n      243.4\n      114\n      41.38\n      121.2\n      110\n      10.30\n      162.6\n      104\n      7.32\n      12.2\n      5\n      3.29\n      0\n      False\n    \n    \n      3\n      OH\n      84\n      408\n      375-9999\n      yes\n      no\n      0\n      299.4\n      71\n      50.90\n      61.9\n      88\n      5.26\n      196.9\n      89\n      8.86\n      6.6\n      7\n      1.78\n      2\n      False\n    \n    \n      4\n      OK\n      75\n      415\n      330-6626\n      yes\n      no\n      0\n      166.7\n      113\n      28.34\n      148.3\n      122\n      12.61\n      186.9\n      121\n      8.41\n      10.1\n      3\n      2.73\n      3\n      False\n    \n    \n      5\n      AL\n      118\n      510\n      391-8027\n      yes\n      no\n      0\n      223.4\n      98\n      37.98\n      220.6\n      101\n      18.75\n      203.9\n      118\n      9.18\n      6.3\n      6\n      1.70\n      0\n      False\n    \n    \n      6\n      MA\n      121\n      510\n      355-9993\n      no\n      yes\n      24\n      218.2\n      88\n      37.09\n      348.5\n      108\n      29.62\n      212.6\n      118\n      9.57\n      7.5\n      7\n      2.03\n      3\n      False\n    \n    \n      7\n      MO\n      147\n      415\n      329-9001\n      yes\n      no\n      0\n      157.0\n      79\n      26.69\n      103.1\n      94\n      8.76\n      211.8\n      96\n      9.53\n      7.1\n      6\n      1.92\n      0\n      False\n    \n    \n      8\n      LA\n      117\n      408\n      335-4719\n      no\n      no\n      0\n      184.5\n      97\n      31.37\n      351.6\n      80\n      29.89\n      215.8\n      90\n      9.71\n      8.7\n      4\n      2.35\n      1\n      False\n    \n    \n      9\n      WV\n      141\n      415\n      330-8173\n      yes\n      yes\n      37\n      258.6\n      84\n      43.96\n      222.0\n      111\n      18.87\n      326.4\n      97\n      14.69\n      11.2\n      5\n      3.02\n      0\n      False\n    \n  \n\n\n\n\n\n\nCheck the Shape and Column types of the Dataframe\n\ntelecom_churn.shape\n\n(3333, 21)\n\n\n\ntelecom_churn.dtypes\n\nstate                      object\naccount length              int64\narea code                   int64\nphone number               object\ninternational plan         object\nvoice mail plan            object\nnumber vmail messages       int64\ntotal day minutes         float64\ntotal day calls             int64\ntotal day charge          float64\ntotal eve minutes         float64\ntotal eve calls             int64\ntotal eve charge          float64\ntotal night minutes       float64\ntotal night calls           int64\ntotal night charge        float64\ntotal intl minutes        float64\ntotal intl calls            int64\ntotal intl charge         float64\ncustomer service calls      int64\nchurn                        bool\ndtype: object\n\n\n\n\nExploratory Analysis\n\n# Format the column names, remove space and special characters in column names\ntelecom_churn.columns =  telecom_churn.columns.str.strip().str.lower().str.replace(' ', '_').str.replace('(', '').str.replace(')', '')\n\n\ntelecom_churn\n\n\n\n\n\n  \n    \n      \n      state\n      account_length\n      area_code\n      phone_number\n      international_plan\n      voice_mail_plan\n      number_vmail_messages\n      total_day_minutes\n      total_day_calls\n      total_day_charge\n      total_eve_minutes\n      total_eve_calls\n      total_eve_charge\n      total_night_minutes\n      total_night_calls\n      total_night_charge\n      total_intl_minutes\n      total_intl_calls\n      total_intl_charge\n      customer_service_calls\n      churn\n    \n  \n  \n    \n      0\n      KS\n      128\n      415\n      382-4657\n      no\n      yes\n      25\n      265.1\n      110\n      45.07\n      197.4\n      99\n      16.78\n      244.7\n      91\n      11.01\n      10.0\n      3\n      2.70\n      1\n      False\n    \n    \n      1\n      OH\n      107\n      415\n      371-7191\n      no\n      yes\n      26\n      161.6\n      123\n      27.47\n      195.5\n      103\n      16.62\n      254.4\n      103\n      11.45\n      13.7\n      3\n      3.70\n      1\n      False\n    \n    \n      2\n      NJ\n      137\n      415\n      358-1921\n      no\n      no\n      0\n      243.4\n      114\n      41.38\n      121.2\n      110\n      10.30\n      162.6\n      104\n      7.32\n      12.2\n      5\n      3.29\n      0\n      False\n    \n    \n      3\n      OH\n      84\n      408\n      375-9999\n      yes\n      no\n      0\n      299.4\n      71\n      50.90\n      61.9\n      88\n      5.26\n      196.9\n      89\n      8.86\n      6.6\n      7\n      1.78\n      2\n      False\n    \n    \n      4\n      OK\n      75\n      415\n      330-6626\n      yes\n      no\n      0\n      166.7\n      113\n      28.34\n      148.3\n      122\n      12.61\n      186.9\n      121\n      8.41\n      10.1\n      3\n      2.73\n      3\n      False\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      3328\n      AZ\n      192\n      415\n      414-4276\n      no\n      yes\n      36\n      156.2\n      77\n      26.55\n      215.5\n      126\n      18.32\n      279.1\n      83\n      12.56\n      9.9\n      6\n      2.67\n      2\n      False\n    \n    \n      3329\n      WV\n      68\n      415\n      370-3271\n      no\n      no\n      0\n      231.1\n      57\n      39.29\n      153.4\n      55\n      13.04\n      191.3\n      123\n      8.61\n      9.6\n      4\n      2.59\n      3\n      False\n    \n    \n      3330\n      RI\n      28\n      510\n      328-8230\n      no\n      no\n      0\n      180.8\n      109\n      30.74\n      288.8\n      58\n      24.55\n      191.9\n      91\n      8.64\n      14.1\n      6\n      3.81\n      2\n      False\n    \n    \n      3331\n      CT\n      184\n      510\n      364-6381\n      yes\n      no\n      0\n      213.8\n      105\n      36.35\n      159.6\n      84\n      13.57\n      139.2\n      137\n      6.26\n      5.0\n      10\n      1.35\n      2\n      False\n    \n    \n      3332\n      TN\n      74\n      415\n      400-4344\n      no\n      yes\n      25\n      234.4\n      113\n      39.85\n      265.9\n      82\n      22.60\n      241.4\n      77\n      10.86\n      13.7\n      4\n      3.70\n      0\n      False\n    \n  \n\n3333 rows × 21 columns\n\n\n\n\nprofile = ProfileReport(telecom_churn, title = \"Telecom Churn Report\")\n\n\nprofile.to_notebook_iframe()"
  },
  {
    "objectID": "scaled/scaled_req.html",
    "href": "scaled/scaled_req.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "What I can highlight\n\ncomputer vision - Moderation and Tagging\nML algorithms - Churn prediction model\nExtract insights - Dashboarding\nDevelop and maintain ml pipelines - Data lake project\n5+ years experience\nDeployed 100’s of look-alike models\nWorked with Tensorflow, Pytorch libraries\nStrong experience in AWS and Sagemaker\nResults-driven\nTeam player and more than 5 years of startup experience\nRead\nwhat is equivariant data - GNN, Pointsets?\n\nQuestions * Can you tell me about the team I’ll be working with? what is the structure of platform team, who are all members of this team? * What’s your favorite part about working here? * At what stage of development is this product? Is it Generally available to everyone? * Are there plans to launch this product in India? * What will be the interview process? what are the timelines for the same? * Can I answer any final questions for you? * Is there an option to migrate to Barcelona if selected? * Any plans to introduce new products or plans for growth?\n\nDeep learning models for equivariant data: GNNs, Pointsets\nComputer vision, 3D vision\nTensorflow, Keras, Pytorch\nPython\nAWS - Sagemaker, ETL with AWS Glue, AWS Lambdas\nCUDA programming - Non linear optimization, Bayesian statistics\nAutomation solutions\nStartup experience\n\nLink to JD"
  },
  {
    "objectID": "scaled/lti_mindtree.html",
    "href": "scaled/lti_mindtree.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "What I can highlight\n\nDatabricks and AWS sagemaker - Data lake & MAAS\nML Piplines - Deploying MAAS\nNLP Techniques - Getting summary for a video and using it to find similar videos and using it for search, ISS case study\nML algorithms - Churn prediction modelling, Recommendation system, Content moderation etc\nOperations research - Highlight operations experience - Supply chain managment etc.\nBI - Dashboards for ZEE5\n\nQuestions * what are the projects on which I will be working? * Can you tell me about the role and the team I’ll be working with? * What is the location for this role? * What will be the interview process? what are the timelines for the same? * Can I answer any final questions for you?"
  },
  {
    "objectID": "data_privacy/basic_privacy_approaches.html",
    "href": "data_privacy/basic_privacy_approaches.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Data which could be used separately or in combination with other information to identify a person or small group of persons\n\nPersonally Identifiable Information (PII)\nPerson-Related data\nProprietary and Confidential data\n\n\n\n\nIdentifying PII in images and text\n\n\n\n\nWe can have a classification system for the data.\nUse classification as an initial step for documentation\nToolkit for documenting Data - Data Cards\nFramework for documenting Models - Model Cards\nTool for Data Management\n\nDocumenting data Collection\nDocumenting Data Quality\nDocumenting Data Security\nDocumenting Data Privacy\nDocumenting Data Descriptions\nDocumenting Data Statistics\nDocumenting consent\n\nTrack Data Lineage\nData version control\n\n\n\n\n\n\n\nIt is a technique that allows us to use “Pseudonyms” instead of real names and data\n\n\n\n\npseudonymization approaches\n\n\n\n\n\nHow a linkage attack works\n\n\n\nLinking is a primary attack vector to determine the identity of an individual.\n\n\n\n\nAdvantages and Disadvantages of Pseudonymization\n\n\n\nIf the data will be only used intenally by a small group of individuals who may require privileged access, then pseudonymization might be a good fit for the needs\nTools for pseudonymization\n\nKIProtect’s Kodex\nFormat preserving library by Mysto\nMicrosoft’s Presidio\nPrivate Input Masked output based on GO"
  },
  {
    "objectID": "data_privacy/differential_privacy.html",
    "href": "data_privacy/differential_privacy.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Anonymize is to remove identifying information from data so that original source cannot be known\nPrivacy needs to be understood as a gradient and not a “on” or “off” thing \n\n\n\n\n\n\nHow Differential Privacy works\n\n\nA process A is epsilon-differentially private if for all databases D1 and D2 which differ in only one individual:\n\n\n\nDefinition\n\n\nThis must be true for all possible outputs O. If epsilon is very close to 0, then exponential of epsilon is very close to 1, so the probabilities are very similar. The bigger epsilon is, the more the probabilities can differ.\n\nIt is a rigorous and scientific definition of privacy-sensitive information release - that defines a limit or bounds on the amount of privacy loss you can have when you release information\nThis method focuses on the process rather than the result\nDifferential privacy shifts to thinking about what guarantees a particular algorithm can provide by measuring the information that is being continuously released via the algorithm itself.\nWhy is differential privacy special:-\n\nNo longer need attack modeling\nWe can quantify the privacy loss\nWe can compose multiple mechanisms - We can add the epsilon of multiple queries to arrive at the privacy loss for all the queries together.we can allocate budget for the user queries\n\nSensitivity measures the maximum change in the query result based on change in the underlying dataset.\n\n\n\n\n\n\nFree Udacity course\n\n\n\npython package pydp Tutorials using pydp python package opendp spark package PipelineDP Tensorflow Privacy\n\n\n\nBeautiful book with lot of plots and code\n\n\n\nDifferential Privacy blog by Damien Desfontaines\n\n\n\nMicrosoft session on privacy preserving ML"
  },
  {
    "objectID": "data_quality/setup_datasource.html",
    "href": "data_quality/setup_datasource.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Use this notebook to configure a new pandas Datasource and add it to your project.\n\nimport os\nos.chdir('/home/thulasiram/personal/going_deep_and_wide/GiveDirectly/gx_tutorials/great_expectations')\n\n\nimport great_expectations as gx\nfrom great_expectations.cli.datasource import sanitize_yaml_and_save_datasource, check_if_datasource_name_exists\ncontext = gx.get_context()\n\n\n\nIf you are new to Great Expectations Datasources, you should check out our how-to documentation\nMy configuration is not so simple - are there more advanced options? Glad you asked! Datasources are versatile. Please see our How To Guides!\nGive your datasource a unique name:\n\ndatasource_name = \"data_quality_demo\"\n\n\n\nHere we are creating an example configuration. The configuration contains an InferredAssetFilesystemDataConnector which will add a Data Asset for each file in the base directory you provided. It also contains a RuntimeDataConnector which can accept filepaths. This is just an example, and you may customize this as you wish!\nAlso, if you would like to learn more about the DataConnectors used in this configuration, including other methods to organize assets, handle multi-file assets, name assets based on parts of a filename, please see our docs on InferredAssetDataConnectors and RuntimeDataConnectors.\n\nexample_yaml = f\"\"\"\nname: {datasource_name}\nclass_name: Datasource\nexecution_engine:\n  class_name: PandasExecutionEngine\ndata_connectors:\n  default_inferred_data_connector_name:\n    class_name: InferredAssetFilesystemDataConnector\n    base_directory: ../data\n    default_regex:\n      group_names:\n        - data_asset_name\n      pattern: (.*)\n  default_runtime_data_connector_name:\n    class_name: RuntimeDataConnector\n    assets:\n      my_runtime_asset_name:\n        batch_identifiers:\n          - runtime_batch_identifier_name\n\"\"\"\nprint(example_yaml)\n\n\nname: data_quality_demo\nclass_name: Datasource\nexecution_engine:\n  class_name: PandasExecutionEngine\ndata_connectors:\n  default_inferred_data_connector_name:\n    class_name: InferredAssetFilesystemDataConnector\n    base_directory: ../data\n    default_regex:\n      group_names:\n        - data_asset_name\n      pattern: (.*)\n  default_runtime_data_connector_name:\n    class_name: RuntimeDataConnector\n    assets:\n      my_runtime_asset_name:\n        batch_identifiers:\n          - runtime_batch_identifier_name"
  },
  {
    "objectID": "data_quality/setup_datasource.html#save-your-datasource-configuration",
    "href": "data_quality/setup_datasource.html#save-your-datasource-configuration",
    "title": "My Datascience Journey",
    "section": "Save Your Datasource Configuration",
    "text": "Save Your Datasource Configuration\nHere we will save your Datasource in your Data Context once you are satisfied with the configuration. Note that overwrite_existing defaults to False, but you may change it to True if you wish to overwrite. Please note that if you wish to include comments you must add them directly to your great_expectations.yml.\n\nsanitize_yaml_and_save_datasource(context, example_yaml, overwrite_existing=False)\ncontext.list_datasources()\n\n[{'execution_engine': {'class_name': 'PandasExecutionEngine',\n   'module_name': 'great_expectations.execution_engine'},\n  'module_name': 'great_expectations.datasource',\n  'class_name': 'Datasource',\n  'name': 'data_quality_demo',\n  'data_connectors': {'default_inferred_data_connector_name': {'default_regex': {'group_names': ['data_asset_name'],\n     'pattern': '(.*)'},\n    'base_directory': '../data',\n    'module_name': 'great_expectations.datasource.data_connector',\n    'class_name': 'InferredAssetFilesystemDataConnector'},\n   'default_runtime_data_connector_name': {'module_name': 'great_expectations.datasource.data_connector',\n    'class_name': 'RuntimeDataConnector',\n    'assets': {'my_runtime_asset_name': {'module_name': 'great_expectations.datasource.data_connector.asset',\n      'class_name': 'Asset',\n      'batch_identifiers': ['runtime_batch_identifier_name']}}}}}]"
  },
  {
    "objectID": "data_quality/four_steps_to_data_quality.html",
    "href": "data_quality/four_steps_to_data_quality.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Great Expectations is a python package which helps ensuring data quality in four steps.\n\nSetup the Data context\nConnect to data\nCreate Expectations\nValidate Data\n\n\n\n\nFollow these four steps\n\n\n\n\nIn this Demo we will be using NYC taxi data to show how we can ensure the quality of data in production. This is an open data set which is updated every month. Each record in the data corresponds to one taxi ride and contains information such as the pick-up and drop-off location, the payment amount, and the number of passengers, among others.\nWe will be using two CSV files, each with 10,000 row sample of taxi trip records. A sample for January 2019 and a sample for February 2019.\nFor purposes of this tutorial, we are treating the January 2019 taxi data as our “current” data, and the February 2019 taxi data as “future” data that we have not yet looked at. We will use Great Expectations to build a profile of the January data and then use that profile to check for any unexpected data quality issues in the February data. In a real-life scenario, this would ensure that any problems with the February data would be caught (so it could be dealt with) before the February data is used in a production application!\n\n\n\n\n\n\n\n\n\n\n\nExplore Expecatations\n\n\n\n\nExplore validations"
  },
  {
    "objectID": "data_quality/run_checkpoint.html",
    "href": "data_quality/run_checkpoint.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Create Checkpoint\nUse this notebook to configure a new Checkpoint and add it to your project:\nCheckpoint Name: data_quality_demo_checkpoint\n\nimport os\nos.chdir('/home/thulasiram/personal/going_deep_and_wide/GiveDirectly/gx_tutorials/great_expectations')\n\n\nfrom ruamel.yaml import YAML\nimport great_expectations as gx\nfrom pprint import pprint\n\nyaml = YAML()\ncontext = gx.get_context()\n\n\n\nCreate a Checkpoint Configuration\nIf you are new to Great Expectations or the Checkpoint feature, you should start with SimpleCheckpoint because it includes default configurations like a default list of post validation actions.\nIn the cell below we have created a sample Checkpoint configuration using your configuration and SimpleCheckpoint to run a single validation of a single Expectation Suite against a single Batch of data.\nTo keep it simple, we are just choosing the first available instance of each of the following items you have configured in your Data Context: * Datasource * DataConnector * DataAsset * Partition * Expectation Suite\nOf course this is purely an example, you may edit this to your heart’s content.\nMy configuration is not so simple - are there more advanced options?\nGlad you asked! Checkpoints are very versatile. For example, you can validate many Batches in a single Checkpoint, validate Batches against different Expectation Suites or against many Expectation Suites, control the specific post-validation actions based on Expectation Suite / Batch / results of validation among other features. Check out our documentation on Checkpoints for more details and for instructions on how to implement other more advanced features including using the Checkpoint class: - https://docs.greatexpectations.io/docs/reference/checkpoints_and_actions - https://docs.greatexpectations.io/docs/guides/validation/checkpoints/how_to_create_a_new_checkpoint - https://docs.greatexpectations.io/docs/guides/validation/checkpoints/how_to_configure_a_new_checkpoint_using_test_yaml_config\n\nmy_checkpoint_name = \"data_quality_demo_checkpoint\" # This was populated from your CLI command.\n\nyaml_config = f\"\"\"\nname: {my_checkpoint_name}\nconfig_version: 1.0\nclass_name: SimpleCheckpoint\nrun_name_template: \"%Y%m%d-%H%M%S-my-run-name-template\"\nvalidations:\n  - batch_request:\n      datasource_name: data_quality_demo\n      data_connector_name: default_inferred_data_connector_name\n      data_asset_name: yellow_tripdata_sample_2019-02.csv\n      data_connector_query:\n        index: -1\n    expectation_suite_name: data_quality_expectation_demo\n\"\"\"\nprint(yaml_config)\n\n\nname: data_quality_demo_checkpoint\nconfig_version: 1.0\nclass_name: SimpleCheckpoint\nrun_name_template: \"%Y%m%d-%H%M%S-my-run-name-template\"\nvalidations:\n  - batch_request:\n      datasource_name: data_quality_demo\n      data_connector_name: default_inferred_data_connector_name\n      data_asset_name: yellow_tripdata_sample_2019-02.csv\n      data_connector_query:\n        index: -1\n    expectation_suite_name: data_quality_expectation_demo\n\n\n\n\n\nCustomize Your Configuration\nThe following cells show examples for listing your current configuration. You can replace values in the sample configuration with these values to customize your Checkpoint.\n\n# Run this cell to print out the names of your Datasources, Data Connectors and Data Assets\npprint(context.get_available_data_asset_names())\n\n{'data_quality_demo': {'default_inferred_data_connector_name': ['yellow_tripdata_sample_2019-01.csv',\n                                                                'yellow_tripdata_sample_2019-02.csv'],\n                       'default_runtime_data_connector_name': ['my_runtime_asset_name']}}\n\n\n\ncontext.list_expectation_suite_names()\n\n['data_quality_expectation_demo']\n\n\n\n\nTest Your Checkpoint Configuration\nHere we will test your Checkpoint configuration to make sure it is valid.\nThis test_yaml_config() function is meant to enable fast dev loops. If your configuration is correct, this cell will show a message that you successfully instantiated a Checkpoint. You can continually edit your Checkpoint config yaml and re-run the cell to check until the new config is valid.\nIf you instead wish to use python instead of yaml to configure your Checkpoint, you can use context.add_checkpoint() and specify all the required parameters.\n\nmy_checkpoint = context.test_yaml_config(yaml_config=yaml_config)\n\nAttempting to instantiate class from config...\n    Instantiating as a SimpleCheckpoint, since class_name is SimpleCheckpoint\n    Successfully instantiated SimpleCheckpoint\n\n\nCheckpoint class name: SimpleCheckpoint\n\n\n\n\nReview Your Checkpoint\nYou can run the following cell to print out the full yaml configuration. For example, if you used SimpleCheckpoint this will show you the default action list.\n\nprint(my_checkpoint.get_config(mode=\"yaml\"))\n\nname: data_quality_demo_checkpoint\nconfig_version: 1.0\ntemplate_name:\nmodule_name: great_expectations.checkpoint\nclass_name: Checkpoint\nrun_name_template: '%Y%m%d-%H%M%S-my-run-name-template'\nexpectation_suite_name:\nbatch_request: {}\naction_list:\n  - name: store_validation_result\n    action:\n      class_name: StoreValidationResultAction\n  - name: store_evaluation_params\n    action:\n      class_name: StoreEvaluationParametersAction\n  - name: update_data_docs\n    action:\n      class_name: UpdateDataDocsAction\n      site_names: []\nevaluation_parameters: {}\nruntime_configuration: {}\nvalidations:\n  - batch_request:\n      datasource_name: data_quality_demo\n      data_connector_name: default_inferred_data_connector_name\n      data_asset_name: yellow_tripdata_sample_2019-02.csv\n      data_connector_query:\n        index: -1\n    expectation_suite_name: data_quality_expectation_demo\nprofilers: []\nge_cloud_id:\nexpectation_suite_ge_cloud_id:\n\n\n\n\n\nAdd Your Checkpoint\nRun the following cell to save this Checkpoint to your Checkpoint Store.\n\ncontext.add_checkpoint(**yaml.load(yaml_config))\n\n{\n  \"action_list\": [\n    {\n      \"name\": \"store_validation_result\",\n      \"action\": {\n        \"class_name\": \"StoreValidationResultAction\"\n      }\n    },\n    {\n      \"name\": \"store_evaluation_params\",\n      \"action\": {\n        \"class_name\": \"StoreEvaluationParametersAction\"\n      }\n    },\n    {\n      \"name\": \"update_data_docs\",\n      \"action\": {\n        \"class_name\": \"UpdateDataDocsAction\",\n        \"site_names\": []\n      }\n    }\n  ],\n  \"batch_request\": {},\n  \"class_name\": \"Checkpoint\",\n  \"config_version\": 1.0,\n  \"evaluation_parameters\": {},\n  \"module_name\": \"great_expectations.checkpoint\",\n  \"name\": \"data_quality_demo_checkpoint\",\n  \"profilers\": [],\n  \"run_name_template\": \"%Y%m%d-%H%M%S-my-run-name-template\",\n  \"runtime_configuration\": {},\n  \"validations\": [\n    {\n      \"batch_request\": {\n        \"datasource_name\": \"data_quality_demo\",\n        \"data_connector_name\": \"default_inferred_data_connector_name\",\n        \"data_asset_name\": \"yellow_tripdata_sample_2019-02.csv\",\n        \"data_connector_query\": {\n          \"index\": -1\n        }\n      },\n      \"expectation_suite_name\": \"data_quality_expectation_demo\"\n    }\n  ]\n}\n\n\n\n\nRun Your Checkpoint & Open Data Docs(Optional)\nYou may wish to run the Checkpoint now and review its output in Data Docs. If so uncomment and run the following cell.\n\ncontext.run_checkpoint(checkpoint_name=my_checkpoint_name)\ncontext.open_data_docs()"
  },
  {
    "objectID": "data_quality/generate_expectation.html",
    "href": "data_quality/generate_expectation.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "This process helps you avoid writing lots of boilerplate when authoring suites by allowing you to select columns and other factors that you care about and letting a profiler write some candidate expectations for you to adjust.\nExpectation Suite Name: data_quality_expectation_demo\n\nimport os\nos.chdir('/home/thulasiram/personal/going_deep_and_wide/GiveDirectly/gx_tutorials/great_expectations')\n\n\nimport datetime\n\nimport pandas as pd\n\nimport great_expectations as gx\nimport great_expectations.jupyter_ux\nfrom great_expectations.core.batch import BatchRequest\nfrom great_expectations.checkpoint import SimpleCheckpoint\nfrom great_expectations.exceptions import DataContextError\n\ncontext = gx.data_context.DataContext()\n\nbatch_request = {'datasource_name': 'data_quality_demo', 'data_connector_name': 'default_inferred_data_connector_name', 'data_asset_name': 'yellow_tripdata_sample_2019-01.csv', 'limit': 1000}\n\nexpectation_suite_name = \"data_quality_expectation_demo\"\n\nvalidator = context.get_validator(\n    batch_request=BatchRequest(**batch_request),\n    expectation_suite_name=expectation_suite_name\n)\ncolumn_names = [f'\"{column_name}\"' for column_name in validator.columns()]\nprint(f\"Columns: {', '.join(column_names)}.\")\nvalidator.head(n_rows=5, fetch_all=False)\n\n2022-12-23T15:35:54+0530 - INFO - Great Expectations logging enabled at 20 level by JupyterUX module.\n\n\n\n\n\nColumns: \"vendor_id\", \"pickup_datetime\", \"dropoff_datetime\", \"passenger_count\", \"trip_distance\", \"rate_code_id\", \"store_and_fwd_flag\", \"pickup_location_id\", \"dropoff_location_id\", \"payment_type\", \"fare_amount\", \"extra\", \"mta_tax\", \"tip_amount\", \"tolls_amount\", \"improvement_surcharge\", \"total_amount\", \"congestion_surcharge\".\n\n\n\n\n\n\n\n\n\n  \n    \n      \n      vendor_id\n      pickup_datetime\n      dropoff_datetime\n      passenger_count\n      trip_distance\n      rate_code_id\n      store_and_fwd_flag\n      pickup_location_id\n      dropoff_location_id\n      payment_type\n      fare_amount\n      extra\n      mta_tax\n      tip_amount\n      tolls_amount\n      improvement_surcharge\n      total_amount\n      congestion_surcharge\n    \n  \n  \n    \n      0\n      1\n      2019-01-15 03:36:12\n      2019-01-15 03:42:19\n      1\n      1.0\n      1\n      N\n      230\n      48\n      1\n      6.5\n      0.5\n      0.5\n      1.95\n      0.0\n      0.3\n      9.75\n      NaN\n    \n    \n      1\n      1\n      2019-01-25 18:20:32\n      2019-01-25 18:26:55\n      1\n      0.8\n      1\n      N\n      112\n      112\n      1\n      6.0\n      1.0\n      0.5\n      1.55\n      0.0\n      0.3\n      9.35\n      0.0\n    \n    \n      2\n      1\n      2019-01-05 06:47:31\n      2019-01-05 06:52:19\n      1\n      1.1\n      1\n      N\n      107\n      4\n      2\n      6.0\n      0.0\n      0.5\n      0.00\n      0.0\n      0.3\n      6.80\n      NaN\n    \n    \n      3\n      1\n      2019-01-09 15:08:02\n      2019-01-09 15:20:17\n      1\n      2.5\n      1\n      N\n      143\n      158\n      1\n      11.0\n      0.0\n      0.5\n      3.00\n      0.0\n      0.3\n      14.80\n      NaN\n    \n    \n      4\n      1\n      2019-01-25 18:49:51\n      2019-01-25 18:56:44\n      1\n      0.8\n      1\n      N\n      246\n      90\n      1\n      6.5\n      1.0\n      0.5\n      1.65\n      0.0\n      0.3\n      9.95\n      0.0"
  },
  {
    "objectID": "data_quality/generate_expectation.html#next-steps",
    "href": "data_quality/generate_expectation.html#next-steps",
    "title": "My Datascience Journey",
    "section": "Next steps",
    "text": "Next steps\nAfter you review this initial Expectation Suite in Data Docs you should edit this suite to make finer grained adjustments to the expectations. This can be done by running great_expectations suite edit data_quality_expectation_demo."
  },
  {
    "objectID": "Interpretable_ml/theory/resources.html",
    "href": "Interpretable_ml/theory/resources.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Resources\n\nMegasource\n\nCynthia Rudin page\n\n\n\nPackage\n\n\n\nwhat If tool\n\n\n\n\nVideo\n\nThis looks like that\n\n\n\nPPT\n\nEBM explained\n\n\n\nResearch papers\n\nStop using blackbox models for high stakes\nNeural additive models\nunderrepresentation\nExplainable Neural Networks\nICE\nFooling Lime and shap\nFooling PDP\nFairwashing\nGAMchanger\nFIGS\n\n\n\nBlogs\n\ncorels\nDiCE\nGAMchanger\nInterpretability for NN- Distill blog\nFeature visualization - Distill\n\n\n\n\nTutorials\n\ncausal Inference\nInterpretableML\nLot of notebooks\nTutorial\n\n\n\nSoftware packages\n\nGAMchanger\nslim package\npyGAM\noptimal sparse decision tree\nPyMC\ndowhy\nAletheia\nThis looks like that\nRuleFit\nskope rules\nGLRM\nDeepLift\nEli5\nlofo\nAnchor\nPDP\nconditional Expectation plot\nALE\nDICE"
  },
  {
    "objectID": "Interpretable_ml/theory/07_rulefit.html",
    "href": "Interpretable_ml/theory/07_rulefit.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Sparse linear models that include automatically detected interaction effects in the form of decision rules\nNew features captures interactions between the features\nRuleFit automatically generates the interaction features from decision trees\nThe decision rules are binary\nThe feature importance can be calculated at the local and global level\nInterpretation of feature importance for interactions \nBagged ensembles, random forest, adaboost can be used for generating rules\n\n\n\n\nAutomatically adds feature interactions\nRules are easy to interpret\nFor an individual only a few rules will apply\n\n\n\n\n\nMany rules may get non-zero weight in the Lasso model\nInterpretation tricky when we have overlapping rules\n\n\n\n\n\nskope-rules (Seems the development stopped 2 years ago)\nimodels\n\n\n\n\n\nFast Interpretable greedy-tree sums (FIGS)\nHierarchical shrinkage:post-hoc regularization of tree based methods"
  },
  {
    "objectID": "Interpretable_ml/theory/11_neural_network.html",
    "href": "Interpretable_ml/theory/11_neural_network.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Techniques for interpreting NN\n\n\n\n\n\nThe approach of making the learned features explicit is called Feature Visualization\nFeature visualization for a unit of NN is done by finding the input that maximizes the activation of that unit\nFeature visualization can be done at Neuron, feature map, entire convolution layer etc.\n\n\n\n\n\nIt links highly activated areas of CNN channels with human concepts \n\n\n\n\n\nThis method highlights the pixels that were relevant for a certain image classification\nPixel attribution can be of two methods\n\nPerturbation based - Manipulate parts of the image to generate explanations\nGradient based - Many methods compute the gradient of the prediction with respect to the input features\n\n\n\n\n\nVanilla Gradient\nDeconvNet\nGrad-CAM\nGuided Grad-CAM\nSmoothGrad\n\n\n\n\n\n\nTCAV - Testing with concept activation vectors - For any given concept, TCAV measures the extent of that concept’s influence on the model’s prediction for a certain class. For example, TCAV can answer questions such as how the concept of “striped” influences a model classifying an image as a “zebra”. Since TCAV describes the relationship between a concept and a class, instead of explaining a single prediction, it provides useful global interpretation for a model’s overall behavior.\nDifferent types of CAV:-\n\nAutomated concept based explanation (ACE)\nConcept bottleneck models\nConcept whitening\n\n\n\n\n\n\nAn instance with small, intentional feature perturbations that cause a ml model to make a false prediction\n\n\n\n\n\nWe call a training instance “influential” when its deletion from the training data affect the resulting model.\nTwo approaches:-\n\nDeletion diagnostics\nInfluence functions\n\n\n\n\n\nDFBETA - Measures the effect of deleting an instance on model parameters\nCook’s distance - Measures the effect of deleting an instance on model predictions\nDFBETA works only for parameterized models\nwe can build a interpretable model between the influence of the instances and their features. This will provide more insights into the model.\nThe disadvantage of this method is that retraining is required for each instance in the dataset.\n\n\n\n\nlucid package with Notebook tutorials\ntf_cnnvis package\nKeras filter visualization\nDeepExplain\ninnvestigate\n\n\n\n\n\nNetwork Dissection"
  },
  {
    "objectID": "Interpretable_ml/theory/08_naive_bayes_and_knn.html",
    "href": "Interpretable_ml/theory/08_naive_bayes_and_knn.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Based on Bayes conditional probabilities\nStrong assumption of conditional independence of features \nIt is an interpretable model because of the independence of assumption"
  },
  {
    "objectID": "Interpretable_ml/theory/08_naive_bayes_and_knn.html#k-nearest-neighbors",
    "href": "Interpretable_ml/theory/08_naive_bayes_and_knn.html#k-nearest-neighbors",
    "title": "My Datascience Journey",
    "section": "K-Nearest Neighbors",
    "text": "K-Nearest Neighbors\n\nThe tricky part is finding the right K and how to measure the distance between instances\nThere are no parameters to learn, so no interpretability on a modular level\nNo global interpretability\nLocal interpretation depends on the number of features in a data instance. If the features are less then it can give good explanations."
  },
  {
    "objectID": "Interpretable_ml/theory/01_introduction.html",
    "href": "Interpretable_ml/theory/01_introduction.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Local explanations can be more accurate than global explanations.\nRashomon effect - An event can be explained by various causes\n\n\n\n\nExplanations are contrastive - The best explanation is the one that highlights the greatest difference between the object of interest and the reference object. Humans are interested in conterfactual explanations. Instead of knowing the reasons for why their loan was rejected, they would be interested in knowing what changes can be done to secure the loan.\nExplanations are selected - Give only 1 to 3 good reasons even if the world is very complex. Ensemble methods use different combinations of features with different models. It also means that there is more than one selective explanation of why a certain prediction was made.\nExplanations are social - Explanations should be contextualized to the social environment of the ML model and target audience.\nExplanations focus on the abnormal - (Teacher asking question and behaving abnormally as the reason for student failing the course). Any feature which was abnormal and which influenced the prediction should be included in explanation.\nGood explanations are consistent with prior beliefs of the explainee - How can we enforce monotonicity constraints on a feature? (feature can only affect the prediction in one direction)\nGood explanations are general and probable - In the absence of abnormal causes, a general explanation (explain many instances) is considered as a good explanation.\n\nTransparency is an enabler of trust\n\n\n\n* constraints - Sparsity, monotonic, interaction constriants\n* Additivity of Inputs - Example GAM\n* Prototypes - Using well understood data points for explaining previously unseen data point\n* Summarization - variable important measures, surrogate models and other post-hoc approaches\n\n\n\n\nGAM, GA2Ms, penalized regression\n\n\n\n\nIteratively Reweighted least squares (IRLS) - Minimize the effect of outliers. After first iteration IRLS checks which rows of input data are leadng to large errors. It then reduces the weight of those rows in subsequent iterations\nL1 Norm peanlty - LASSO - Avoid multiple comparison problemsthat arise in older stepwise feature selection.\nL2 Norm penalty - Ridge - Stabilize model parameters in the presence of correlation\nLink function\n\nlogarthim link function - count data\ninverse link function - Ganna distributed output\n\nElatic net\n\nIRLS, L1, L2 and link functions for various target or error distributions ##### when to use penalized regression:-\nwhen more features than rows\ncorrelated features\nNeed transparency\n\n\n\n\n\nState of the art in penalized regression models\nTransparent (Highest interpretability)\n\n\n\n\n\n\nGold standard of interpretability\nThey don’t fit to noise as traditional black-box ML models\n\n\n\n\n\nUse the same principles as GAM, EBM with some twists\nSimple additive combinations of shape functions\nBack propagation is used to learn optimal combinations of variables to act as inputs to shape functions learned via subnetworks\nShape functions are combined in additive fashion with weights to get output of the network\n\n\n\n\n\nprototype method\n\n\n\n\n\nRuleFit and scope rules are two techniques to find interpretable rules from the training data\nCertifiable optimal rule lists (CORELS)\nScalable Bayesian rule lists\n\n\n\n\n\nL1 penalty introduced to matrix factorization\nWe can extract new features from a large matrix of data, where only a few of the original columns have large weights on any new features\nsparse principal components analysis (SPCA) ???\nNonnegative Matrix Factorization (NMF) - Training data only takes positive values\nMany unsupervised learning techniques are instances of generalized low-rank models\n\n\n\n\n\n\n\n\nWhat input features value would have to be changed to change the outcome of a model prediction\n\n\n\n\n\nCommon in deep learning\nFor image and text data gradients are overlaid on input images to create highly visual explanations depicting where a function exhibits large gradients when creating a prediction for that input.\nIntegrated gradients - ????\nLayerwise relevance propagation - ???\nDeeplift - ???\nGrad-CAM\n\n\n\n\n\nRemove features from a model prediction and tracking the resulting change in the prediction\nThis method is basis for occlusion and leave-one-feature-out(LOFO) method\n\n\n\n\n\ncriticisms are data points that are not represented well by prototypes\nPrototypes and criticisms are used to generate local explanations\n\n\n\n\n\nAverage of all possible sets of inputs that do not include the feature of interest. Different groups of inputs are called coalitions.\nIt takes into account much more information than most other local feature methods\nTree SHAP - Exact - Applicable for tree based methods\nKernel and Deep SHAP - Approximate\nKernel SHAP - model agnostic\nSHAP is an offset from the average model prediction\nIt does not provide causal or counterfactual explanations\n\n\n\n\n\n\n\n\nAggreagte of local importance.\n\n\n\n\n\nSimple models of complex models\nModel agnostic\n\n\n\n\nDecision tree is trained on the inputs and predictions of a complex model\n\n\n\n\n\nLocal interpretable model-agnostic explanations (LIME)\nFitting linear model to the predictions of some small region of a more complex model’s predictions\nSparse local explanations\nA locally weighted interpretable surrogate model with a penalty to induce sparsity\nChallenges\n\nSampling is a problem for real-time explanation\nGenerated data may contain out-of-range data leading to unrealistic local feature importance values\nExtreme non-linearity and interactions in the selected local region can cause lime to fail completely\nLocal feature importance values are offsets from the local GLM intercept, and this intercept can sometimes account for the most important local phenomena.\n\n\n\n\n\n\nRule based methods to describe a ml model\nSpecial instance of using rule-based models as surrogate models\nRule based models have good capacity to learn non-linearities and interactions\n\n\n\n\n\n\n\n\nHow predictions change based on the values of one or two input features of interest, while averging out the effects of all oher input features.\nICE ??? depict how a model behaves for a single row of data\nPDP should be paired with ICE plots\n\n\n\n\n\nValuable when strong correlations exist in the training data"
  },
  {
    "objectID": "Interpretable_ml/theory/09_global_model_agnostic.html",
    "href": "Interpretable_ml/theory/09_global_model_agnostic.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "They describe the average behavior of ml models. They are often expressed as expected values based on the distribution of the data.\n\n\n\n\n\nGlobal Model Agnostic Methods\n\n\n\n\n\n\nPDP at a particular feature value represents the average prediction if we force all data points to assume that feature value\nIt shows the marginal effect one or two features have on the predicted outcome of an ML model. Other features are not touched or considered as random\nAssumption is that the feature for which we are calculating average behavior is not correlated with other features in the dataset\nIf this assumption is violated, then PDP will include data points that are very unlikely or even impossible\nFor classification where we have output probabilties, PDP display probabilities for a certain class given different values for a feature\nFor categorical features - For each of the category, we get a PDP estimate by forcing all data instances to have the same category and average the predictions\nFeature importance - A flat PDP indicates that the feature is not important, and the more the PDP varies, the more important the feature. This method ignores the effect of possible feature interactions.\n\n\n\n\nEasy to implement\nPDP has causal interpretation - We intervene on a feature and measure the changes in predictions\nMaximum number of features for PDP is two\nNeed to look at PDP along with data distribution, omititng the distribution can be misleading because we might overinterpret regions with almost no data\nAssumption of Independence - Unlikely values or impossible values\n\n\n\n\n\n\nFaster and unbiased alternative to PDP\nALE calculate how the model predictions change in a small window of the feature around v for data instances in that window.  \nWe average the changes of predictions, not predictions itself. We accumulate the average effects across all intervals. The effect is centered so that the mean effect is zero\nThis method isolates the effect of the feature of interest and blocks the effect of correlated features\nThe value of ALE can be interpreted as the main effect of the feature at a certain value compared to the average prediction of the data\nQuantiles of the feature are used as the grid that defines the intervals.\n\n\n\n\nALE plots are unbiased, work for correlated features\nFaster to compute\nInterpretation of ALE plots is clear: Conditional on a given value, the relative effect of changing the feature on the prediction can be read from the ALE plot. ALE plots are centered at zero. This makes their interpretation nice, because the value at each point of the ALE curve is the difference to the mean prediction. The 2D ALE plot only shows the interaction: If two features do not interact, the plot shows nothing.\nEntire prediction function can be decomposed into a sum of lower-dimensional ALE functions\nALE plots can become a bit shaky (many small ups and downs) with a high number of intervals\nUnlike PDPs, ALE plots are not accompanied by ICE curves\nImplementation of ALE plots are much more complex and interpretation remains difficult when features are strongly correlated.\nAs a rule of thumb use ALE instead of PDP\n\n\n\n\n\n\nThe interaction between two features is the change in the prediction that occurs by varying the features after considering the individual feature effects.\nH-Statistic - One way to estimate the interaction strength is to measure how much of the variation of the prediction depends on the interaction of the features\nWe measure the difference between the observed partial depedence function and the decomposed one without interactions. We calculate teh variance of the output. The amount of variance explained by the interaction is used as interaction strength statistic. The statistic is 0 if there is no interaction at all and 1 if all of the variance is explained.\n\n\n\n\nBacked by underlying theory, meaningful interpretation (share of variance explained by interaction), comparable across features and models\nComputationally expensive, results can be unstable in case of sampling, H-statistic can be greater than 1\nIt does not tell how the interaction look like. Measure the interaction strength and then create a 2D PDP plot for the interactions.\nWe can’t use this for images\nAssumption of independence of features\nVariable Interaction Networks (VIN) - is an approach that decomposes the prediction function into main effects and feature interactions. The interactions between features are then visualized as a network. Unfortunately no software is available yet.\n\n\n\n\n\n\nMeasures the increase in prediction error after we permute the feature’s values, which breaks the relationship between the feature and the true outcome.\nThis should be used on the test data\n\n\n\n\nNice interpretation - Feature importance is the increase in model error when the feature’s information is destroyed\nProvides highly compressed, global insight\nTakes into account all interactions with other features - It takes into account both the main feature effect and interaction effects on model performance (this is also a disadvantage - This is also a disadvantage because the importance of the interaction between two features is included in the importance measurements of both features. This means that the feature importances do not add up to the total drop in performance, but the sum is larger)\nIt needs access to true outcome\nIf features are correlated, feature importance can be biased by unrealistic data instances\n\n\n\n\n\n\nAn interpretable model that is trained to approximate the predictions of a black box model \nR-Squared can be used to measure how well surrogate replicates the black box model\nAny interpretable model can be used as a surrogate model\nEven if the underlying black-box model changes, you do not have to change your method of interpretation\nWith this method we should draw conclusions about the model and not the data\nIn case of close enough R-Squared, the surrogate model may be close enough for one subset of the dataset but widely divergent for another subset.\n\n\n\n\n\nA prototype is a data instance that is representative of all the data\nA criticism is a data instance that is not well represented by the set of prototypes\nPrototypes and criticisms are always actual instances from the data\nK-mediods can be used to find the prototypes - Any clustering algorithm that returns actual data points as cluster centers would qualify\nMMD-Critic approach is used to find prototypes and criticisms in a single framework. This method compares the distribution of the data and the distribution of the selected prototypes. Prototypes are selected that minimize the discrepancy between the two distributions. Data points from regions that are not well explained are selected as criticisms.\nMaximum Mean Discrepancy (MMD) is used to measure the discrepancy between two distributions. The closer the MMD squared is to zero, the better the distribution of the prototypes fits the data.\nMMD, Kernel and greedy search are used to find the prototypes\nwitness function is used to find the criticism. This tells us how much two density estimates differ at a particular point. Criticisms are points with high absolute value in the witness function \nThis method works with any type of data and any type of ML model\nWe are free to choose the number of prototypes and criticisms\nCriticisms depend on the number of existing prototypes\n\n\n\n\nIf we have a loss function that is twice differentiable with respect to its parameters, we can estimate the influence of the instance on the model parameters and on the prediction with influence functions.\nInstead of deleting the instance, the method upweights the instance in the loss by a very small step. Loss upweighting is similar to deleting the instance.\nAccess to the loss gradient with respect to the model parameters are required\n\n\n\n\n\nUnderstanding the weakness of a model by identifying influential instances helps to form a “mental model” of the machine learning model behavior in the mind\nDebugging model errors\nFixing the training data\n\n\n\n\n\nOne of the best debugging tools for ML models\nHelp to identify instances which should be checked for errors\nInfluence functions via derivaties can also be used to create adversarial training data\n\n\n\n\n\n\nprototype Implementation on text data using AIX360\nprototype on tabular data using AIX360\nprototype using Alibi\nALE using Alibi\nPDP and ICE using sklearn\npdp using PDPbox\nInfluence functions"
  },
  {
    "objectID": "Interpretable_ml/theory/12_nodegam.html",
    "href": "Interpretable_ml/theory/12_nodegam.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "EBMs will not work for large datasets and if the data consists of more features\n\n\n\n\nTrain a MLP for each feature and sum the outputs across features\n\n\n\n\nNAM working process\n\n\n\n\n\nNew activation function ExU is required\nAs ExU can be overfitting, they need to train multiple neural nets with different random seeds and take average\nComputationally very expensive\n\n\n\n\n\n\nNeural Oblivious Decision Trees (NODEs)\nInstead of neurons it uses differentiable oblivious decision trees (ODT) to learn\nIn the input layer, instead of summing all the input features and going through a Relu function, NodeGAM uses differentiable attention with temperature annealing to take only 1 feature. This makes sure there is no feature interaction in the model.\nFor the connections between layers, NodeGAM uses differentiable gates that only connect trees that belong to the same (set of) features. This also prevents feature interactions.\nFinally, it uses the DenseNet-like skip connections that take all the previous layers’ outputs as inputs. Also, in the output layer, it takes all the intermediate layers embedding as the inputs to the final linear layer as outputs. This helps the gradient to flow through the model since the tree response function is similar to the sigmoid that has a gradient vanishing problem.\n\n\n\n\nNodeGAM Architecture\n\n\n\n\n\n\nNeural Oblivious Decision Trees (NODEs)\nThis method learn pairwise interactions but removes any 3rd or higher-order interactions\nThis is achieved by limiting each tree to take at most 2 features. And the connections are only allowed among trees with the same sets of features.\nAn attention mechanism is added between layers to improve accuracy\n\n\n\n\nNode2GAM Architecture\n\n\nReference:- Medium Blogpost on NodeGAM code implementation of NodeGAM\n\n\n\nLink to research paper"
  },
  {
    "objectID": "Interpretable_ml/theory/10_local_model_agnostic.html",
    "href": "Interpretable_ml/theory/10_local_model_agnostic.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Local Methods\n\n\n\n\n\nHow the instance prediction changes when a feature changes\nThe equivalent of PDP for a individual row is called ICE.\nICE will provide more insight in case of interactions\nCentered ICE plot - Centre the curves at a certain point (may be at minimum) in the feature and display only the difference in prediction to this point.\nDerivative ICE plot - Individual derivaties of the prediction function with respect to the feature. It takes long time to compute and is impractical\n\n\n\n\nICE can uncover heterogeneous relationships\nCan display only one feature meaningfully\nSuffer from same points as pdp\nplot can become overcrowded - Add transparency or draw only a sample of the lines\nICE does not show average. Combine ICE with PDP\n\n\n\n\n\n\nInterpretable models that are used to explain individual predictions of a black box machine learning models\nLIME generates a new dataset consisting of perturbed samples and the corresponsing predictions of the black box model\nOn this new dataset LIME then trains an interpretable model, which is weighted by the proximity of the sampled instances to the instance of interest. \nDefining a meaningful neighborhood around a point is difficult. LIME uses exponential smoothing kernel to define the neighborhood.\nLIME can be used for Text and Images also. In case of text new data is generated by randomly removing words from the original text.\nLIME for images - variations of the images are created by segmenting the image into “superpixels” and turning superpixels off or on. Superpixels are interconnected pixels with similar colors and can be turned off by replacing each pixel with a user-defined color such as gray. The user can also specify a probability for turning off a superpixel in each permutation.\nThe fidelity measure (how well the interpretable model approximates the black box predictions) gives us a good idea of how reliable the interpretable model is in explaining the black box predictions in the neighborhood of the data instance of interest.\nThe surrogate model can use other features than the original model was trained on.\nFor defining the neighborhood we need to try different kernel setting and see if explanations make sense (Tabular data)\nInstability of explanations. Explanations of two very close points can be varied greatly. Repeating the sampling process can give different explanations.\n\n\n\n\n\nA counterfactual explanation of a prediction describes the smallest change to the feature values that changes the prediction to a predefined output.\nCounterfactuals suffer from ‘Rashomon effect’\nCriteria for counterfactuals\n\nCounterfactual instance produces the predefined prediction as closely as possible\nCounterfactual should be as similar as possible to the instance regarding feature values. It should change as few features as possible\nGenerate multiple diverse counterfactual explanations\nCounterfactual instance should have feature values that are likely\n\nInstead of trial and error, a loss function can be designed using the above four criteria and loss can be optimized to find counterfactuals. A genetic algorithm is used for optimization. (Nondominated Sorting Genetic Algorithm)\n\n\n\n\nClear explanations\nDoes not require access to the data or the model. (Attractive for companies which are audited by third parties)\nDisadvantage - ‘Rashomon effect’.\n\n\n\n\n\n\nExplains individual predictions of any black box classification model by finding a decision rule that “Anchors” the prediction sufficiently.\nA rule anchors a prediction if changes in other feature values do not affect the prediction\nUse perturbation based strategy to generate local explanations for predictions of black box ml models. The resulting explanations are expressed as easy to understand IF-THEN rules called “anchors”\nCoverage - To which other possibly unseen instances anchors apply\nReinforcement learning is used to find the anchors\n\n\n\nAnchors working\n\n\nThe algorithm’s efficiency decreases with many features\n\n\n\n\nEasy to interpret\nworks when model predictions are non-linear\nModel agnostic and parallelized\nNeed high configuration with hyperparameter tuning\nNeed discretization of numeric features\nNeed many calls to the ML model\nCoverage is undefined in some domains\nPresently it is implemented only for tabular data.\n\n\n\n\n\n\nA prediction can be explained by assuming that each feature value of the instance is a “player” in a game where the prediction is the payout. Shapley values – a method from coalitional game theory – tells us how to fairly distribute the “payout” among the features.\nShapley value is the average marginal contribution of a feature value across all possible coalitions.\nThe interpretation is how much a feature contributed to the prediction of this particular instance compared to the average prediction for the dataset. The sum of shapley values yields the difference of actual and average prediction\nThe Shapley value is the average contribution of a feature value to the prediction in different coalitions.\n\n\n\n\nThe difference between the prediction and the average prediction is fairly distributed among the feature values of the instance – the Efficiency property of Shapley values. This property distinguishes the Shapley value from other methods such as LIME. LIME does not guarantee that the prediction is fairly distributed among the features. The Shapley value might be the only method to deliver a full explanation.\nThe Shapley value allows contrastive explanations. Instead of comparing a prediction to the average prediction of the entire dataset, you could compare it to a subset or even to a single data point. This contrastiveness is also something that local models like LIME do not have.\nThe Shapley value is the only explanation method with a solid theory.\nIt requires a lot of computing time, only approximate solution is feasible.\nThe exponential number of the coalitions is dealt with by sampling coalitions and limiting the number of iterations M. Decreasing M reduces computation time, but increases the variance of the Shapley value. There is no good rule of thumb for the number of iterations M. M should be large enough to accurately estimate the Shapley values, but small enough to complete the computation in a reasonable time.\nShapley value method always use all the features.Humans prefer selective explanations as provided by LIME. SHAP can provide explanations with few features.\nNeed access to the data\nIt suffers from inclusion of unreaistic data instances when features are correlated\n\n\n\n\n\n\nKernelSHAP - A kernel based estimation approach for shapley values inspired by local surrogate models \nAs KernelSHAP uses linear regression for estimating shap values, we can go for sparse explanations.(I am not so sure whether the resulting coefficients would still be valid Shapley values though.)\n\n\n\n\n\nA variant of SHAP for tree based models\nIt uses conditional expectation to estimate effects which has its drawbacks. The shapley value of a feature not included in the coalition will not be zero in case of conditional expectation.\n\n\n\n\n\nShapley values can be combined into global explanations\nFeatures with large absolute shapely values are important \nDifference between Permuatation feature importance and shapley value. Permutation feature importance is based on the decrease in model performance. SHAP is based on magnitude of feature attributions.\nSummary plot combines feature importance with feature effects. #### SHAP Summary plot  #### SHAP feature dependence plot  #### SHAP Interaction values\nThe interaction effect is the additional combined feature effect after accounting for the individual feature effects. \n\n\n\n\n\nWe can cluster with shapley values. Features are often on different scales.The difficulty is to compute distances between instances with such different, non-comparable features.SHAP clustering works by clustering the Shapley values of each instance.This means that you cluster instances by explanation similarity. All SHAP values have the same unit – the unit of the prediction space. Any clustering technique can be selected"
  },
  {
    "objectID": "Interpretable_ml/theory/05_GLM_GAM.html",
    "href": "Interpretable_ml/theory/05_GLM_GAM.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Linear regression models have their drawbacks. Their assumptions also may not hold in real datasets. To overcome the assumptions and limitations of linear regression we have GLM\nGLM are used when target is not following gaussian distribution\nGAM are used when there is non-linear relationship between features and target\n\n\n\n\nThe core concept of GLM is to keep the weighted sum of features, but allow non-gaussian outcome distribution and connect the expected mean of this distribution (from exponential family of distributions) and the weighted sum through a possibly nonlinear function (link function). Example, logistic regression model assumes bernoulli distribution for the outcome and links the expected mean and weighted sum using the logistic function\n\n\n\n\n\nIt assumes that the outcome can be modeled by a sum of arbitary functions of each feature. \nUsing a flexible function allow non-linear relationships between some features and the output.\nSpline functions help learn nonlinearity\nSplines are functions that are constructed from similar basis functions. Splines can be used to approximate other, more complex functions.\nNonlinear modeling with slines is fancy feature engineering\nLet us assume we need to model a feature. We remove the feature from the data and replcae it with spline basis functions. For example four spine basis functions for the feature which we are interested. The value for each of the four spline feature depends on the original feature instance. Then weights are assigned to each of the spline basis function.\n\n\n\n\nData violates IID, Example - Repeated measurements from same patient - Mixed models\nHeteroscedastic - Variance of errors is not constant - For expensive houses error of price prediction will be higher - Robust regression\nOutliers - Robust regression\nTime until an event occurs - Parametric survival models, cox regression, survival analysis\nPredict ordered categories - Proportional odds model\nOutcome is a count - Poisson regression\nIf the count value of zero is very frequent - Zero inflated poisson regression, hurdle model\ncause and effect - Causal inference, mediation analysis\nMissing data - Multiple imputation\nIntegrate prior knowledge into my model - Bayesian inference\n\n\n\n\n\nIt keeps input features independent and also allow for complex modeling of each feature’s behavior\nThe output is a linear combination of parameters and some functions applied to data input values\nNeural Additive Model - A neural network is used to fit the shape function\nModel Editing - Change out parts of a model to better match reality or human intuition is model editing.\nAmenable to model editing\n\n\n\n\n\n2 in GA2M means consideration of a small group of pairwise interactions as inputs to the model\nEBM\n\nEach feature fitted with boosted tree\nTrees have advantages over spline functions - scalable, accept categorical or missing data\n\n\n\n\n\n\nParent-child relationships, especially near the top of the tree, tend to point toward feature interactions\nDecision tree does not end up with the best model for the dataset, but instead is one good candidate for the best model out of many, many possible options. (Rashomon effect)\nRashomon effect will lead to underspecification - Good performance on validation data but failing in real world\nTo overcome this problem:-\n\noptimal decision trees\nManual constraints using human domain knowledge\n\n\n\n\n\n\nXGBoost now supports monotonicity and interaction constraints\nconstrained xgboost can be used with a post-hoc explainable methods\n\n\n\n\n\nGAM - pyGAM"
  },
  {
    "objectID": "Interpretable_ml/theory/03_logistic_regression.html",
    "href": "Interpretable_ml/theory/03_logistic_regression.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Logistic Regression\n\nInterpretation\n\nNumerical Feature: If we increase the value of feature by one unit, the estimated odds change by a factor of exp(weight). Final weight = weight * exp(weight)\nCategorical Feature: One of the value of a feature is reference category. Changing the value of feature from reference category to other category will change estimated odds by a factor of exp(weight)\nIntercept: When all numeric features are zero and categorical features are at reference category, the extimated odds are exp(bias). Intercept weight is ususally not relevant\n\n\n\nPros and Cons\n\nSame as linear regression\nInterpretation is more difficult as the interpretation of weights is multiplicative and not additive"
  },
  {
    "objectID": "Interpretable_ml/theory/04_explainable_boosting_machine.html",
    "href": "Interpretable_ml/theory/04_explainable_boosting_machine.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Explainable Boosting Machines\n\nIt is a tree based, cyclic gradient boosting generalized additive model with automatic interaction detection\nOften accurate as blackbox models\nSlower to train than other modern algorithms\nExtremely compact and fast at prediction time\nFor both classification and regression\n\n\nReferences:-\n\nCornell Paper\nResearch paper\nInterpret Ml\n\n\n\nBlogs:-\n\nMedium Blog post\nNotebook\n\n\n\nGithub:-\n\nlink"
  },
  {
    "objectID": "Interpretable_ml/theory/06_Decision_tree.html",
    "href": "Interpretable_ml/theory/06_Decision_tree.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Decision Trees\n\nFeature Importance\n\nThe number of splits for which a feature was used and measure how much it has reduced the variance or gini index compared to the parent node. The sum of all importances is scaled to 100.\n\n\n\nDisadvantages\n\nTrees fail to deal with linear relationships\nSlight changes in input can have a big impact on outcome - Lack of smoothness\nTrees are unstable. A few changes in the training data can create a completely different tree\nThe number of terminal nodes increases quickly with depth"
  },
  {
    "objectID": "Interpretable_ml/theory/02_linear_regression.html",
    "href": "Interpretable_ml/theory/02_linear_regression.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Linear regression\n\nWeights come with confidence intervals.\n\n\nAssumptions of linear regression\n\nLinearity between independent and dependent features\nTarget follows a normal distribution. If this assumption is violated, the estimated confidence interval of the feature weights are invalid\nHomoscedasticity (constant variance of errors for prediction) - Suppose the average error (difference between predicted and actual price) in your linear regression model is 50,000 Euros. If you assume homoscedasticity, you assume that the average error of 50,000 is the same for houses that cost 1 million and for houses that cost only 40,000. This is unreasonable because it would mean that we can expect negative house prices.\nIndependence between data instances\n\nFixed features - Features are free of measurement errors\nAbsence of multicollinearity\n\n\n\nInterpretation\n\nIncreasing the numerical feature by one unit changes the estimated outcome by its weight\nChanging the categorical feature from reference category to the other category changes the estimated outcome by the feature’s weight\nFor an instance with all numeric feature values at zero and categorical features at the reference categories, the model prediction is the intercept weight\nR-Squared increases with increase in number of features, even if they do not contain any information about the target value. We use adjusted R-Square, which accounts for the number of features used in a model.\nThe importance of feature is measured by the value of t-statistic. The t-statistic is the estimated weight scaled with the standard error.\nWeight plot is used to visualize the weight and variance of the features \nEffect plot can help understand how much the combination of weight and feature contributes to the predictions in the data  For individual prediction we can overlay the value for each feature on top of the effect plot \nIn Lasso regression, we can use lambda as a parameter to control the interpretability of the model\n\n\n\nFeature selection\n\nForward selection: Fit a model with one feature. Do this for all the features. Select the model with high R-squared value. Keep adding features and selecting the best model. Continue till some criterion such as the max number of features are reached\nBackward selection: Similar to forward, here we keep removing features till some criterion is reached.\nLasso can be used, as it considers all features simultaneously and can be automated.\n\n\n\nPros and Cons\n\nAdvantages\n\nTransparent model\nComes with confidence intervals, tests etc\n\n\n\nDisadvantages\n\nNon-linearities need to be hand-crafted\nNot good predictive performance\nThe interpretation of weights can be unintuitive as it depends on all other features. House size and number of rooms are highly correlated. The weight of house size can be positive and number of rooms negative due to correlation."
  },
  {
    "objectID": "Interpretable_ml/interpret_ai/notebooks/01_ebm.html",
    "href": "Interpretable_ml/interpret_ai/notebooks/01_ebm.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Editable Interpretable Models\n\nLoad the libraries\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom interpret.glassbox import ExplainableBoostingClassifier\nfrom json import load\nfrom interpret import set_visualize_provider\nfrom interpret.provider import InlineProvider\nfrom interpret import show\nimport gamchanger as gc\nset_visualize_provider(InlineProvider())\n\n\n\nLoad the data\n\ndf = pd.read_csv(\n    \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\",\n    header=None)\n\n\ndf.columns = [\n    \"Age\", \"WorkClass\", \"fnlwgt\", \"Education\", \"EducationNum\",\n    \"MaritalStatus\", \"Occupation\", \"Relationship\", \"Race\", \"Gender\",\n    \"CapitalGain\", \"CapitalLoss\", \"HoursPerWeek\", \"NativeCountry\", \"Income\"\n]\n\n\n\nExplore the data\n\ndf.head()\n\n\n\n\n\n  \n    \n      \n      Age\n      WorkClass\n      fnlwgt\n      Education\n      EducationNum\n      MaritalStatus\n      Occupation\n      Relationship\n      Race\n      Gender\n      CapitalGain\n      CapitalLoss\n      HoursPerWeek\n      NativeCountry\n      Income\n    \n  \n  \n    \n      0\n      39\n      State-gov\n      77516\n      Bachelors\n      13\n      Never-married\n      Adm-clerical\n      Not-in-family\n      White\n      Male\n      2174\n      0\n      40\n      United-States\n      <=50K\n    \n    \n      1\n      50\n      Self-emp-not-inc\n      83311\n      Bachelors\n      13\n      Married-civ-spouse\n      Exec-managerial\n      Husband\n      White\n      Male\n      0\n      0\n      13\n      United-States\n      <=50K\n    \n    \n      2\n      38\n      Private\n      215646\n      HS-grad\n      9\n      Divorced\n      Handlers-cleaners\n      Not-in-family\n      White\n      Male\n      0\n      0\n      40\n      United-States\n      <=50K\n    \n    \n      3\n      53\n      Private\n      234721\n      11th\n      7\n      Married-civ-spouse\n      Handlers-cleaners\n      Husband\n      Black\n      Male\n      0\n      0\n      40\n      United-States\n      <=50K\n    \n    \n      4\n      28\n      Private\n      338409\n      Bachelors\n      13\n      Married-civ-spouse\n      Prof-specialty\n      Wife\n      Black\n      Female\n      0\n      0\n      40\n      Cuba\n      <=50K\n    \n  \n\n\n\n\n\ndf.shape\n\n(32561, 15)\n\n\n\ndf.isna().sum()\n\nAge              0\nWorkClass        0\nfnlwgt           0\nEducation        0\nEducationNum     0\nMaritalStatus    0\nOccupation       0\nRelationship     0\nRace             0\nGender           0\nCapitalGain      0\nCapitalLoss      0\nHoursPerWeek     0\nNativeCountry    0\nIncome           0\ndtype: int64\n\n\n\ntrain_cols = df.columns[0:-1]\n\n\nlabel = df.columns[-1]\n\n\nX = df[train_cols]\n\n\ny = df[label]\n\n\nseed = 163\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.20,random_state=seed)\n\n\n\nTrain the model\n\nebm = ExplainableBoostingClassifier(random_state=seed)\nebm.fit(X_train,y_train)\n\nExplainableBoostingClassifier(feature_names=['Age', 'WorkClass', 'fnlwgt',\n                                             'Education', 'EducationNum',\n                                             'MaritalStatus', 'Occupation',\n                                             'Relationship', 'Race', 'Gender',\n                                             'CapitalGain', 'CapitalLoss',\n                                             'HoursPerWeek', 'NativeCountry',\n                                             'Relationship x HoursPerWeek',\n                                             'Age x Relationship',\n                                             'EducationNum x Occupation',\n                                             'EducationNum x MaritalStatus',\n                                             'Age x HoursPerWeek',\n                                             'MaritalSta...\n                              feature_types=['continuous', 'categorical',\n                                             'continuous', 'categorical',\n                                             'continuous', 'categorical',\n                                             'categorical', 'categorical',\n                                             'categorical', 'categorical',\n                                             'continuous', 'continuous',\n                                             'continuous', 'categorical',\n                                             'interaction', 'interaction',\n                                             'interaction', 'interaction',\n                                             'interaction', 'interaction',\n                                             'interaction', 'interaction',\n                                             'interaction', 'interaction'],\n                              random_state=163)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.ExplainableBoostingClassifierExplainableBoostingClassifier(feature_names=['Age', 'WorkClass', 'fnlwgt',\n                                             'Education', 'EducationNum',\n                                             'MaritalStatus', 'Occupation',\n                                             'Relationship', 'Race', 'Gender',\n                                             'CapitalGain', 'CapitalLoss',\n                                             'HoursPerWeek', 'NativeCountry',\n                                             'Relationship x HoursPerWeek',\n                                             'Age x Relationship',\n                                             'EducationNum x Occupation',\n                                             'EducationNum x MaritalStatus',\n                                             'Age x HoursPerWeek',\n                                             'MaritalSta...\n                              feature_types=['continuous', 'categorical',\n                                             'continuous', 'categorical',\n                                             'continuous', 'categorical',\n                                             'categorical', 'categorical',\n                                             'categorical', 'categorical',\n                                             'continuous', 'continuous',\n                                             'continuous', 'categorical',\n                                             'interaction', 'interaction',\n                                             'interaction', 'interaction',\n                                             'interaction', 'interaction',\n                                             'interaction', 'interaction',\n                                             'interaction', 'interaction'],\n                              random_state=163)\n\n\n\nebm.feature_names\n\n['Age',\n 'WorkClass',\n 'fnlwgt',\n 'Education',\n 'EducationNum',\n 'MaritalStatus',\n 'Occupation',\n 'Relationship',\n 'Race',\n 'Gender',\n 'CapitalGain',\n 'CapitalLoss',\n 'HoursPerWeek',\n 'NativeCountry',\n 'Relationship x HoursPerWeek',\n 'Age x Relationship',\n 'EducationNum x Occupation',\n 'EducationNum x MaritalStatus',\n 'Age x HoursPerWeek',\n 'MaritalStatus x HoursPerWeek',\n 'WorkClass x CapitalLoss',\n 'Age x CapitalLoss',\n 'Occupation x Relationship',\n 'fnlwgt x HoursPerWeek']\n\n\n\n\nGlobal explanations\n\n\nebm_global = ebm.explain_global()\nshow(ebm_global)\n\n\n\n    \n    \n    \n\n\n\n\nLocal Explanations\n\nebm_local = ebm.explain_local(X_test[:5],y_test[:5])\n\n/tmp/ipykernel_312818/1167488945.py:1: FutureWarning:\n\nThe behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`.\n\n\n\n\nshow(ebm_local)\n\n\n\n    \n    \n    \n\n\n\n\nEdit the model to match expert expectations\n\ngc.visualize(ebm=ebm,x_test=X_test,y_test=y_test)\n\n\n        \n        \n    \n\n\n\n\nLoad the model and check if the changes took place\n\ngc_dict = load(open('/home/thulasiram/personal/going_deep_and_wide/Interpretable_ml/interpret_ai/interpret_ai/data/modified_model.gamchanger','r'))\n\n\nnew_ebm = gc.get_edited_model(ebm,gc_dict)\n\n\ngc.visualize(ebm=new_ebm,x_test=X_test,y_test=y_test)"
  },
  {
    "objectID": "Interpretable_ml/interpret_ai/notebooks/02_gamchanger.html",
    "href": "Interpretable_ml/interpret_ai/notebooks/02_gamchanger.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "from json import load\nimport gamchanger as gc\nimport pandas as pd\nfrom interpret.glassbox import ExplainableBoostingClassifier\n\n\nsamples = load(open('/home/thulasiram/personal/going_deep_and_wide/Interpretable_ml/interpret_ai/interpret_ai/data/sample.json','r'))\nebm_model = load(open('/home/thulasiram/personal/going_deep_and_wide/Interpretable_ml/interpret_ai/interpret_ai/data/model.json','r'))\n\n\nsamples.keys()\n\ndict_keys(['featureNames', 'featureTypes', 'samples', 'labels'])\n\n\n\nX_test = pd.DataFrame(samples['samples'],columns=samples['featureNames'])\n\n\ny_test = pd.Series(samples['labels'])\n\n\nebm = ExplainableBoostingClassifier(random_state=123)\n\n\ngc.visualize(ebm=ebm,model_data=ebm_model,sample_data=samples)\n\n\n        \n        \n    \n\n\n\ngc_dict = load(open('/home/thulasiram/personal/going_deep_and_wide/Interpretable_ml/interpret_ai/interpret_ai/data/modified_model.gamchanger','r'))\n\n\nnew_ebm = gc.get_edited_model(ebm=ebm,gamchanger_export=gc_dict)\n\nTypeError: 'NoneType' object is not iterable\n\n\n\ngc."
  },
  {
    "objectID": "Interpretable_ml/interpret_ai/notebooks/03_rulefit.html",
    "href": "Interpretable_ml/interpret_ai/notebooks/03_rulefit.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom imodels import FIGSClassifier\nfrom sklearn.datasets import load_breast_cancer\nimport os\n\n\nraw_data = load_breast_cancer()\n\n\nX = raw_data.data\ny = raw_data.target\nfeatures = raw_data.feature_names\ntarget_names = raw_data.target_names\n\n\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.20,random_state=123)\n\n\nfigs = FIGSClassifier(max_rules=4)\n\n\nfigs.fit(X_train,y_train,feature_names=features)\n\n\n> ------------------------------\n> FIGS-Fast Interpretable Greedy-Tree Sums:\n>    Predictions are made by summing the \"Val\" reached by traversing each tree\n> ------------------------------\nworst radius <= 16.795 (Tree #0 root)\n    worst concave points <= 0.136 (split)\n        Val: 0.985 (leaf)\n        worst texture <= 25.670 (split)\n            Val: 0.812 (leaf)\n            Val: 0.091 (leaf)\n    mean concavity <= 0.072 (split)\n        Val: 0.500 (leaf)\n        Val: 0.007 (leaf)\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.FIGSClassifier> ------------------------------\n> FIGS-Fast Interpretable Greedy-Tree Sums:\n>    Predictions are made by summing the \"Val\" reached by traversing each tree\n> ------------------------------\nworst radius <= 16.795 (Tree #0 root)\n    worst concave points <= 0.136 (split)\n        Val: 0.985 (leaf)\n        worst texture <= 25.670 (split)\n            Val: 0.812 (leaf)\n            Val: 0.091 (leaf)\n    mean concavity <= 0.072 (split)\n        Val: 0.500 (leaf)\n        Val: 0.007 (leaf)\n\n\n\n\nprint(figs)\n\n> ------------------------------\n> FIGS-Fast Interpretable Greedy-Tree Sums:\n>   Predictions are made by summing the \"Val\" reached by traversing each tree\n> ------------------------------\nworst radius <= 16.795 (Tree #0 root)\n    worst concave points <= 0.136 (split)\n        Val: 0.985 (leaf)\n        worst texture <= 25.670 (split)\n            Val: 0.812 (leaf)\n            Val: 0.091 (leaf)\n    mean concavity <= 0.072 (split)\n        Val: 0.500 (leaf)\n        Val: 0.007 (leaf)\n\n\n\n\nprint(figs.print_tree(X_train, y_train))\n\n------------\nworst radius <= 16.795 284/455 (62.42%)\n    worst concave points <= 0.136 275/302 (91.06%)\n        ΔRisk = 0.98 260/264 (98.48%)\n        worst texture <= 25.670 15/38 (39.47%)\n            ΔRisk = 0.81 13/16 (81.25%)\n            ΔRisk = 0.09 2/22 (9.09%)\n    mean concavity <= 0.072 9/153 (5.88%)\n        ΔRisk = 0.50 8/16 (50.0%)\n        ΔRisk = 0.01 1/137 (0.73%)\n\n\n\n\nfigs.plot(fig_size=8)\n\n\n\n\n\nfigs.complexity_\n\n4\n\n\n\nfigs.get_params()\n\n{'max_features': None,\n 'max_rules': 4,\n 'min_impurity_decrease': 0.0,\n 'random_state': None}\n\n\n\nfigs.feature_names_\n\narray(['mean radius', 'mean texture', 'mean perimeter', 'mean area',\n       'mean smoothness', 'mean compactness', 'mean concavity',\n       'mean concave points', 'mean symmetry', 'mean fractal dimension',\n       'radius error', 'texture error', 'perimeter error', 'area error',\n       'smoothness error', 'compactness error', 'concavity error',\n       'concave points error', 'symmetry error',\n       'fractal dimension error', 'worst radius', 'worst texture',\n       'worst perimeter', 'worst area', 'worst smoothness',\n       'worst compactness', 'worst concavity', 'worst concave points',\n       'worst symmetry', 'worst fractal dimension'], dtype='<U23')"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "My Notes on Data Science, Machine learning and Artificial Intelligence along with project portfolio."
  },
  {
    "objectID": "graph_ml/02_resources.html",
    "href": "graph_ml/02_resources.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Standford course Geometric Deep learning youtube course\n\n\n\npytorch geometric Networkx pygod UGFraud igraph Gephi\n\n\n\nGetting started in Graph ML Graph Representational learning Understanding Deepwalk Graph CNN Graphsage CHEBNET Graph Attention Networks\n\n\n\nRepo of datasets\n\n\n\nSNAP node2vec tutorial networkx tutorial\n\n\n\nGraph Representation learning\n\n\n\npytorch Graph Attention Networks Graph Representational Learning List of resources\n\n\n\n\n\n\nList of resources\n\n\n\n\n\n\n\nGithub repo Microcluster-based Detector of Anomalies"
  },
  {
    "objectID": "graph_ml/01_intro.html",
    "href": "graph_ml/01_intro.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Node Centrality measures the node importance in a graph. Node degree is the quantity of direct neighbours it has. clustering coefficient measures how connected are the node neighbours. Graphlets degree Vectors count how many different graphlets are rooted at a given node.\n\n\n\n2-to-5 node graphlets\n\n\n\n\n\n\nShortest distance between two nodes\nCommon neighbours of two nodes\nKatz index - Number of possible walks up to a certain length between two nodes\n\n\n\n\n\nGraphlet counts\nKernel methods measure similarity between graphs through different “bag of nodes” methods (similar to bag of words)\n\n\n\n\n\n\n\nAdvances in GraphML\n\n\n\n\n\n * Node2Vec simulates random walks between nodes of a graph, then processes these walks with skip-gram, to compute embeddings * The drawback of Node2Vec is as the graph changes the embeddings should change. Suitable for a static graph\n\n\n\n\nGraph Convolutional Networks averages the normalised representation of the neighbours for a node (most GNNs are actually GCNs) Graph Attention Networks learn to weigh the different neighbours based on their importance (like transformers); GraphSAGE samples neighbours at different hops before aggregating their information in several steps with max pooling. Graph Isomorphism Networks aggregates representation by applying an MLP to the sum of the neighbours’ node representations.\n\n\n\nHuggingface blog"
  },
  {
    "objectID": "distributed_processing/fugue_quickstart.html",
    "href": "distributed_processing/fugue_quickstart.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Fugue Quickstart\n\nImport the required libraries\n\nimport warnings\nwarnings.filterwarnings(action='ignore')\n\n\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.getOrCreate()\n\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nfrom typing import List, Dict, Iterable, Any\n\n\n\nCreate a model in Sklearn and do predictions using Spark\n\n\nX = pd.DataFrame({\"x_1\": [1, 1, 2, 2], \"x_2\":[1, 2, 2, 3]})\ny = np.dot(X, np.array([1, 2])) + 3\nreg = LinearRegression().fit(X, y)\n\n\n# define our predict function\ndef predict(df: pd.DataFrame, model: LinearRegression) -> pd.DataFrame:\n    \"\"\"\n    Function to predict results using a pre-built model\n    \"\"\"\n    return df.assign(predicted=model.predict(df))\n\n# create test data\ninput_df = pd.DataFrame({\"x_1\": [3, 4, 6, 6], \"x_2\":[3, 3, 6, 6]})\n\n# test the predict function\npredict(input_df, reg)\n\n\n\n\n\n  \n    \n      \n      x_1\n      x_2\n      predicted\n    \n  \n  \n    \n      0\n      3\n      3\n      12.0\n    \n    \n      1\n      4\n      3\n      13.0\n    \n    \n      2\n      6\n      6\n      21.0\n    \n    \n      3\n      6\n      6\n      21.0\n    \n  \n\n\n\n\n\n# import Fugue\nfrom fugue import transform\n\n# create a spark dataframe\nsdf = spark.createDataFrame(input_df)\n\n# use Fugue transform to switch exection to spark\nresult = transform(\n    df=sdf,\n    using=predict,\n    schema=\"*,predicted:double\",\n    params=dict(model=reg),\n    engine=spark\n)\n\n# display results\nprint(type(result))\nresult.show()\n\n<class 'pyspark.sql.dataframe.DataFrame'>\n\n\n[Stage 2:==========================================>               (8 + 3) / 11]\n\n\n+---+---+------------------+\n|x_1|x_2|         predicted|\n+---+---+------------------+\n|  3|  3|              12.0|\n|  4|  3|              13.0|\n|  6|  6|20.999999999999996|\n|  6|  6|20.999999999999996|\n+---+---+------------------+\n\n\n\n                                                                                \n\n\n\n\nDo the predictions in Dask\n\n# using transform to bring predict to dask execution\nresult = transform(\n    df=input_df.copy(),\n    using=predict,\n    schema=\"*,predicted:double\",\n    params=dict(model=reg),\n    engine=\"dask\"\n)\n\n# display results\nprint(type(result))\nresult.compute().head()\n\n<class 'dask.dataframe.core.DataFrame'>\n\n\n\n\n\n\n  \n    \n      \n      x_1\n      x_2\n      predicted\n    \n  \n  \n    \n      0\n      3\n      3\n      12.0\n    \n    \n      0\n      4\n      3\n      13.0\n    \n    \n      0\n      6\n      6\n      21.0\n    \n    \n      0\n      6\n      6\n      21.0\n    \n  \n\n\n\n\n\n\nReturn the output as a Pandas Dataframe\n\n# use as_local=True to return a Pandas DataFrame\nlocal_result = transform(\n    df=input_df,\n    using=predict,\n    schema=\"*,predicted:double\",\n    params=dict(model=reg),\n    engine=\"dask\",\n    as_local=True\n)\n\nprint(type(local_result))\nlocal_result.head()\n\n<class 'pandas.core.frame.DataFrame'>\n\n\n\n\n\n\n  \n    \n      \n      x_1\n      x_2\n      predicted\n    \n  \n  \n    \n      0\n      3\n      3\n      12.0\n    \n    \n      1\n      4\n      3\n      13.0\n    \n    \n      2\n      6\n      6\n      21.0\n    \n    \n      3\n      6\n      6\n      21.0\n    \n  \n\n\n\n\n\n\nType Hints\nThe input type annotation tells Fugue what to convert the input data to before the function is applied whereas the output type annotation informs Fugue how to convert it back to a Pandas, Spark, Dask, or Ray DataFrame.\n\n\nSchema\nWhen using transform() function, the best practice is to provide schema definition.When using the transform(), the * in a schema expression means all existing columns. From there we can add new columns by adding “,column_name:type”\n\ndf = pd.DataFrame({\"a\": [1,2,3], \"b\": [1,2,3], \"c\": [1,2,3]})\n\n\ndef add_col(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Function that creates a column with a value of column a + 1.\n    \"\"\"\n    return df.assign(new_col=df[\"a\"] + 1)\n\ntransform(\n    df=df, \n    using=add_col, \n    schema=\"*,new_col:int\"\n    )\n\n\n\n\n\n  \n    \n      \n      a\n      b\n      c\n      new_col\n    \n  \n  \n    \n      0\n      1\n      1\n      1\n      2\n    \n    \n      1\n      2\n      2\n      2\n      3\n    \n    \n      2\n      3\n      3\n      3\n      4\n    \n  \n\n\n\n\n\n\nPartitioning\nThe type hint conversion is applied on the partition level.\n\ndf = pd.DataFrame({\"a\": [1,2,3,4], \"b\": [1,2,3,4], \"c\": [1,2,3,4]})\n\ndef size(df: pd.DataFrame) -> Iterable[Dict[str,Any]]:\n    \"\"\"\n    Function that calculates the size of a DataFrame.\n    \"\"\"\n    yield {\"size\":df.shape[0]}\n\n\ntransform(\n    df=df, \n    using=size, \n    schema=\"size:int\", \n    engine=\"dask\",\n    as_local=True\n    )\n\n\n\n\n\n  \n    \n      \n      size\n    \n  \n  \n    \n      0\n      1\n    \n    \n      1\n      1\n    \n    \n      2\n      1\n    \n    \n      3\n      1\n    \n  \n\n\n\n\nThe type hint conversion happens on each partition. We can control the partition by specifying the column.\n\ndf = pd.DataFrame({\"col1\": [\"a\",\"a\",\"a\",\"b\",\"b\",\"b\"], \n                   \"col2\": [1,2,3,4,5,6]})\ndf\n\n\n\n\n\n  \n    \n      \n      col1\n      col2\n    \n  \n  \n    \n      0\n      a\n      1\n    \n    \n      1\n      a\n      2\n    \n    \n      2\n      a\n      3\n    \n    \n      3\n      b\n      4\n    \n    \n      4\n      b\n      5\n    \n    \n      5\n      b\n      6\n    \n  \n\n\n\n\n\ndef min_max(df:pd.DataFrame) -> List[Dict[str,Any]]:\n    \"\"\"\n    Calculates the min and max of a given column based\n    on the grouping of a separate column.\n    \"\"\"\n    return [{\"group\": df.iloc[0][\"col1\"], \n             \"max\": df['col2'].max(), \n             \"min\": df['col2'].min()}]\n\n\ntransform(\n    df=df, \n    using=min_max, \n    schema=\"group:str, max:int, min:int\",\n    partition={\"by\": \"col1\"}\n    )\n\n\n\n\n\n  \n    \n      \n      group\n      max\n      min\n    \n  \n  \n    \n      0\n      a\n      3\n      1\n    \n    \n      1\n      b\n      6\n      4\n    \n  \n\n\n\n\nWe can use transform() operation to save the output as a parquet file\n\ndf = pd.DataFrame({\"a\": [1,2,3], \"b\": [1,2,3], \"c\": [1,2,3]})\ndf.to_parquet(\"../data/df.parquet\")\n\n\ndef drop_col(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    A function that drops a column labelled 'b'.\n    \"\"\"\n    return df.drop(\"b\", axis=1)\n\ntransform(\n    df=\"../data/df.parquet\",\n    using=drop_col,\n    schema=\"*-b\",\n    engine=spark,\n    save_path=\"../data/processed.parquet\"\n    )\n\npd.read_parquet(\"../data/processed.parquet/\").head()\n\n                                                                                \n\n\n\n\n\n\n  \n    \n      \n      a\n      c\n    \n  \n  \n    \n      0\n      1\n      1\n    \n    \n      1\n      2\n      2\n    \n    \n      2\n      3\n      3\n    \n  \n\n\n\n\nThis expression makes it easy for users to toggle between running Pandas with sampled data and using Spark, Dask or Ray on the full dataset.We can use transform() to distribute the processing of a single step in our process."
  },
  {
    "objectID": "distributed_processing/fugue_intro.html",
    "href": "distributed_processing/fugue_intro.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Fugue\nThis module is included to show how we can easily port pandas code to a compute engine of our choice (with minimal code changes).\nFugue provides an easier interface to using distributed compute effectively and accelerates big data projects.Fugue ports Python, Pandas and SQL code to Spark, Dask and Ray\n\n\n\nBenefits of Fugue"
  },
  {
    "objectID": "distributed_processing/fugue_sql.html",
    "href": "distributed_processing/fugue_sql.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "FugueSQL\nFugueSQL can be used on top of Pandas, Spark and Dask. FugueSQL is parsed and then executed on top of the underlying engine.\n\nimport warnings\nwarnings.filterwarnings(action='ignore')\n\n\nfrom fugue_notebook import setup\nsetup(is_lab=False)\n\n\n\n\n\n\nimport pandas as pd\n\ndf = pd.DataFrame({\"col1\": [\"A\",\"A\",\"A\",\"B\",\"B\",\"B\"], \"col2\": [1,2,3,4,5,6]})\ndf2 = pd.DataFrame({\"col1\": [\"A\", \"B\"], \"col3\": [1, 2]})\n\n\nRun FugueSQL\n\n%%fsql\n   SELECT df.col1, df.col2, df2.col3\n     FROM df\nLEFT JOIN df2\n       ON df.col1 = df2.col1\n    WHERE df.col1 = \"A\"\n    PRINT\n\n\n\n\n\n  \n    \n      \n      col1\n      col2\n      col3\n    \n  \n  \n    \n      0\n      A\n      1\n      1\n    \n    \n      1\n      A\n      2\n      1\n    \n    \n      2\n      A\n      3\n      1\n    \n  \n\n\n\n\nschema: col1:str,col2:long,col3:long\n\n\n\n\nUsing FugueSQL dataframe in Python\n\n%%fsql\nSELECT *\n  FROM df\n YIELD DATAFRAME AS result\n\n\nprint(type(result))\nprint(result.native.head())\n\n<class 'fugue.dataframe.pandas_dataframe.PandasDataFrame'>\n  col1  col2\n0    A     1\n1    A     2\n2    A     3\n3    B     4\n4    B     5\n\n\n\n\nLoading files\n\n%%fsql\ndf = LOAD \"../data/processed.parquet\"\n\nnew = SELECT *\n        FROM df\n       YIELD DATAFRAME AS result\n\n\nprint(result.native)\n\n   a  c\n0  1  1\n1  2  2\n2  3  3\n\n\nCommon table expressions (CTEs) are also supported by FugueSQL\n\n\nUsing python code on SQL\n\nf = pd.DataFrame({\"col1\": [\"A\",\"A\",\"A\",\"B\",\"B\",\"B\"], \"col2\": [1,2,3,4,5,6]})\n\n\n# schema: *+col2:float\ndef std_dev(df: pd.DataFrame) -> pd.DataFrame:\n    return df.assign(col2=df['col2']/df['col2'].max())\n\nThe function above is defined to handle one group of data at a time. In order to apply it per group, we partition the DataFrame first by group using the PREPARTITION and TRANSFORM keywords of FugueSQL.\n\n%%fsql\nTRANSFORM df PREPARTITION BY col1 USING std_dev\nPRINT\n\n\n\n\n\n  \n    \n      \n      col1\n      col2\n    \n  \n  \n    \n      0\n      A\n      0.333333\n    \n    \n      1\n      A\n      0.666667\n    \n    \n      2\n      A\n      1.000000\n    \n    \n      3\n      B\n      0.666667\n    \n    \n      4\n      B\n      0.833333\n    \n    \n      5\n      B\n      1.000000\n    \n  \n\n\n\n\nschema: col1:str,col2:float\n\n\n\n\nRun SQL code using either Duckdb, Spark or Dask engine\nFugue supports Pandas, Spark, Dask, and DuckDB. For operations on a laptop or single machine, DuckDB may give significant improvements over Pandas because it has a query optimizer.\nFor data that is too large to process on a single machine, Spark or Dask can be used. All we need to do is specify the engine in the cell. For example, to run on DuckDB we can do:\n\n%%fsql duckdb\nTRANSFORM df PREPARTITION BY col1 USING std_dev\nPRINT\n\n\n\n\n\n  \n    \n      \n      col1\n      col2\n    \n  \n  \n    \n      0\n      A\n      0.333333\n    \n    \n      1\n      A\n      0.666667\n    \n    \n      2\n      A\n      1.000000\n    \n    \n      3\n      B\n      0.666667\n    \n    \n      4\n      B\n      0.833333\n    \n    \n      5\n      B\n      1.000000\n    \n  \n\n\n\n\nschema: col1:str,col2:float\n\n\n\n%%fsql spark\nTRANSFORM df PREPARTITION BY col1 USING std_dev\nPRINT\n\n\n\n\n\n  \n    \n      \n      col1\n      col2\n    \n  \n  \n    \n      0\n      A\n      0.333333\n    \n    \n      1\n      A\n      0.666667\n    \n    \n      2\n      A\n      1.000000\n    \n    \n      3\n      B\n      0.666667\n    \n    \n      4\n      B\n      0.833333\n    \n    \n      5\n      B\n      1.000000\n    \n  \n\n\n\n\nschema: col1:str,col2:float"
  },
  {
    "objectID": "Time Series/resources.html",
    "href": "Time Series/resources.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "statsforecast\nmlforecast\nNeuralforecast\nNixtla\nGreykite\nprophet"
  },
  {
    "objectID": "Time Series/03_stats_models.html",
    "href": "Time Series/03_stats_models.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Autoregressive(AR) models\nMoving Average(MA) models\nHierarchical models\n\n\n\n\nAssumes data is iid\nIn time series data points are correlated to each other\n\n\n\n\n\n\n\n\n\n\n\n\n\nFuture values of a time series are a function of its past values\nThis is the first model people use when they have no information other than the time series itself\nRegression on past values to predict future values\nStationarity is required for AR models\nAssumption of stationarity - Expected value of the process must be the same at all times\nweak stationarity - Mean and variance of a process be time invariant\nThe lag which should be considered for modeling can be found manually by looking at PACF plots\nAuto Arima can be used to find the lag automatically\nAfter modelling we should check the ACF of residuals to see if there are any patterns\nBox Jenkins hypothesis test should be done to see if there is any pattern which is left out by the model\nUse for short term forecast. In the long term the forecast will be mean value of the data\n\n\n\n\n\nThese are not Moving Averages\nThis is like linear regression but we consider errors with respect to lag. Here I think error means difference from the mean.\nThe lag which should be considered for modeling can be found manually by looking at ACF plots\nAuto Arima can be used to find the lag automatically\nAfter modelling we should check the ACF of residuals to see if there are any patterns and model needs to be improved\nLjung-Box test hypothesis test should be done to see if there is any pattern which is left out by the model. Null hypothesis means data does not exhibit serial correlation. H1 is data exhibit serial correlation\nDon’t do the forecast for more than number of time lag considered for modelling.  \n\n\n\n\n\nAuto Regressive Integrated Moving Average.\nIntegrated means differencing. we take difference of a value from its previous time step (I think so), to make the time series stationary. In practice we should not do this more than two times.\nBoth Auto regression and Moving Average are considered for modelling\nARIMA(P, D, Q) - P is number of time lags to be considered for AR. D is number of times differencing considered and Q is time lags to be considered for the MA model\nSARIMA(Seasonal ARIMA) - Model assumes multiplicative seasonality\nARCH, GARCH Family\n\nARCH - Autoregressive Condition Heteroskedasticity - Variance is not constant. Variance of a process is modeled as an autoregressive process rather than the process itself.\n\n\n\n\n\n [Disadvantages] (/Images/stats_ts_disad.png)"
  },
  {
    "objectID": "Time Series/02_exploratory_analysis.html",
    "href": "Time Series/02_exploratory_analysis.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Often the relationship between data at different points or the change over time that is most informative about how our data behaves\n\n\n\n\nstationarity\nNormal distribution of input variables - Boxcox transformation\n\n\n\n\n\nDoes the time series reflect a system that is “stable” or one that is constantly changing\nOnce we have assessed stability (i.e stationarity), we try to determine whether there are internal dynamics in that series i.e self-correlations\nwhen we think we have found certain behavioral dynamics within the system, we need to make sure we are not identifying relationships based on dynamics that do not in any way imply the causal relationships we wish to discover; hence, we must look for spurious correlations.\n\n\n\n\n\nMany statistical time series models rely on time series being stationary\nStationary time series is one that has fairly stable statistics properties over time\nA stationary time series is one in which a time series measurement reflects a system in a steady state. Mean and variance should remain same and seasonality should not be there for stationarity\nA simple definition of a stationary process is the following: a process is stationary if for all possible lags, k, the distribution of yt, yt+1,…, yt+k, does not depend on t.\nAugmented Dickey-Fuller (ADF) test is the most commonly used metric to assess a time series for stationarity. This test posits a null hypothesis that a unit root is present in a time series. Depending on the results of the test, this null hypothesis can be rejected for a specified significance level, meaning the presence of a unit root test can be rejected at a given significance level. (If unit root is present, then the series is non-stationary)\nA model to help you estimate the mean of a time series with a nonstationary mean and variance, the bias and error in your model will vary over time, at which point the value of your model becomes questionable.\nTime series can be made stationary with a few simple transformations - Log and a square root transformations are popular. Removing a trend is commonly done by differencing. Sometimes a series must be differenced more than once. However, if you find yourself differencing too much (more than two or three times) it is unlikely that you can fix your stationarity problem with differencing.\n\n\n\n\n\nSelf-correlation of a time series is the idea that a value in a time series at one given point in time may have a correlation to the value at another point in time\nAutocorrelation Fuction (ACF) - This generalizes self-correlation by not anchoring to a specific point in time. It is the similarity between observations as a function of the time lag between them\nPartial autocorrelation function (PACF) - The PACF of a time series for a given lag is the partial correlation of the time series with itself at that lag given all the information between the two points in time. PACF shows which data points are informative and which are harmonics of shorter time periods\n\n\n\n\n\nData with an underlying trend is likely to produce spurious correlations"
  },
  {
    "objectID": "Time Series/01_overview.html",
    "href": "Time Series/01_overview.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Time series analysis often comes down to the question of causality: How did the past influence the future?\n\n\n\n\n\nwhat process generated the timestamp, how and when\nTime zone information\nRecord your own actions and see how they are captured\nLocal or universal time\n\n\n\n\n\nForward fill\nMoving average of past values - use only the data that occurred before the missing data point\nInterpolation - Determining the values of missing points based on geometric constraints regarding how we want the overall data to behave\n\n\n\n\n\nTo match the frequency of different time series data\nWe either increase or decrease the timestamp frequency\n\n\n\n\nThe original resolution of the data isn’t sensible\nMatch against data at a lower frequency - we can either downsample, take average, weighted mean etc.\nSelecting out every nth element\n\n\n\n\n\nconvert irregularly sampled time series to a regularly timed one\nInputs sampled at different frequencies\nKnowledge of time series dynamics - Treat as a missing data problem and missing data techniques can be applied.\n\n\n\n\n\nMoving average is used to eliminate measurement spikes, errors of measurement etc.\npurpose of smoothing\nData preparation - Raw data unsuitable\nFeature generation\nPrediction - For many processes prediction is mean, which we get from a smoothed feature\nVisualization - Add signal to a noisy plot\nExponential smoothing - All data points are not treated equally\nMore weight to recent data\n\n\n\nEquation for exponential smoothing\n\n\n\n\n\nDoing smoothing with pandas\n\n\nExponential smoothing does not perform well in case of data with a long-term trend\nHolt’s method and holt-winters smoothing are two exponential smoothing methods applied to data with a trend or with a trend and a seasonality\nKalman filters and LOESS(locally estimated scatter plot smoothing) are other computationally involved methods - These methods leak information from the future - Not appropriate for forecasting\nSmoothing is a commonly used form of forecasting, and you can use a smoothed time series (without lookahead) as one easy null model when you are testing whether a fancier method is actually producing a successful forecast\n\n\n\n\n\n\nSeasonal time series are time series in which behaviors recur over a fixed period. There can be multiple periodicities reflecting different tempos of seasonality, such as the seasonality of the 24-hour day versus the 12-month calendar season, both of which exhibit strong features in most time series relating to human behavior.\n\n\n\n\n\nPython libraries to deal with Timezone - datetime, pytz and dateutil\n\n\n\n\ncurrent time in python\n\n\n\nBe careful with timezone conversions\nDealing with daylight savings can be tricky\n\n\n\n\nProcessing and cleaning time-related data can be a tedious and detail-oriented process.There is tremendous danger in data cleaning and processing of introducing a lookahead! You should have lookaheads only if they are intentional, and this is rarely appropriate."
  },
  {
    "objectID": "Time Series/05_ml_for_ts.html",
    "href": "Time Series/05_ml_for_ts.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "ML for Time Series\n\nWe extract and select features from time series and use the results for building an ML model\nXGBoost and Random forest models can be used for classification. Random forest is not successful in forecasting. Gradient Boosted models are giving good results in case of forecasting.\nClustering of time series - we should use Temporally aware distance metrics for clustering. One such metric is Dynamic Time Warping (DTW). Other distance metrics which are used are ‘Frechet distance’, ‘Pearson correlation’ etc.\nClassification and Forecasting can be combined with clustering in case of any requirement.\n\n\n\nDeep learning for time series\n\nDeep learning did not deliver amazing results for forecasting\nIn case of deep learning model assumptions are not required - stationarity etc. In practice, deep learning is not doing a good job of fitting data with a trend, unless architectures are modified to fit the trend. Preprocessing is required"
  },
  {
    "objectID": "Time Series/04_feature_eng.html",
    "href": "Time Series/04_feature_eng.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Feature Engineering for Time Series\n\nBy describing a time series not with a series of numbers detailing the step-by-setp outputs of a process but rather by describing it with a set of features, we can access ML methods designed for cross sectional data\nThe features can be computed over the entire time series of as rolling or expanding window functions\n\n\nConsiderations for extracting features from Time series\n\nStationarity\nLength of the time series - features may become unstable as the length of time series increases\nDomain knowledge\nExternal considerations\n\n\n\nCatalog of common features\n\nMean and variance\nMaximum and minimum\nDifference between last and first values\nNumber of local maxima and minima\nSmoothness of the time series\nPeriodicity and autocorrelation of the time series\n\n\n\nPackages for feature generation\n\ntsfresh\nThe following categories of features are computed\n\nDescriptive statistics\nPhysics inspired category of indicators - nonlinearity (C3), complexity(cid_ce), friedrich_coefficients(returns coefficients of a model fitted to describe complex nonlinear motion) etc\nHistory-compressing counts\n\n\n*Cesium. This library also has a web-based GUI for feature generation\n\n\nFeature selection\n\nAutomatic feature selection based on automatic feature generation\nFRESH - feature extraction based on scalable hypothesis tests - Implemented in tsfresh\nRecursive Feature Elimination (RFE) can be used (forward, backward methods)"
  },
  {
    "objectID": "industry_use_cases/insurance.html",
    "href": "industry_use_cases/insurance.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Vehicle damage assessment\nFraud Detection\nClaims management to ease the process\n\n\n\n\nSeverity of damage\nWhich parts are damaged\nDoes it require repair or replacement\nEstimating the cost of repair or replacement\n\n \n\n\n\n\nFinding a proper data set\nPrivacy concerns\nOptimizing performance and cost\nIf possible building 3D models and using them to train classifiers to achieve higher precision\n\nAltoros Blog post\n\n\n  \n\n\n\nWired article\n\n\n\nJust Analytics Flowmagic\n\n\n\nGithub Repo"
  },
  {
    "objectID": "math_for_ai/theory/04_neural_network.html",
    "href": "math_for_ai/theory/04_neural_network.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "We don’t know the true function which generated the data. Neural networks training function are used to approximate the true function which generated the data. Training function is used to generate the predictions. Universal approximation theorem assert that neural networks can approximate the underlying functions to any precision.\n\n\n\n\n\nAn exmaple of a loss function (in the context of neural networks). It quantifies the amount of information lost when the learned distribution is used to approximate the true distribution or the relative entropy of the true distribution with respect to the learned distribution\n\n\n\n\n\nThe landscape of the loss function is non-convex, it has local minima\nDeep learning still lacks a solid theoretical foundation\n\n\n\n\nActivation functions\n\n\n\n\n\nGradient Descent\n\n\n\nDont initialize with all zeros or all equal numbers. This will diminish the network’s ability to learn different features, since different nodes will output exactly the same numbers.\nAs neural networks are non-convex, how we initiate the w’s matters a lot. weights are sampled either from the uniform distribution over small intervals (Xavier Glorot initialization) or from Gaussian distribution with a preselected mean and variance (Kaiming He initialization)\nThe scale of the features affects the performance of the gradient descent. Very different scales of the input features change the shape of the bowl of the loss function, making minimization process harder. This means that the shape of the bowl of loss function is a long narrow valley. Gradient descent zigzags as it tries to locate the minimum and slowing down the convergence considerably.\nNear the minima (local or global), flat regions or saddle points of the loss function, the gradient descent method crawls.\nStochastic gradient descent - the points hop a lot, as opposed to following a more consistent route toward the minimum.\nLoss functions like mean squared error, cross entropy and hinge loss are all convex but not nondecreasing (what is non descreasing???)\n\n\n\n\n\n\n\nUsually about twenty percent of the input layer’s nodes and about half of each of the hidden layers nodes are randomly dropped.\n\n\n\n\n\nThe error on the validation set start increasing after a decrease. This indicates the start of overfitting and we stop training.\n\n\n\n\n\nNormalize the inputs to each layer of the network. Inputs to each layer will have mean zero and variance one. \n\n\n\n\n\nSome regularization is always good\nRidge differentiation is more stable\nIf we need to select features than use lasso\nElasticnet is preferred over lasso as it might behave badly when features are greater than data instances or when several features are correlated."
  },
  {
    "objectID": "math_for_ai/theory/02_distributions.html",
    "href": "math_for_ai/theory/02_distributions.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Distributions\n\nContinous distribution\n\nUniform Distribution\nNormal Distribution\nProbability of observing an exact value is zero\nProbability will exist for an interval\nTo find the probability of a random variable in an interval, we integrate the probability density function over the interval\nTo find the joint probability of two variables between intervals, we double integrate the joint probability density function. For example height between 6.0 and 7.0, weight between 70 and 80 Kgs\n\n\nUniform Distribution\n\nThe probability density function for uniform distribution is constant\nIt is used for generating random numbers from any other probability distribution\n\n\n\nThe Exponential Distribution\n\nIf we happen to know that a certain event occurs at a constant rate , then exponential distribution predicts the waiting time until this event occurs.\nThe time until a machine part fails - This is very useful for reliability field  \n\n\n\nThe Weibull Distribution\n\nSimilar to exponential distribution but it can model rates which occur with increase or decrease with time\nUsed in the field of product life times\nA car will not work if the battery fails, or if a fuse in the gearbox burns out. A Weibull distribution provides a good approximation for the lifetime of a car before it stops working, after accounting for its many parts and their weakest link.\nIt is controlled by shape, scale and location\nExponential is a special case of this distribution - constant rate of event occurrence\n\n\n\nThe Log-normal distribution\n\nIf we take logarithms of each value provided in this distribution, we get normal distribution.\nThis is a good distribution to use when using skewed data with low mean, large variance and assuming only positive values\nLog normal distribution will appear when we take product of many positive sample values\nParameters - shape, scale and location\nReal world examples\n\nvolume of gas in a petroleum reserve\nThe ratio of the price of a security at the end of one day to its price at the end of the day before.\n\n\n\n\nChi-Squared distribution\n\nIt is a distribution for the sum of squares of normally distributed independent random variables\nStatistical test associated:-\n\nThe goodness of fit test - How far is our expectation from observations\nIndependence and homogeneity of data features test\n\n\n\n\nNormal Distribution\n\nThe distribution is symmetrical from the mean\nMean, Median and Mode are the same\n68% of the data within one standard deviation, 95% of the data falls within 2 SD and 99.7% within 3 SDs.\nCentral limit theorem - The average of independent random samples from any distribution is normally distributed.\nIf you happen to find yourself in a situation where you are uncertain and have no prior knowledge about which distribution to use for your application, the normal distribution is usually a reasonable choice. In fact, among all choices of distributions with the same variance, the normal distribution is the choice with maximum uncertainty, so it does in fact encode the least amount of prior knowledge into your model.\n\n\n\n\nEquation\n\n\n\n\nStudent’s t-distribution\n\nUsed when sample size is small and population variance is unknown\n\n\n\nGamma distribution\n\nTime until n independent events occur, instead of only one event  \n\n\n\nBeta distribution\n  \n\n\n\nDiscrete distribution\n\nBinomial distribution\n\nProbability of obtaining a certain number of successess when repeating one experiment,independently, multiple times\nParameters - n (number of experiments), p - predefined probablity of success\nReal world examples\n\nNumber of patients who will develop side-effects for vaccine\nNumber of ad-clicks that will result in a purchase\nNumber of customers who will default on monthly credit card payments\n\n\n  * The special case when N=1 corresponds to the Bernoulli distribution * \n\n\nPoisson Distribution\n\nIt will predict the number of rare events that will occur in a given period of time\nparameter - Event occur at a known average rate (lambda)\nReal world examples\n\nNumber of babies born in a given hour\nThe number of earthquakes happening within a particular time period\n\n\n\n\n\npoisson Distribution\n\n\n\n\n\npoisson Distribution with different lambda values\n\n\n\n\nGeometric Distribution\n\nIt predicts the number of trials needed before we obtain a success when performing independent trials, each with a known probability p for success.\nParameter - Probability of success\nNumber of weeks a company can function without experiencing failure\n\n\n\nNegative Binomial\n\nNumber of independent trails needed to obtain a certain number of successes\n\n\n\nHypergeometric Distribution\n\nSimilar to binomial but the trials are not independent\n\n\n\nNegative Hypergeometric Distribution\n\nNumber of dependent trails needed before we obtain a certain number of successes"
  },
  {
    "objectID": "math_for_ai/theory/03_fitting_functions_to_data.html",
    "href": "math_for_ai/theory/03_fitting_functions_to_data.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Fitting functions to data\n\nLinear functions can be either concave or convex. when we take the maximum of linear functions, we are compensated with convexity\nIf we have non-linear convex function, then the maximum of all the linear functions that stay below our function is exactly equal to it. This gives us a path to exploit the simplicity of linear functions when we have convex functions.\nFor convex functions the global and local minima are same\nA non-linear function can be either convex or non-convex\nFor non-convex functions with peaks, valleys and saddle points we run the risk of getting stuck at local minima\nNumerical method’s search for minimum happens on the ground level and does not search in the same space dimension as the landscape of the function is embedded in.\nIt is important to locate, on ground level, a direction that quickly decreseas the function height and how far we can move in that direction on ground level (learning rate) while still decreasing the function height above us\n\n\nLinear regression\n\nAnalytic and numeric method exist for linear regression. For others analytic solution is difficult, so we depend on numeric methods \n\n\n\nLogistic regression\n \n\n\nMulticlass logistic function\n  \n\n\nSVM\n\nSeek to separate a labeled data using widest possible margin\nAn optimal highway of separation instead of thin line of separation\nSVM uses hinge loss function. When the loss is greater than 1 then it is a high penalty, if the loss is between 0 and 1 then it is still penalized and if the loss is 0, there is no penalty.    \n\n\n\nDecision Tree\n\nIt is a non-parametric model, it does not fix the shape of the function ahead of time.\nThis flexibility makes it overfitting to the data\nEntropy and Gini Index are used to measure the importance of a feature. Gini Index is less expensive, so it a default in most packages.\nEntropy approach - Feature split that provides the maximum information gain\nGini Impurity - Split that provides lowest average Gini Impurity\n\n       \n\nPros & Cons\n\nunstable\nSensitive to rotations in the data, since their decision boundaries are ususally horizontal and vertical. Fix - Transform the data to match its principal axes, using SVD\noverfit the data\n\n\n\n\nK-Means clustering\n\nIt minimizes the variance (the squared euclidean distances to the mean) within each cluster\n\n\n\nFeature selection\n\nF-test and mutual information test for feature selection\n\n\nWhat is bagging, pasting, random patches and stacking?\n\nBagging is taking random subsets with replacement and pasting taking random subsets without replacement\nRandom patches means taking a subset of features for modelling\nStacking is using a prediction model on all the results of all models to get final prediction\n\n\n\n\nGradient Descent\n\nGradient - The effect on the loss by changing a single parameter, while keeping everything else constant. How much the loss changes if one parameter changes a little bit\nGradient is steepness. Learning rate can be equated to step size and the steepness dictates the number of steps (relative impact of the parameter). we then take a number of steps that’s proportional to the relative impact: more impact, more steps\nAs we will be using a single learning rate for all the parameters, the size of the learning rate is limited by the steepest curve. All other curves will be using suboptimal learning rate, given their shapes. (why can’t we use different learning rates for different curves? - Is it a implementation problem or other challenge?)\nTo make the gradients equally steep - we should do feature standardization or normalization Impact on loss with and without scaling for two parameters. We should use training set only to fit the standardscaler, we should use its transform method to apply the preprocessing step to all datasets: training, validation and test\n\n\n\n\nchange in loss\n\n\n\n\n\nEffect of learning rate on loss"
  },
  {
    "objectID": "math_for_ai/theory/01_intro.html",
    "href": "math_for_ai/theory/01_intro.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "contionous variables\n\nProbability density function\n\nDiscrete variables\n\nProbability mass function\n\nMarginal probability distribution\n\nGetting the probability distribution of a single or multiple variables from the full joint probability distribution of multiple random variables.\n\nJoint probability distribution\n\nProbability distribution of two or more variables occuring together\nIf we fix the value of one of the variable, we get a distribution proportional to conditional probability distribution\nJoint probability distribution = marginal probability distribution + conditional probability distribution\nJoint probability in case of dependent variables is not separable. we need to store each value for every co-occurance between the variables. (curse of dimensionality)\n\n\n\n\n\n\n\nEstimating conditional probabilities when full joint probability distribution is not known\n\n\n\n\n\nMix probability distributions\n\n\n\n\n\nIf the data we care for is not sampled or observed, we speculate on it using the langauge of expectation.\nLaw of large numbers - When sample size goes to infinity, expectation matches the sample mean\n\n\n\n\n\nCorrelation works on normalized random variables\nCovariance works on unnormalized random variables"
  },
  {
    "objectID": "transformers/image_worth_16_16.html",
    "href": "transformers/image_worth_16_16.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Split an image into patches and provide the sequence of linear embeddings of these patches as an input to Transformer.\nImage patches are treated the same way as tokens (words) in an NLP application. We train the model on image classification in supervised fashion.\nTransformers lack some of the inductive biases inherent to CNNs such as translation equivariance and locality and therefore do not generalize well when trained on insufficient amounts of data\nIt is often beneficial to fine-tune at higher resolution than pre-training\nViT performs better when trained on large scale data and transferred to tasks with fewer datapoints\n\n\n\n\nViT Architecture\n\n\n\nThe image patches are flattened and mapped to D dimensions with a trainable linear projection.These are called patch embeddings\nLearnable embedding is prepended to the sequence of embedded patches, whose state at the output of the transformer encoder serves as the image representation.\nLearnable 1D position embeddings are added to patch embeddings to retain positional information\nMLP contains two layers with GELU (Gaussina Error Linear Units) non-linearity\n\n\n\n\nVariants of ViT\n\n\n\nRegularization used - weight decay, dropout and label smoothing\n\nIn the lowest layers some heads attend to most of the image, showing that the ability to integrate information globally is indeed used by the model. The attention distance increases with network depth. Globally, the model attends to image regions that are semantically relevant for classification\n\n\nResearch Paper"
  },
  {
    "objectID": "transformers/data_eff_img_transformer.html",
    "href": "transformers/data_eff_img_transformer.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "This uses a teacher-student strategy specific to transformers. It relies on a distillation token ensuring that the student learns from the teacher through attention.\n\n\nResearch paper"
  },
  {
    "objectID": "transformers/vision_transformer_architectures.html",
    "href": "transformers/vision_transformer_architectures.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "pyramid vision transformer\n\n\n\nPyramid vision transformer\n\n\nTo overcome the quadratic complexity of the attention mechanism, Pyramid Vision Transformers (PVTs) employed a variant of self-attention called Spatial-Reduction Attention (SRA), characterized by a spatial reduction of both keys and values. By applying SRA, the spatial dimensions of the features slowly decrease throughout the model.\n\n\nSWIN Transformer"
  },
  {
    "objectID": "transformers/transformers_from_scratch.html",
    "href": "transformers/transformers_from_scratch.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "The trick to use one-hot vectors to pull out a particular row of a matrix is at the core of how transformers work.\n\n\n\nMatrix multiplication with one-hot vectors\n\n\n\n\n\n\n\n\nMarkov model\n\n\nMarkov chains can be expressed conveniently in matrix form.\n\n\n\nMarkov chain represented as a matrix form\n\n\nUsing one-hot vector to pull out the relevant row and shows the probability distribution of what the next word will be.\n\n\n\nUsing one-hot vector ro pull out the transition probabilities associated with given word\n\n\n\n\n\nThe mask has the effect of hiding a lot of the transition matrix (which is not relevant for the word combinations)\n\n\n\nMasking\n\n\n\n\n\nMasking hiding transition matrix\n\n\n\n\n\n\n\n\n\nAttention\n\n\nThe highlighted piece in the above formula Q represents the feature of interest nad the matrix K represents the collection of masks (which words are important for the given query of interest)\n\n\n\nMatrix multiplication of query with masks\n\n\n\n\n\nThe following happens in Feed Forward Network * Feature creation matrix multiplication * Transition matrix multiplication * ReLU nonlinearity\n\n\n\nFFN\n\n\n\n\n\nThe area in the architecture the above operations happen\n\n\n\n\nEmbedding can be learned during training\n\n\n\nInput Embedding\n\n\n\n\n\n\n\n\nWorking of positional Embedding\n\n\n\n\n\n\n\n\nConverting the embedding to original vocabulary\n\n\nTo get the softmax of the value x in a vector, divide the exponential of x, e^x, by the sum of the exponentials of all the values in the vector.\n\n\n\nDeembedding\n\n\n\n\n\n\n\n\nMatrix Dimensions\n\n\n\n\n\nMatrix Dimensions for Multihead attention\n\n\n\n\n\nScaled Dot-product attention\n\n\n\n\n\n\nThey help keep the gradient smooth\nThe second purpose is specific to Transformers - Preserving the original input sequence. Even with a lot of attention heads, there’s no guarantee that a word will attend to its own position. It’s possible for the attention filter to forget entirely about the most recent word in favor of watching all of the earlier words that might be relevant. A skip connection takes the original word and manually adds it back into the signal, so that there’s no way it can be dropped or forgotten.\n\n\n\n\nskip connections\n\n\n\n\n\nThe values of the matrix are shifted to have a mean of zero and scaled to have standard deviation of one\n\n\n\nCross-attention works just like self-attention with the exception that the key matrix K and value matrix V are based on the output of the final encoder layer, rather than the output of the previous decoder layer. The query matrix Q is still calculated from the results of the previous decoder layer. This is the channel by which information from the source sequence makes its way into the target sequence and steers its creation in the right direction. It’s interesting to note that the same embedded source sequence is provided to every layer of the decoder, supporting the notion that successive layers provide redundancy and are all cooperating to perform the same task.\n\n\n\ncross attention\n\n\n\n\n\n\nBrandon Rohrer blog post"
  },
  {
    "objectID": "transformers/attention.html",
    "href": "transformers/attention.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Attention\nThe context vector was a bottleneck for the RNN models. It made it challenging for models to process long sentences. In RNN models a single hidden state was passed between the encoder and the decoder.\nIn attention models the encoder passes a lot more data to the decoder.Instead of passing the last hidden state of the encoding stage, the encoder passes all the hidden states to the decoder.\nThe decoder will look at the set of hidden states it received. It will give each hidden state a score. Multiply each hidden state by its softmaxed score. This scoring is done at each time step on the decoder side.\nThis is how the deocder works:-\n\nThe attention decoder RNN takes in the embedding of the  token, and an initial decoder hidden state.\nThe RNN processes its inputs, producing an output and a new hidden state vector (h4). The output is discarded. 3.Attention Step: We use the encoder hidden states and the h4 vector to calculate a context vector (C4) for this time step.\nWe concatenate h4 and C4 into one vector.\nWe pass this vector through a feedforward neural network (one trained jointly with the model).\nThe output of the feedforward neural networks indicates the output word of this time step.\nRepeat for the next time steps\n\n\nReference\nJay Alammar Blog post"
  },
  {
    "objectID": "transformers/vision_transformer.html",
    "href": "transformers/vision_transformer.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Transformers cannot process grid-structured data. It needs sequences. We need to convert non-sequential signal to a sequence.\n\n\n\nSplit an image into patches\nFlatten the patches\nProduce lower-dimensional linear embeddings from the flattened patches\nAdd positional embeddings\nFeed the sequence as an input to standard transformer encoder\nPretrain the model with image labels\nFinetune on the downstream dataset for image classification\n\nThe architecure is the same as the original Attention is all you need paper. The number of blocks is changed.\n\n\n\nProposed Architecture for Vision Transformers\n\n\nThere is no decoder in the architecture.\n\n\n[CLS] embedding begins as a “blank slate” for each sentence in BERT. The final output from [CLS] embedding is used as the input into a classification head during pretraining.Using a “blank slate” token as the sole input to a classification head pushes the transformer to learn to encode a “general representation” of the entire sentence into that embedding. ViT applies the same logic by adding a learnable embedding.\n\n\n\nLearnable Embedding\n\n\n\n\n\nSpecifically, if ViT is trained on datasets with more than 14M (at least :P) images it can approach or beat state-of-the-art CNNs.\nViT is pretrained on the large dataset and then fine-tuned to small ones. The only modification is to discard the prediction head (MLP head) and attach a new D KD×K linear layer, where K is the number of classes of the small dataset.\nEven though many positional embedding schemes were applied, no significant difference was found. Hence, after the low-dimensional linear projection, a trainable position embedding is added to the patch representations.\nDeep learning is all about scale. Indeed, scale is a key component in pushing the state-of-the-art. In this study7 by Zhai et al. from Google Brain Research, the authors train a slightly modified ViT model with 2 billion parameters, which attains 90.45% top-1 accuracy on ImageNet7. The generalization of this over-parametrized beast is tested on few-shot learning: it reaches 84.86% top-1 accuracy on ImageNet with only 10 examples per class.\n\n\n\n\nViTs are robust against data corruptions, image occlusions and adversarial attacks\n\n\n\n\nNeed More data In CNNs the model knows how to focus, we tell them on how much to focus. In case of transformers, the model does not know how to focus. It pays attention to all the patches and learns where to focus during the training process. Due to this they need huge amounts of data.\nOverfitting to small datasets Due to their flexibility they are notorious for overfitting on small datasets. A lot of data augmentation would be required to make it work on small datasets.\n\n\n\n\nAI Summer blog Pinecone blog"
  },
  {
    "objectID": "transformers/how_to_train_ViT.html",
    "href": "transformers/how_to_train_ViT.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Without the translational equivariance of CNNs, ViT models are generally found to perform best in settings with large amounts of training data or to require strong AugReg schemes to avoid overfitting\nCarefully selected regularization and augmentations roughly correspond to a 10x increase in training data size\n\n\n\n\nRegularization used - Dropout to intermediate activations of ViT, stochastic depth regularization\nData augmentations - Mixup, RandAugment\nWeight decay\n\n\n\n\n\nFor most practical purposes, transferring a pre-trained model is both more cost-efficient and leads to better results\n\n\n\n\nOne approach is to run downstream adaptation for all available pre-trained models and then select the best performing model, based on validation score on the downstream task. (expensive)\nSelect a single pre-trained model based on the upstream validation accuracy and then only use this model for adaptation (cheaper)\nCheaper strategy works equally well as the more expensive strategy in the majority of scenarios\n\n\n\n\n\nResearch Paper"
  },
  {
    "objectID": "transformers/object_detection_transformers.html",
    "href": "transformers/object_detection_transformers.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Object Detection with DETR\n\nNon-maximum supression and anchor generation are not required\nThey don’t require customized layers\n\n\n\n\nDETR\n\n\n\nDETR predicts all objects at once and is trained end-to-end with a set loss function which performs bipartite matching between predicted and ground-truth objects.\nPerforms better on larger objects\nA segmentation head trained on top of a pre-trained DETR works on Panoptic segmentation\nNo autogression in decoder\nUse transformers with parallel decoding\nDirectly predicting the set of detections with absolute box prediction\n\n\n\n\nDETR Architecture"
  },
  {
    "objectID": "transformers/gaps.html",
    "href": "transformers/gaps.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Considerations and pitfalls of attention for your own data Advantages and disadvantages of ViT for Images ViT Vs CNN Classification on Videos Validation for video classification other classification metrics Scene change detection in videos Continual learning - Update model Closing the presentation with summary"
  },
  {
    "objectID": "transformers/transformers.html",
    "href": "transformers/transformers.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "The Transformer is a model which uses attention to boost the speed of the training. The transformer lends itself to parallelization.\nThe transformers will consist of encoders and decoders.\nAn encoder consists two sub-layers self-attention and feed forward neural network. A decoder along with these two sub-layers will also consist of an attention layer between them to focus on relevant parts of the input sentence.\n\n\n\nComponents of Encoder and Decoder\n\n\n\n\nEmbedding happens in the bottom most encoder. The word in each position flows through its own path in the self-attention layer. The feed-forward layer does not have those dependencies and various paths can be executed in parallel.\n\n\n\nProcessing by an Encoder\n\n\n\n\n\n\n\nself attention\n\n\n\n\n\nSelf attention at a glance\n\n\n\n\n\nSteps for calculating self-attention\n\n\n\n\n\nself attention in matrix form\n\n\n\n\n\nSelf attention in a single formula\n\n\n\n\n\nIt expands the model’s ability to focus on different positions.\nIt gives the attention layer multiple “representation subspaces”\n\n\n\nMulti-headed attention\n\n\nIn the original paper, eight attention heads were used. This way we will end up with eight different Z matrices. The feed-forward layer is not expecting eight matrices – it’s expecting a single matrix (a vector for each word). So we need a way to condense these eight down into a single matrix.\n\n\n\nConcatenating Attention Heads\n\n\n\n\n\nMultiple attention heads focusing on different representations\n\n\n\n\n\nThis will account for the order of the words in the input sequence.\n\n\n\npositional encoding\n\n\n\n\n\nEach sub-layer in each encoder has a residual connection around it, and is follwed by a layer-normalization step\n\n\n\nResidual connection and layer normalization\n\n\n\n\n\n2 layer encoder decoder architecture\n\n\n\n\n\n\nThe encoder start by processing the input sequence. The output of the top encoder is then transformed into a set of attention vectors K and V. These are to be used by each decoder in its “encoder-decoder attention” layer which helps the decoder focus on appropriate places in the input sequence.\n\n\n\nDecoder\n\n\nThe following steps repeat the process until a special symbol is reached indicating the transformer decoder has completed its output. The output of each step is fed to the bottom decoder in the next time step, and the decoders bubble up their decoding results just like the encoders did. And just like we did with the encoder inputs, we embed and add positional encoding to those decoder inputs to indicate the position of each word.\nIn the decoder, the self-attention layer is only allowed to attend to earlier positions in the output sequence. This is done by masking future positions (setting them to -inf) before the softmax step in the self-attention calculation.\nThe “Encoder-Decoder Attention” layer works just like multiheaded self-attention, except it creates its Queries matrix from the layer below it, and takes the Keys and Values matrix from the output of the encoder stack.\n\n\n\nSteps in Decoding\n\n\n\n\n\nThe Linear layer is a simple fully connected neural network that projects the vector produced by the stack of decoders, into a much, much larger vector called a logits vector.\nLet’s assume that our model knows 10,000 unique English words (our model’s “output vocabulary”) that it’s learned from its training dataset. This would make the logits vector 10,000 cells wide – each cell corresponding to the score of a unique word. That is how we interpret the output of the model followed by the Linear layer.\nThe softmax layer then turns those scores into probabilities (all positive, all add up to 1.0). The cell with the highest probability is chosen, and the word associated with it is produced as the output for this time step.\n\n\n\nworking of final and softmax layer\n\n\n\n\n\nThe illustrated Transformer"
  },
  {
    "objectID": "transformers/Attention_for_vision.html#topics-covered",
    "href": "transformers/Attention_for_vision.html#topics-covered",
    "title": "Computer Vision Using Transformers",
    "section": "Topics Covered",
    "text": "Topics Covered\n\n\nCore concepts of Attention is all you need\nAdapting Attention to vision\nUse case Implementation"
  },
  {
    "objectID": "transformers/Attention_for_vision.html#attention-is-all-you-need",
    "href": "transformers/Attention_for_vision.html#attention-is-all-you-need",
    "title": "Computer Vision Using Transformers",
    "section": "Attention is all you need",
    "text": "Attention is all you need"
  },
  {
    "objectID": "transformers/Attention_for_vision.html#why-a-new-architecture-is-required",
    "href": "transformers/Attention_for_vision.html#why-a-new-architecture-is-required",
    "title": "Computer Vision Using Transformers",
    "section": "Why a new architecture is required?",
    "text": "Why a new architecture is required?\n\n\nRNNs process words sequentially\nRNN cannot consider long sequence lengths"
  },
  {
    "objectID": "transformers/Attention_for_vision.html#attention-transformer-architecture",
    "href": "transformers/Attention_for_vision.html#attention-transformer-architecture",
    "title": "Computer Vision Using Transformers",
    "section": "Attention Transformer Architecture",
    "text": "Attention Transformer Architecture"
  },
  {
    "objectID": "transformers/Attention_for_vision.html#blocks-of-the-architecture",
    "href": "transformers/Attention_for_vision.html#blocks-of-the-architecture",
    "title": "Computer Vision Using Transformers",
    "section": "Blocks of the Architecture",
    "text": "Blocks of the Architecture\n\n\nEmbedding layer\n\nReduce the dimension of word tokens\nProjection to latent space\n\nPositional Encoding\n\nTo track the relative position of the words"
  },
  {
    "objectID": "transformers/Attention_for_vision.html#self-attention",
    "href": "transformers/Attention_for_vision.html#self-attention",
    "title": "Computer Vision Using Transformers",
    "section": "Self Attention",
    "text": "Self Attention"
  },
  {
    "objectID": "transformers/Attention_for_vision.html#self-attention-1",
    "href": "transformers/Attention_for_vision.html#self-attention-1",
    "title": "Computer Vision Using Transformers",
    "section": "Self Attention",
    "text": "Self Attention"
  },
  {
    "objectID": "transformers/Attention_for_vision.html#self-attention-2",
    "href": "transformers/Attention_for_vision.html#self-attention-2",
    "title": "Computer Vision Using Transformers",
    "section": "Self Attention",
    "text": "Self Attention"
  },
  {
    "objectID": "transformers/Attention_for_vision.html#why-multi-head-attention",
    "href": "transformers/Attention_for_vision.html#why-multi-head-attention",
    "title": "Computer Vision Using Transformers",
    "section": "Why Multi-Head Attention",
    "text": "Why Multi-Head Attention\n\n\nIt expands the models ability to focus on different positions\nIt gives the attention layer multiple “representation subspaces”"
  },
  {
    "objectID": "transformers/Attention_for_vision.html#importance-of-attention",
    "href": "transformers/Attention_for_vision.html#importance-of-attention",
    "title": "Computer Vision Using Transformers",
    "section": "Importance of Attention",
    "text": "Importance of Attention\n\n\nEncoder providing a context to the decoder query by providing keys and values\nEach position in the encoder can attend to all positions in the previous layer of encoder\nEach position in decoder attending to all positions in the decoder"
  },
  {
    "objectID": "transformers/Attention_for_vision.html#skip-connections",
    "href": "transformers/Attention_for_vision.html#skip-connections",
    "title": "Computer Vision Using Transformers",
    "section": "Skip connections",
    "text": "Skip connections\n\n\nSkip connection help a word to pay attention to its own position\nKeep the gradients smooth"
  },
  {
    "objectID": "transformers/Attention_for_vision.html#steps-in-decoder",
    "href": "transformers/Attention_for_vision.html#steps-in-decoder",
    "title": "Computer Vision Using Transformers",
    "section": "Steps in Decoder",
    "text": "Steps in Decoder"
  },
  {
    "objectID": "transformers/Attention_for_vision.html#adapting-attention-to-vision",
    "href": "transformers/Attention_for_vision.html#adapting-attention-to-vision",
    "title": "Computer Vision Using Transformers",
    "section": "Adapting Attention to Vision",
    "text": "Adapting Attention to Vision"
  },
  {
    "objectID": "transformers/Attention_for_vision.html#vision-transformer",
    "href": "transformers/Attention_for_vision.html#vision-transformer",
    "title": "Computer Vision Using Transformers",
    "section": "Vision Transformer",
    "text": "Vision Transformer\n\nViT Architecture"
  },
  {
    "objectID": "transformers/Attention_for_vision.html#vit-vs-cnn",
    "href": "transformers/Attention_for_vision.html#vit-vs-cnn",
    "title": "Computer Vision Using Transformers",
    "section": "ViT Vs CNN",
    "text": "ViT Vs CNN\n\n\nInductive Bias and Locality Vs Global"
  },
  {
    "objectID": "transformers/Attention_for_vision.html#vit-vs-cnn-1",
    "href": "transformers/Attention_for_vision.html#vit-vs-cnn-1",
    "title": "Computer Vision Using Transformers",
    "section": "ViT Vs CNN",
    "text": "ViT Vs CNN\n\n\nInductive Bias and Locality Vs Global\nFlexibility\nCNN works with less amount of data than ViT\nSpecifically, if ViT is trained on datasets with more than 14M (at least) images it can approach or beat state-of-the-art CNNs."
  },
  {
    "objectID": "transformers/Attention_for_vision.html#vit-vs-cnn-2",
    "href": "transformers/Attention_for_vision.html#vit-vs-cnn-2",
    "title": "Computer Vision Using Transformers",
    "section": "ViT Vs CNN",
    "text": "ViT Vs CNN\n\n\nTransformer models are more memory efficient than ResNet models\nViT are prone to over-fitting due to their flexibility\nTransformers can learn meaningful information even in the lowest layers"
  },
  {
    "objectID": "transformers/Attention_for_vision.html#vit-vs-cnn-3",
    "href": "transformers/Attention_for_vision.html#vit-vs-cnn-3",
    "title": "Computer Vision Using Transformers",
    "section": "ViT Vs CNN",
    "text": "ViT Vs CNN\n\n\nViT reaches 84.86% top-1 accuracy on ImageNet with only 10 examples per class.\nViT is suitable for Transfer learning\nViTs are robust against data corruptions, image occlusions and adversarial attacks"
  },
  {
    "objectID": "transformers/Attention_for_vision.html#hybrid-architectures",
    "href": "transformers/Attention_for_vision.html#hybrid-architectures",
    "title": "Computer Vision Using Transformers",
    "section": "Hybrid Architectures",
    "text": "Hybrid Architectures\n\n\nCNN is used to extract features\nThe extracted features are used by the transformers"
  },
  {
    "objectID": "transformers/Attention_for_vision.html#use-case-discussion",
    "href": "transformers/Attention_for_vision.html#use-case-discussion",
    "title": "Computer Vision Using Transformers",
    "section": "Use Case Discussion",
    "text": "Use Case Discussion"
  },
  {
    "objectID": "transformers/Attention_for_vision.html#problem-statement",
    "href": "transformers/Attention_for_vision.html#problem-statement",
    "title": "Computer Vision Using Transformers",
    "section": "Problem Statement",
    "text": "Problem Statement\n\n\nMonitoring when the children are in danger of leaving the front yard\nPredict when childrean are about to leave the yard to trigger the alarm\nWe have videos of children playing sports"
  },
  {
    "objectID": "transformers/Attention_for_vision.html#considerations",
    "href": "transformers/Attention_for_vision.html#considerations",
    "title": "Computer Vision Using Transformers",
    "section": "Considerations",
    "text": "Considerations\n\n\nCollect and train the model on low quality images\nAmount of data available\nTraining time available\nImportance of Interpretability\nDeployment requirements\n\nlatency\nModel size\nInference cost"
  },
  {
    "objectID": "transformers/Attention_for_vision.html#approach",
    "href": "transformers/Attention_for_vision.html#approach",
    "title": "Computer Vision Using Transformers",
    "section": "Approach",
    "text": "Approach\n\n\nObject Detection\nTrain a ML model on the object detection output"
  },
  {
    "objectID": "transformers/Attention_for_vision.html#object-detection-with-detr",
    "href": "transformers/Attention_for_vision.html#object-detection-with-detr",
    "title": "Computer Vision Using Transformers",
    "section": "Object Detection with DETR",
    "text": "Object Detection with DETR\n\nDEtection TRansformer(DETR)"
  },
  {
    "objectID": "transformers/Attention_for_vision.html#object-detection-with-detr-1",
    "href": "transformers/Attention_for_vision.html#object-detection-with-detr-1",
    "title": "Computer Vision Using Transformers",
    "section": "Object Detection with DETR",
    "text": "Object Detection with DETR"
  },
  {
    "objectID": "transformers/Attention_for_vision.html#object-detection-with-detr-2",
    "href": "transformers/Attention_for_vision.html#object-detection-with-detr-2",
    "title": "Computer Vision Using Transformers",
    "section": "Object Detection with DETR",
    "text": "Object Detection with DETR"
  },
  {
    "objectID": "transformers/Attention_for_vision.html#object-detection-with-detr-3",
    "href": "transformers/Attention_for_vision.html#object-detection-with-detr-3",
    "title": "Computer Vision Using Transformers",
    "section": "Object Detection with DETR",
    "text": "Object Detection with DETR"
  },
  {
    "objectID": "transformers/Attention_for_vision.html#why-detr",
    "href": "transformers/Attention_for_vision.html#why-detr",
    "title": "Computer Vision Using Transformers",
    "section": "Why DETR",
    "text": "Why DETR\n\n\nHand-crafted anchors not required\nThey don’t require customized layers\nPredict all objects at once\nPost-processing not required for predicting bounding boxes\nAttention maps can be used for Interpretation"
  },
  {
    "objectID": "transformers/Attention_for_vision.html#why-detr-1",
    "href": "transformers/Attention_for_vision.html#why-detr-1",
    "title": "Computer Vision Using Transformers",
    "section": "Why DETR",
    "text": "Why DETR"
  },
  {
    "objectID": "transformers/Attention_for_vision.html#why-detr-2",
    "href": "transformers/Attention_for_vision.html#why-detr-2",
    "title": "Computer Vision Using Transformers",
    "section": "Why DETR",
    "text": "Why DETR\n\n\nWhat if object detection model doesn’t work?\nA segmentation head can be trained on top of a pre-trained DETR"
  },
  {
    "objectID": "transformers/Attention_for_vision.html#why-detr-3",
    "href": "transformers/Attention_for_vision.html#why-detr-3",
    "title": "Computer Vision Using Transformers",
    "section": "Why DETR",
    "text": "Why DETR"
  },
  {
    "objectID": "transformers/Attention_for_vision.html#why-detr-4",
    "href": "transformers/Attention_for_vision.html#why-detr-4",
    "title": "Computer Vision Using Transformers",
    "section": "Why DETR",
    "text": "Why DETR"
  },
  {
    "objectID": "transformers/Attention_for_vision.html#why-detr-5",
    "href": "transformers/Attention_for_vision.html#why-detr-5",
    "title": "Computer Vision Using Transformers",
    "section": "Why DETR",
    "text": "Why DETR\n\n\nWe can get the FPS for processing videos\nPre-trained Pytorch models and code available"
  },
  {
    "objectID": "transformers/Attention_for_vision.html#data-collection",
    "href": "transformers/Attention_for_vision.html#data-collection",
    "title": "Computer Vision Using Transformers",
    "section": "Data Collection",
    "text": "Data Collection\n\n\nBrainstorming how data needs to be annotated\nStandardizing the definitions\nCollecting Diversified data - Different yards, balls, walls, seasons etc\nLabeling the data - Quality Vs Quantity\nDiscussing ambigious cases with labelers and keep improving"
  },
  {
    "objectID": "transformers/Attention_for_vision.html#modelling",
    "href": "transformers/Attention_for_vision.html#modelling",
    "title": "Computer Vision Using Transformers",
    "section": "Modelling",
    "text": "Modelling\n\nUsing a pre-trained model is both more cost-efficient and leads to better results"
  },
  {
    "objectID": "transformers/Attention_for_vision.html#how-to-select-a-pre-trained-model",
    "href": "transformers/Attention_for_vision.html#how-to-select-a-pre-trained-model",
    "title": "Computer Vision Using Transformers",
    "section": "How to select a pre-trained model",
    "text": "How to select a pre-trained model\n\n\nSpot check all the available pre-trained models (expensive)\nSelect a single pre-trained model based on\n\nAmount of data used for training\nVaried upstream data\nBest upstream validation performance\n\n\nCheaper strategy works equally well as the more expensive strategy in the majority of scenarios\nHow to train your ViT - Research paper"
  },
  {
    "objectID": "transformers/Attention_for_vision.html#validation",
    "href": "transformers/Attention_for_vision.html#validation",
    "title": "Computer Vision Using Transformers",
    "section": "Validation",
    "text": "Validation\n\n\nFalse alarms are better than not raising alarm when necessary\nRecall is more important in this case\nToo many false alarms will reduce the customer satisfaction\nImprove Recall while maintaining Precision at an acceptable level"
  },
  {
    "objectID": "transformers/Attention_for_vision.html#validation-1",
    "href": "transformers/Attention_for_vision.html#validation-1",
    "title": "Computer Vision Using Transformers",
    "section": "Validation",
    "text": "Validation\n\n\nFPS\nmAP for object detection\nRecall, Precision and F1 for the classification"
  },
  {
    "objectID": "transformers/Attention_for_vision.html#deployment",
    "href": "transformers/Attention_for_vision.html#deployment",
    "title": "Computer Vision Using Transformers",
    "section": "Deployment",
    "text": "Deployment\n\n\nFrame sampling instead of predicting on all frames?\nDeployment using platforms like Ray for effective GPU utilization\nDeployment at Edge to meet latency requirements - TensorRT\nUse AB testing framework to deploy new models"
  },
  {
    "objectID": "transformers/Attention_for_vision.html#continual-learning",
    "href": "transformers/Attention_for_vision.html#continual-learning",
    "title": "Computer Vision Using Transformers",
    "section": "Continual Learning",
    "text": "Continual Learning\n\n\nContinual learning suits deep learning models\nIncentivize customers to label the data in real time\nUpdate the model parameters in real time\nContinously monitor the model performance"
  },
  {
    "objectID": "transformers/Attention_for_vision.html#tool-suggestion-for-monitoring",
    "href": "transformers/Attention_for_vision.html#tool-suggestion-for-monitoring",
    "title": "Computer Vision Using Transformers",
    "section": "Tool Suggestion for Monitoring",
    "text": "Tool Suggestion for Monitoring\n\n\nFiftyone"
  },
  {
    "objectID": "transformers/Attention_for_vision.html#summary",
    "href": "transformers/Attention_for_vision.html#summary",
    "title": "Computer Vision Using Transformers",
    "section": "Summary",
    "text": "Summary\n\n\nStart simple\nSmall improvements on regular basis\nLookout for new discovery in the field\nGet Feedback, Iterate, Improve\nKeep the cycle going"
  },
  {
    "objectID": "transformers/attention_is_all_you_need.html",
    "href": "transformers/attention_is_all_you_need.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Sequential nature of RNN precludes parallelization within training examples which becomes critical at longer sequence lengths.\nTransformer allows for significantly more parallelization\n\n\n\n\nTransformer Architecture\n\n\n\n\n\nEncoder * stack of 6 identical layers * To faciliate residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension 512.\nDecoder * Stack of 6 indentical layers\n\n\n\n\n8 attention heads are used\n\n\n\n\nAttention and Multi-head Attention\n\n\n\nFor large values of dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients.To counteract this effect scaling of dot product is done.\n\nMulti-head attention allows the model to jointly attend to information from different representation subspaces at different positions.\n\n\n\n\nIn “encoder-decoder attention” layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence.\nEach position in the encoder can attend to all positions in the previous layer of the encoder.\nSimilarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position.\n\n\n\n\n\nThe input and output dimensions are 512\nThe inner layer has dimensionality of 2048\n\n\n\n\nPositional Embeddings\n\n\nSelf attention could yield more interpretable models"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "My Notes on Data Science, Machine learning and Artificial Intelligence I am learning from different books, videos, courses and Notebooks. These pages are replication of various source material from which I am learning."
  }
]