[
  {
    "objectID": "bayesian_analysis/resources.html",
    "href": "bayesian_analysis/resources.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Resources\n\nBayesian Analysis for Hackers - Book\nBayesian Methods for Hackers - Book Bayesian reasoning and ML Book Good Book on Bayesian Modeling in python"
  },
  {
    "objectID": "bayesian_analysis/Bayesian_analysis.html",
    "href": "bayesian_analysis/Bayesian_analysis.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Bayesian inference is updating your beliefs after considering new evidence.\nWe re-weight our prior beliefs after observing the evience and arrive at posterior probabilities\nAs we gather an infinite amount of evidence, Bayesian results align with frequentist results. For large N, statistical inference is more or less objective. For small N inference is much more unstable, frequentist estimates have more variance and larger confidence intervals.\nBayesian inference preserve the uncertainty that reflects the instability for a small N dataset. \nA good starting point for Bayesian modeling is to think about how the data might have been generated.  \nFor getting the samples from posterior probability Monte Carlo Marko Chain (MCMC) algorithms are used \nIn MCMC only the current location matters (memoryless)\nLaplace Approximations, Variational Bayes methods are also used to approximate the posterior.\nMetropolis sampling is used fElemwiseCategorical for categorical variables.\nA low auto-correlation between posterior samples are preferred\nThinning - If we take every nth sample, we can remove some autocorrelation, with more thinning, the autocorrelation drops quicker. Higher thinning requires more MCMC iterations to achieve the same number of returned samples\nLaw of large numbers - The average of a sequence of random variables from the same distribution converges to the expected value of that distribution. The law of large numbers is only valid as N gets infinitely large:never truly attainable.\nSmall datasets should not be processed using the law of large numbers. We will encounter this problem when try to analyze subset of the data on some specific categories\nIn Bayesian inference are computed using expected values. If we can sample from the posterior distribution directly, we simply need to evaluate averages.If further accuracy is desired, take more samples from the posterior.\nBayesian inference also include loss functions. Instead of returning the average of posterior distribution, we can calculate minimum of the loss function to choose the estimate that minimized the expected loss. Minimum of the expected loss is called ‘Bayes action’\nIn traditional ML it often happens that prediction measure and what frequentist methods are optimizing for are very different. For example in logistic regression trying to optimize cross entropy loss and trying to measure AUC, ROC, Precision etc. On the contrary, Bayes action is equivalent to finding parameters that optimize not parameter accuracy but an arbitary performance measure like loss functions, AUC, ROC, precision / Recall etc.\nPrior selected can be either objective or subjective. Objective prior is following the “principle of indifference” (all values are equally probable). Subjective prior is like adding our belief into the system ex. injecting domain knowledge.\nIf the posterior does not make sense, then it means that the prior may not contain all the information and needs to be updated\nEmpirical Bayes combines frequentist and bayesian inference. Frequentist methods are used to find the prior’s hyperparameters and then proceed with bayesian methods. It was suggested not to use empirical bayes unless you have lots of data \n\n\n\n\nGamma distribution\nWishart distribution - It is a distribution over all positive semi-definite matrices. covariance matrices are positive-definite, hence the Wishart is an appropriate prior for covariance matrices.\nBeta distribution is a generalization of uniform distribution\nA beta prior with binomial observations creates a beta posterior"
  },
  {
    "objectID": "Interesting_packages/01_pandas_coding_assistant.html",
    "href": "Interesting_packages/01_pandas_coding_assistant.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Coding Assistant for Pandas"
  },
  {
    "objectID": "computer_vision/object_detection/introduction_part2.html",
    "href": "computer_vision/object_detection/introduction_part2.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Using sliding window for object detection. choosing the window size is a challenge\nRegion Proposal Netword - use traditional image processing techniques to come up with proposals for objects in the image. This is very fast. Selective search algorithm gives 2000 region proposals in a few seconds on CPU. Use convolutions on the proposed regions\nDetection without proposals\n\n\n\n\n\n\n\n\nPredicting Bounding boxes from Region Proposal\n\n\n\n\n\nInference Steps\n\n\n\n\n\nUsing NMS to remove overlapping bounding boxes\n\n\n\n\n\nUsing NMS to remove overlapping bounding boxes\n\n\n\n\n\n\n\nFast R-CNN flips ROI proposals and covnets used in R-CNN to speed up the process\n\n\n\nFast R-CNN\n\n\n\n\n\nUsing Resnet as backbone for Fast R-CNN\n\n\nROI proposal will crop and resize the feature map. This cropping has to happen in a differential manner. To achieve the same ROI pooling is done.\n\n\n\nROI pooling will provide the same size features for different sized proposal regions\n\n\nAnother method for cropping the feature maps is to use ROI Align method\nDuring inference, 90% of the time is consumed by ROI Proposal which is done on a CPU\n\n\n\n\n\n\n\n\nLearnable Regional Proposal Network (RPN)\n\n\nRPN Network uses image features and anchor boxes to generate proposals. This network uses K-different anchor boxes at each point.\nFor each anchor box at each point the network classifies if the anchor is an object and also predicts the box transforms.\nThese inputs are provided to the convolutional network by the RPN to predict the class and bounding boxes.\n\n\n\nWorking of RPN with Anchor boxes\n\n\n\n\n\n\n\nUsing only the RPN network to predict the final categories and bounding box. (Getting rid of the second stage to speed up the process)\n\n\n\nTwo stages in Faster R-CNN\n\n\n\n\n\nSSD Working\n\n\n\n\n\n\n\nAverage precision provides a balance between precision and recall\n\n\n\ncalculating mean Average Precision\n\n\n\n\n\ncalculating the Area under the PR curve to get Average Precision\n\n\n\n\n\nAverage the Average precision for all categories to get Mean Average Precision\n\n\nIn practice we calculate the mAP at various thresholds and take the average\n\n\n\nThis model is jointly trained on 4 losses\n\n\n\n\n\n\n\nChoose appropriate backbone\nVery big models work better\nTrain longer\nusing multiscale backbone: Feature Pyramid Networks\nSingle stage methods have improved\nTest-time augmentation pushes numbers up\nBig ensembles, more data, etc. provides better accuracy\n\n\n\n\n\nMichigan youtube videos\nMichigan slides for object detection\nMichigan object detection part 2 slides"
  },
  {
    "objectID": "computer_vision/object_detection/localization.html",
    "href": "computer_vision/object_detection/localization.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Image Classification and Localization\n\nThe difference between localization and detection is that in localization, we know in advance that the image will consist of only one object. We classify and predict the bounding box for the object.\nBounding box is treated as a regression problem."
  },
  {
    "objectID": "computer_vision/object_detection/training_custom_models_with_yolov.html",
    "href": "computer_vision/object_detection/training_custom_models_with_yolov.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "yolov7 complete guide\nTrain yolov7 on custom dataset\nyolov7 github repo\nGoogle colab notebook tutorial to train yolov7 on custom dataset\nRoboflow notebooks for computer vision\nTorchvision Object detection finetuning tutorial\n\n\n\n\nThis model can do classification, detection and segmentation\nUsing YOLOv8 to train custom model\nultralytics docs\n\n\n\n\n\n\n\nObject Detection github resources"
  },
  {
    "objectID": "computer_vision/object_detection/yolo.html",
    "href": "computer_vision/object_detection/yolo.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Less accurate than other methods\nLess effective in detecting small objects\nCan be used to detect objects in real time in resource constrained environments\n\n\n\nHow neural networks are trained for object detection\n\n\n\n\n\n\n\n\n\nIt divides the images into different grids \nIf an object belongs to multiple grid,the object belongs to the grid where the centre of the object is located.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMultiple anchor boxes for a single grid \n\nIf the grid cells are small enough, then it will be difficult to have many objects in one cell. However, if we have many objects for a single grid cell, then it is difficult for YOLO to deal with them.\n\n\n\n\n\nFirst introduced by Joseph Redmon, the latest being YOLO v8.\nYOLO V1 - 2016\nYOLO V2 - 2017\nYOLO V3 - 2018\nYOLO V4 - 2020\nYOLO v6 - 2022\n\n\n\n\n\n\nYOLO used a CNN model pretrained on ImageNet\n\n\n\n\n\n\nUsed a different CNN backbone called Darknet-19, a variant of the VGG Architecture\nIntroduced anchor boxes (same size), YOLO v2 uses a combination of the anchor boxes and the predicted offsets to determine the final bounding box. This allows the algorithm to handle a wider range of object sizes and aspect ratios\nUsed Batch normalization\nMulti-scale training - Training the model on images at multiple scales and then averaging the predictions\nAlso introduced a new loss function suited to object detection\n\n\n\n\n\nNew CNN architecture called Darknet-53, a variant of ResNet designed for object detection\nAnchor boxes with different scales and aspect ratios.\nIntroduced Feature pyramid networks (FPN) - a CNN architecture used to detect object at multiple scales. They construct a pyramid of feature maps, with each level of the pyramid being used to detect objects at a different scale. This helps to improve the detection performance on small objects, as the model is able to see the objects at multiple scales.\n\n\n\n\n\nJoseph Redmond left developing YOLO citing ethical concerns\nUse a new CNN architecture called CSPNet “Cross Stage Partial Network”, a variant of ResNet\nIt used a new method to generate anchor boxes called, “K-means clustering.” It involves using a clustering algorithm to group the ground truth bounding boxes into clusters and then using the centroids of the clusters as the anchor boxes. This allows the anchor boxes to be more closely aligned with the detected objects’ size and shape.\nIt introduced “GHM loss” - a variant of focal loss to deal with imbalanced datasets.\nImproved the architecture of FPN used in YOLO V3\n\n\n\n\n\n\nMaintained by Ultralytics\nIt uses a complex architecture called EfficientDet *\nTrained on D5 dataset, which includes 600 object categories\nUses a new method for generating boxes called “dynamic anchor boxes”\nIt uses a concept called spatial pyramid pooling (SPP) - a type of pooling layer used to reduce the spatial dimensions of the feature maps. (SPP was used in YOLO v4, in v5 it was improved)\nIntroduced CIoU - a variant of IoU loss designed to improve the model’s performance on imbalanced datasets\n\n\n\n\n\nIt uses a variant of EfficientNet called EfficientNet-L2\nIt ntroduced a new method for generating the anchor boxes, called “dense anchor boxes.”\n\n\n\n\n\nUses nine anchor boxes\nIt uses “focal loss”\nProcess images at higher resolution*\nIt introduced a new method for generating the anchor boxes, called “dense anchor boxes.”\n\n\n\n\n\n\ncodebasics youtube video\nv7 lab blog post\nDatacamp blog post"
  },
  {
    "objectID": "computer_vision/object_detection/introduction_part1.html",
    "href": "computer_vision/object_detection/introduction_part1.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "The system looks at the image and proposes RoIs for further analysis. RoIs are regions that the system believes have a high likelihood of containing an object, called the objectness score\n\n\n\n\n\nExtract features from ROI to determine the class of the image.\nTwo predictions are made for each region\n\nBounding box prediction\nClass prediction\n\n\n\n\n\n\nMultiple detections map happen for the same object. NMS is a technique that makes sure the detection algorithm detects each object only once.\n\n\n\n\n\n\nFrames per second (FPS) to measure detection speed - R-CNN operates at 7 FPS, SSD operates at 59 FPS.\nMean average precision (mAP) - It is a percentage from 0 to 100, and higher values are better.\n\n\n\n\nIt evaluates the overlap between two bounding boxes - Ground truth bounding box and the predicted bounding box\n\n\nIOU = Area of Intersection / Area of the Union IOU should be greater than a threshold which is tunable according to the problem.\nIf the IOU value is above this threshod, the prediction is considered a True Positive (TP) and if it is below the threshold, it is considered a False Positive (FP)\n\n\n\n\n\n\nsteps to calculate mAP\n\n\n\n\n\n\n\n\n\n\nselective search along with bottom up segmentation for region proposal\n\n\n\n\n\nVery slow\nThree separate modules which does not share computation\n\n\n\n\n\n\nThe loss will be multi-task loss - For predicting the class of the objects and the bounding box.\nThe region proposal is coming from a different model. This slow down the algorithm.\n\n\n\nInstead of using a selective search algorithm on the feature map to identify the region proposals, a region proposal network (RPN) is used to predict the region proposals as part of the training process.\nThe predicted region proposals are then reshaped using an RoI pooling layer and used to classify the image within the proposed region and predict the offset values for the bounding boxes\n\nIn general, single-stage detectors (SSD, yolo) tend to be less accurate than two-stage detectors (R-CNN) but are significantly faster.\n\n\n\n\n\n\nSSD Architecture\n\n\n\n\n\n\n\n\nYolo working\n\n\n\n\n\nYolo3 architecture\n\n\n\n\n\nImage classification is the task of predicting the type or class of an object in an image.\nObject detection is the task of predicting the location of objects in an image via bounding boxes and the classes of the located objects.\nThe general framework of object detection systems consists of four main components: region proposals, feature extraction and predictions, non-maximum suppression, and evaluation metrics.\nObject detection algorithms are evaluated using two main metrics: frame per second (FPS) to measure the network’s speed, and mean average precision (mAP) to measure the network’s precision.\nThe three most popular object detection systems are the R-CNN family of networks, SSD, and the YOLO family of networks.\nThe R-CNN family of networks has three main variations: R-CNN, Fast R-CNN, and Faster R-CNN. R-CNN and Fast R-CNN use a selective search algorithm to propose RoIs, whereas Faster R-CNN is an end-to-end DL system that uses a region proposal network to propose RoIs.\nThe YOLO family of networks include YOLOv1, YOLOv2 (or YOLO9000), and YOLOv3.\nR-CNN is a multi-stage detector: it separates the process to predict the objectness score of the bounding box and the object class into two different stages.\nSSD and YOLO are single-stage detectors: the image is passed once through the network to predict the objectness score and the object class.\nIn general, single-stage detectors tend to be less accurate than two-stage detectors but are significantly faster."
  },
  {
    "objectID": "computer_vision/object_detection/anchor_less_object_detection.html",
    "href": "computer_vision/object_detection/anchor_less_object_detection.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Anchor less object detection\n\nNanoDet and CornerNet are anchor free object detection models"
  },
  {
    "objectID": "computer_vision/object_detection/anchor_boxes.html",
    "href": "computer_vision/object_detection/anchor_boxes.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Anchor boxes are predetermied. The number of anchor boxes per cell is a hyperparameter which needs to be tuned.\nThe size and aspect ratio of the anchor boxes are decided based on the dataset and the bboxes in the ground truth.\nSelecting the number, size and aspect ratio is very crucial for the performance of the model. The neural network is very good at predicting small displacements than large displacement. (This prediction is done by predicting the offset for the anchor boxes). Hence selecting the right anchor boxes is crucial.\nLatest YOLO models come with Auto anchor mechanism to decide the size and aspect ratio of the anchor boxes automatically\nThe algorithm for the auto-anchor is as follows:-\n\nGet bounding box sizes from the train data\nChoose a metric to define the anchor fitness\nClustering to get an initial guess for the anchors\nEvolve anchors to improve anchor fitness\n\n\n\n\n\n\n\n\nGet the bounding box sizes from the resized image according to the model input\n\n\n\n\n\nchoose a metric to define the anchor fitness\n\n\n\n\n\nclustering to get an initial guess for the anchors\n\n\n\n\n\nevolve anchors to improve anchor fitness\n\n\n\n\n\n\n\n\nPredicting bbox from anchors\n\n\n\n\n\n\n\nArticle by Olga Chernytska"
  },
  {
    "objectID": "computer_vision/00_Interesting_links_to_read.html",
    "href": "computer_vision/00_Interesting_links_to_read.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Receptive Field\nskip connections\nDice loss - segmentation\ncomputer vision\nGenerative Learning\nGAN\nMedical\nNLP\nReinforcement learning\nUnsupervised learning\n\n\n\n\nhttps://arxiv.org/abs/1609.01388\nhttps://github.com/yahoo/hecate\nhttps://huggingface.co/spaces/Gradio-Blocks/Create_GIFs_from_Video"
  },
  {
    "objectID": "computer_vision/summary_of_research_papers/oneFormer.html",
    "href": "computer_vision/summary_of_research_papers/oneFormer.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "A multi-task universal image segmentation framework which outperforms existing state-of-the-art on all three segmentation tasks by only training once on a panoptic dataset.\nA task token is introduced in the form of text, to condition the model on the task {panoptic, instance, semantic} in focus, making the architecture task-guided for training and task-dynamic for inference, all with a single model.\nTasks are uniformly sampled {panoptic, instance, semantic} to ensure the model is unbiased in terms of tasks. Motivated by the ability of panoptic data to capture both semantic and instance information,the semantic and instance labels are derived from the corresponding panoptic annotations during training.\n\n\n\n\n\nOneFormer Architecture\n\n\n\nUniformely sample the task [panoptic, instance, semantic]\nTask conditioned architecture\nTask specific labels are derived from the corresponding panoptic annotations\n\n\n\nA set of binary masks are extracted for each category present in the image i.e. task-specific GT label. These set of masks are converted to a list of text. Padding is added to get a constant length, with padded entries representing no-object masks. This padded list is used for computing a query-text contrastive loss\n\n\n\n\nCalculating the ground truth text query for calculating contrastive loss\n\n\n\nThe architecture is conditioned on the task using a task input with the template “the task is {task}”, which is tokenized and mapped to a task token. This task token is used to condition OneFormer on the task.\nA contrastive loss is calculated between the task-conditioned image representation and the ground truth derived text and object binary mask.\n\nBackbone and pixel decoder ImageNet pre-trained backbone is used to extract multi-scale feature representations from the input image. Pixel decoder aids the feature modeling by gradually upsampling the backbone features.\nLosses Four losses are used with different weights for optimization * Contrastive loss on the queries * Classification cross entropy loss for the class predictions. * A combination of cross-entropy and dice loss are calculated for the mask predictions. Bipartite matching is used between set predictions and ground truths to find least cost assignment.\n\n\n\n\n\nEvaluated on Cityscapes, ADE20K and COCO datasets\nPanoptic quality (PQ), Average Precision (AP) and mIoU (mean Intersection over union) is used for panoptic, instance and semantic segmentation\n\nOneFormer sets a new state-of-the-art performance on all three segmentation tasks compared with methods using the standard Swin-L backbone, and improves even more with new ConvNeXt and DiNAT backbones.\n\n\n\nOneFormer Research Paper"
  },
  {
    "objectID": "computer_vision/cnn_architectures/architectures.html",
    "href": "computer_vision/cnn_architectures/architectures.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "It had 5 convolutional layers \n\n\n\n\n\n\n\n\n\n\n\nOne of the first architecture with Design rules\nAll convultional layers are 3x3 with stride 1 and pad 1\nAll max pool are 2x2 with stride 2\nAfter pooling the number of channels are doubled\nIt came with 16 and 19 layer models\nWe have lot of parameters in the early convolutional layer and the fully connected layers at the end of the network\nFuture architectures will remove the fully connected layers to reduce the number of parameters\n\n\n\nVGG Architecture with number of parameters\n\n\n\n\n\nDifferent VGG Models with 16 and 19 layers\n\n\n\n\n\n\n\n\n\nThree 3x3 convolutional layers stacked together (with non-linearities and a stride of 1) will have a receptive field equivalent to one 7x7 layer\n\n\n\n\n\nUsing two 3x3 convolutional layers which is equivalent to using one 5x5 layer will have fewer number of parameters\n\n\n\n\n\nUsing 3x3 conv layers will also reduce the number of Floating Point Operations\n\n\nWe can use activation functions between the two 3x3 convolutional layers which will provide more non-linearity\n\n\n\n\n\n\n\n\n\n\n22 layers with only 5 million parameters\n12x less parameters than Alexnet\nInception modules are used in the network\nNo fully connected layers\nStem network at the start aggressively downsample input (for VGG-16 most of the compute was at the start due to large spatial dimensions)\nNo large Fully connected layers at the end. Instead uses global average pooling to collapse spatial dimensions and one linear layer to produce class scores (VGG-16 most parameters were in the FC layers)\nThey have auxiliary classifiers. Training using loss at the end of the network din’t work well. Network is too deep and gradients don’t propagate cleanly. As a hack, they attached auxiliary classifiers at intermediate points in the network that also try to classify the image and receive loss. (GooGleNet was before batch normalization, with BatchNorm no need to use this trick)\n \n\n\n\n\n\nDesign of a network within a network (local network topology)\nInception modules are stacked on top of each other\nThe presence of multiple convolutional filters in the Inception module will blow the number of parameters and computation required. Bottleneck layers that use 1x1 convolutionals to reduce the depth while preserving spatial dimensions\n\n\n\nInception Module\n\n\n\n\n\nApplying different filter operations on the input\n\n\n\n\n\ncomputational complexity of Inception module\n\n\n\n\n\nInception layers with 1x1 convolution bottleneck layer\n\n\n\n\n\nBottleneck layer reducing the number of operations required in Inception Module\n\n\n\n\n\n\n\n\nWith Batch normalization, teams are able to train networks with 10+ layers\nIt was found that with deeper layers the models were not performing well as shallow models and are underfitting. This was happening due to optimization problem\n152 layer model\nVery deep networks using residual connections\nDeeper models does not perform better by adding more layers. Deeper layers are hard to optimize\nResNet with very large number of layers uses bottleneck blocks with 1x1 conv to reduce the number of channels, do convolutions with 3x3 filter and increase the number of channels with 1x1 filter\n\n\n\nIncrease in Number of layers\n\n\n\n\n\nResidual blocks used to increase network depth\n\n\n\n\n\n\nStanford Slides\nyoutube video explaining receptive fields\nMichigan CNN Lectures\nThe Aisummer blog post"
  },
  {
    "objectID": "computer_vision/image_segmentation/02_architectures.html",
    "href": "computer_vision/image_segmentation/02_architectures.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "This is a typical encoder-decoder architecture.\nEncoder compresses the information\nDecoder will upsample and use skip connections to come up with orginial image\nThis image is used for segmentation\n\n FCN research paper\n\n\n\n Unet research paper\n\n\n\n  Research paper\n\n\n\n\n\n\nDeeplab\n\n\n\n\n\n  Research paper Conditional Random Fields"
  },
  {
    "objectID": "computer_vision/image_segmentation/01_intro.html",
    "href": "computer_vision/image_segmentation/01_intro.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Different types of segmentation\n\nSemantic segmentation (every pixel is classified)\nInstance segmentation (object level classification)\nPanoptic segmentation (combination of pixel and instance)\n\n\n\n\n\nDifferent segmentation\n\n\n\nAfter convolutions and pooling we will have a lot of feature maps in reduced dimensions. With the help of compressed latent space representation we can do lot of things like classification, upsampling etc \nAuto-encoders\n\n\n\n\nUpsampling from Latent space representation to the original size of the image\nUpsampling happens in decoder\nUpsampling is used for image generation, enhancement, mapping and more\n\nNearest Neighor\nBed of Nails\nBilinear upsampling\nTransposed convolutions\nMax unpooling\n\n\n\n\n\n\n\nNearest Neighbors Upsampling\n\n\n\n\n\n\n\n\nBed of nails\n\n\n\n\n\n\nMost popular\n\n\n\n\n\n\nIn Max unpooling we memorize the position of the pixel with max value during max pooling. During upsampling, we fill the value in the exact position of the pixel with max value\n\n\n\n\n\n\nThese is a learnable upsampling method\nFor upsampling the input provides the weight for the filter\n\n\n\nTransposed convolutions\n\n\n\n\n\nLearnable Transposed convolution\n\n\n\n\n\nExample of Transpose convolution\n\n\n\n\n\n\n\nnearest_neighbour_out = F.interpolate(input, scale_factor=2, mode='nearest')\n\nbilinear_interp_out = F.interpolate(input, scale_factor=2, mode='bilinear', align_corners=False)\n\n\n\n\nWhen we are decompressing the image, it is difficult for the network to come up with original image size. Lot of information is lost in compression. To help in the process, skip connections exist between encoder and decoder.\n\n\n\n\nskip connections\n\n\n\nThere are two types of skip connections: Additive and concatenating\nResNet is addition\nDenseNet is concatenation\n\n\n\n\nAddition Vs Concat\n\n\nIn segmentation, skip connections are used to pass features from the encoder path to the decoder path in order to recover spatial information lost during downsampling\n  * 1x1 convolution is to reduce the number of filters (not sure how it happens) \n\n\n\nThe metric used is Intersection over union (i.e Jaccard similarity)\nThe loss function is Focal loss (This is weighted cross entropy loss in addition to ‘gamma’ which will take care of class imbalance)\nDice loss is also used"
  },
  {
    "objectID": "computer_vision/image_segmentation/03_intro_2.html",
    "href": "computer_vision/image_segmentation/03_intro_2.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Using convolutional layers to make predictions for all pixels at once - This method is very expensive to do on original image resolution.\n\n\n\nConvolutions at original image resolution\n\n\nTo efficiently do segmentation, design the network as a bunch of convolutional layers, with downsampling and upsampling inside the network!\n\n\n\nDownsampling and upsampling\n\n\nUsing downsampling, upsampling and using cross-entropy loss for the pixels, we can do image segmentation\n\n\n\n\nStanford Lecture slides\nStanford Lecture video"
  },
  {
    "objectID": "computer_vision/applications/surveillance_with_dl.html",
    "href": "computer_vision/applications/surveillance_with_dl.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Video Feed\nQuality is often lowered to maximize storage Scalalbe system should be able to intrepret low quality images Training should happen on low quality images\n\n\nProcessing power\nProcessing on a centralized server Processing on the edge - TensorRT"
  },
  {
    "objectID": "computer_vision/image_classification/satellite_images_classification.html",
    "href": "computer_vision/image_classification/satellite_images_classification.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Images Classification Implementation\n\nImport the required libraries\n\nimport torch\nimport argparse\nimport torch.nn as nn\nimport torch.optim as optim\nimport argparse\nimport cv2\nfrom matplotlib import pyplot as plt\n\n\nimport warnings\nwarnings.filterwarnings(action='ignore')\n\n\nfrom model import build_model\nfrom utils import save_model, save_plots\nfrom datasets import train_loader, valid_loader, dataset\nfrom tqdm.notebook import tqdm\n\nClasses: ['cloudy', 'desert', 'green_area', 'water']\nTotal number of images: 5631\nTotal training images: 4505\nTotal valid_images: 1126\n\n\n\n\nLoad the weights for Reset Model\n\nlr = 0.001\nepochs = 20\ndevice = ('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"computation device: {device}\\n\")\n\ncomputation device: cuda\n\n\n\n\nmodel = build_model(\n    pretrained=True, fine_tune=False, num_classes=len(dataset.classes)).to(device)\n   \n# total parameters and trainable parameters\ntotal_params = sum(p.numel() for p in model.parameters())\nprint(f\"{total_params:,} total parameters.\")\n\ntotal_trainable_params = sum(\n    p.numel() for p in model.parameters() if p.requires_grad)\nprint(f\"{total_trainable_params:,} training parameters.\\n\")\n\n[INFO]: Loading pre-trained weights\n[INFO]: Freezing hidden layers...\n21,286,724 total parameters.\n2,052 training parameters.\n\n\n\n\n# optimizer\noptimizer = optim.Adam(model.parameters(), lr=lr)\n\n# loss function\ncriterion = nn.CrossEntropyLoss()\n\n\n\nTraining and Validation Functions\n\ndef train(model, trainloader, optimizer, criterion):\n    model.train()\n    print('Training')\n    train_running_loss = 0.0\n    train_running_correct = 0\n    counter = 0\n    for i, data in tqdm(enumerate(trainloader), total=len(trainloader)):\n        counter += 1\n        image, labels = data\n        image = image.to(device)\n        labels = labels.to(device)\n        optimizer.zero_grad()\n        # forward pass\n        outputs = model(image)\n        # calculate the loss\n        loss = criterion(outputs, labels)\n        train_running_loss += loss.item()\n        # calculate the accuracy\n        _, preds = torch.max(outputs.data, 1)\n        train_running_correct += (preds == labels).sum().item()\n        # backpropagation\n        loss.backward()\n        # update the optimizer parameters\n        optimizer.step()\n    \n    # loss and accuracy for the complete epoch\n    epoch_loss = train_running_loss / counter\n    epoch_acc = 100. * (train_running_correct / len(trainloader.dataset))\n    return epoch_loss, epoch_acc\n\n\ndef validate(model, testloader, criterion, class_names):\n    model.eval()\n    print('Validation')\n    valid_running_loss = 0.0\n    valid_running_correct = 0\n    counter = 0\n    \n    # we need two lists to keep track of class-wise accuracy\n    class_correct = list(0. for i in range(len(class_names)))\n    class_total = list(0. for i in range(len(class_names)))\n    \n    with torch.no_grad():\n        for i, data in tqdm(enumerate(testloader), total=len(testloader)):\n            counter += 1\n            \n            image, labels = data\n            image = image.to(device)\n            labels = labels.to(device)\n            # forward pass\n            outputs = model(image)\n            # calculate the loss\n            loss = criterion(outputs, labels)\n            valid_running_loss += loss.item()\n            # calculate the accuracy\n            _, preds = torch.max(outputs.data, 1)\n            valid_running_correct += (preds == labels).sum().item()\n            \n            # calculate the accuracy for each class\n            correct  = (preds == labels).squeeze()\n            for i in range(len(preds)):\n                label = labels[i]\n                class_correct[label] += correct[i].item()\n                class_total[label] += 1\n        \n    # loss and accuracy for the complete epoch\n    epoch_loss = valid_running_loss / counter\n    epoch_acc = 100. * (valid_running_correct / len(testloader.dataset))\n    \n    # print the accuracy for each class after every epoch\n    print('\\n')\n    for i in range(len(class_names)):\n        print(f\"Accuracy of class {class_names[i]}: {100*class_correct[i]/class_total[i]}\")\n    print('\\n')\n        \n    return epoch_loss, epoch_acc\n\n\n\nTrain for 20 Epochs\n\n# lists to keep track of losses and accuracies\ntrain_loss, valid_loss = [], []\ntrain_acc, valid_acc = [], []\n# start the training\nfor epoch in range(epochs):\n    #print(f\"[INFO]: Epoch {epoch+1} of {epochs}\")\n    train_epoch_loss, train_epoch_acc = train(model, train_loader, \n                                              optimizer, criterion)\n    valid_epoch_loss, valid_epoch_acc = validate(model, valid_loader,  \n                                                 criterion, dataset.classes)\n    train_loss.append(train_epoch_loss)\n    valid_loss.append(valid_epoch_loss)\n    train_acc.append(train_epoch_acc)\n    valid_acc.append(valid_epoch_acc)\n    print(f\"Training loss: {train_epoch_loss:.3f}, training acc: {train_epoch_acc:.3f}\")\n    print(f\"Validation loss: {valid_epoch_loss:.3f}, validation acc: {valid_epoch_acc:.3f}\")\n    print('-'*50)\n# save the trained model weights\nsave_model(epochs, model, optimizer, criterion)\n# save the loss and accuracy plots\nsave_plots(train_acc, valid_acc, train_loss, valid_loss)\nprint('TRAINING COMPLETE')\n\nTraining\n\n\n\n\n\nValidation\n\n\n\n\n\n\n\nAccuracy of class cloudy: 99.05660377358491\nAccuracy of class desert: 95.57522123893806\nAccuracy of class green_area: 96.53979238754326\nAccuracy of class water: 93.8566552901024\n\n\nTraining loss: 0.057, training acc: 98.180\nValidation loss: 0.142, validation acc: 96.359\n--------------------------------------------------\nTraining\n\n\n\n\n\nValidation\n\n\n\n\n\n\n\nAccuracy of class cloudy: 99.37106918238993\nAccuracy of class desert: 92.92035398230088\nAccuracy of class green_area: 97.57785467128028\nAccuracy of class water: 83.2764505119454\n\n\nTraining loss: 0.049, training acc: 98.557\nValidation loss: 0.208, validation acc: 93.428\n--------------------------------------------------\nTraining\n\n\n\n\n\nValidation\n\n\n\n\n\n\n\nAccuracy of class cloudy: 99.05660377358491\nAccuracy of class desert: 96.90265486725664\nAccuracy of class green_area: 96.88581314878893\nAccuracy of class water: 84.98293515358361\n\n\nTraining loss: 0.048, training acc: 98.424\nValidation loss: 0.189, validation acc: 94.405\n--------------------------------------------------\nTraining\n\n\n\n\n\nValidation\n\n\n\n\n\n\n\nAccuracy of class cloudy: 99.05660377358491\nAccuracy of class desert: 98.23008849557522\nAccuracy of class green_area: 95.50173010380622\nAccuracy of class water: 91.46757679180887\n\n\nTraining loss: 0.047, training acc: 98.690\nValidation loss: 0.162, validation acc: 96.004\n--------------------------------------------------\nTraining\n\n\n\n\n\nValidation\n\n\n\n\n\n\n\nAccuracy of class cloudy: 98.74213836477988\nAccuracy of class desert: 97.78761061946902\nAccuracy of class green_area: 89.96539792387543\nAccuracy of class water: 96.24573378839591\n\n\nTraining loss: 0.060, training acc: 98.091\nValidation loss: 0.161, validation acc: 95.648\n--------------------------------------------------\nTraining\n\n\n\n\n\nValidation\n\n\n\n\n\n\n\nAccuracy of class cloudy: 98.11320754716981\nAccuracy of class desert: 98.67256637168141\nAccuracy of class green_area: 96.88581314878893\nAccuracy of class water: 88.39590443686006\n\n\nTraining loss: 0.049, training acc: 98.313\nValidation loss: 0.159, validation acc: 95.382\n--------------------------------------------------\nTraining\n\n\n\n\n\nValidation\n\n\n\n\n\n\n\nAccuracy of class cloudy: 99.05660377358491\nAccuracy of class desert: 97.34513274336283\nAccuracy of class green_area: 97.92387543252595\nAccuracy of class water: 91.12627986348123\n\n\nTraining loss: 0.049, training acc: 98.402\nValidation loss: 0.143, validation acc: 96.359\n--------------------------------------------------\nTraining\n\n\n\n\n\nValidation\n\n\n\n\n\n\n\nAccuracy of class cloudy: 99.05660377358491\nAccuracy of class desert: 97.78761061946902\nAccuracy of class green_area: 93.77162629757785\nAccuracy of class water: 93.8566552901024\n\n\nTraining loss: 0.040, training acc: 98.912\nValidation loss: 0.161, validation acc: 96.092\n--------------------------------------------------\nTraining\n\n\n\n\n\nValidation\n\n\n\n\n\n\n\nAccuracy of class cloudy: 99.05660377358491\nAccuracy of class desert: 92.47787610619469\nAccuracy of class green_area: 96.19377162629758\nAccuracy of class water: 91.80887372013652\n\n\nTraining loss: 0.039, training acc: 98.690\nValidation loss: 0.163, validation acc: 95.115\n--------------------------------------------------\nTraining\n\n\n\n\n\nValidation\n\n\n\n\n\n\n\nAccuracy of class cloudy: 99.05660377358491\nAccuracy of class desert: 96.90265486725664\nAccuracy of class green_area: 96.19377162629758\nAccuracy of class water: 89.419795221843\n\n\nTraining loss: 0.044, training acc: 98.468\nValidation loss: 0.162, validation acc: 95.382\n--------------------------------------------------\nTraining\n\n\n\n\n\nValidation\n\n\n\n\n\n\n\nAccuracy of class cloudy: 97.48427672955975\nAccuracy of class desert: 98.67256637168141\nAccuracy of class green_area: 87.5432525951557\nAccuracy of class water: 93.51535836177474\n\n\nTraining loss: 0.043, training acc: 98.513\nValidation loss: 0.178, validation acc: 94.139\n--------------------------------------------------\nTraining\n\n\n\n\n\nValidation\n\n\n\n\n\n\n\nAccuracy of class cloudy: 98.42767295597484\nAccuracy of class desert: 98.67256637168141\nAccuracy of class green_area: 88.23529411764706\nAccuracy of class water: 96.24573378839591\n\n\nTraining loss: 0.041, training acc: 98.602\nValidation loss: 0.162, validation acc: 95.293\n--------------------------------------------------\nTraining\n\n\n\n\n\nValidation\n\n\n\n\n\n\n\nAccuracy of class cloudy: 99.05660377358491\nAccuracy of class desert: 98.23008849557522\nAccuracy of class green_area: 86.85121107266436\nAccuracy of class water: 92.83276450511946\n\n\nTraining loss: 0.044, training acc: 98.513\nValidation loss: 0.183, validation acc: 94.139\n--------------------------------------------------\nTraining\n\n\n\n\n\nValidation\n\n\n\n\n\n\n\nAccuracy of class cloudy: 99.05660377358491\nAccuracy of class desert: 97.34513274336283\nAccuracy of class green_area: 94.80968858131487\nAccuracy of class water: 92.15017064846417\n\n\nTraining loss: 0.042, training acc: 98.668\nValidation loss: 0.157, validation acc: 95.826\n--------------------------------------------------\nTraining\n\n\n\n\n\nValidation\n\n\n\n\n\n\n\nAccuracy of class cloudy: 99.05660377358491\nAccuracy of class desert: 98.67256637168141\nAccuracy of class green_area: 92.38754325259515\nAccuracy of class water: 95.56313993174061\n\n\nTraining loss: 0.044, training acc: 98.446\nValidation loss: 0.143, validation acc: 96.359\n--------------------------------------------------\nTraining\n\n\n\n\n\nValidation\n\n\n\n\n\n\n\nAccuracy of class cloudy: 99.05660377358491\nAccuracy of class desert: 97.78761061946902\nAccuracy of class green_area: 96.53979238754326\nAccuracy of class water: 93.51535836177474\n\n\nTraining loss: 0.039, training acc: 98.468\nValidation loss: 0.125, validation acc: 96.714\n--------------------------------------------------\nTraining\n\n\n\n\n\nValidation\n\n\n\n\n\n\n\nAccuracy of class cloudy: 99.05660377358491\nAccuracy of class desert: 96.90265486725664\nAccuracy of class green_area: 95.15570934256056\nAccuracy of class water: 92.83276450511946\n\n\nTraining loss: 0.038, training acc: 98.713\nValidation loss: 0.149, validation acc: 96.004\n--------------------------------------------------\nTraining\n\n\n\n\n\nValidation\n\n\n\n\n\n\n\nAccuracy of class cloudy: 98.74213836477988\nAccuracy of class desert: 98.67256637168141\nAccuracy of class green_area: 89.27335640138408\nAccuracy of class water: 94.53924914675768\n\n\nTraining loss: 0.041, training acc: 98.713\nValidation loss: 0.154, validation acc: 95.204\n--------------------------------------------------\nTraining\n\n\n\n\n\nValidation\n\n\n\n\n\n\n\nAccuracy of class cloudy: 99.37106918238993\nAccuracy of class desert: 97.78761061946902\nAccuracy of class green_area: 96.19377162629758\nAccuracy of class water: 88.73720136518772\n\n\nTraining loss: 0.040, training acc: 98.602\nValidation loss: 0.163, validation acc: 95.471\n--------------------------------------------------\nTraining\n\n\n\n\n\nValidation\n\n\n\n\n\n\n\nAccuracy of class cloudy: 99.05660377358491\nAccuracy of class desert: 97.78761061946902\nAccuracy of class green_area: 97.2318339100346\nAccuracy of class water: 91.46757679180887\n\n\nTraining loss: 0.042, training acc: 98.557\nValidation loss: 0.133, validation acc: 96.359\n--------------------------------------------------\nTRAINING COMPLETE\n\n\n\n\n\n\n\n\n\n\nInference\n\nimport torch\nimport cv2\nimport torchvision.transforms as transforms\nfrom model import build_model\n\n\ndevice = 'cpu'\n\n\n# list containing all the labels\nlabels = ['cloudy', 'desert', 'green_area', 'water']\n# initialize the model and load the trained weights\nmodel = build_model(\n    pretrained=False, fine_tune=False, num_classes=4\n).to(device)\n\nprint('[INFO]: Loading custom-trained weights...')\ncheckpoint = torch.load('outputs/model.pth', map_location=device)\nmodel.load_state_dict(checkpoint['model_state_dict'])\nmodel.eval()\n\n# define preprocess transforms\ntransform = transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.Resize(224),\n    transforms.ToTensor(),\n    transforms.Normalize(\n        mean=[0.485, 0.456, 0.406],\n        std=[0.229, 0.224, 0.225]\n    )\n]) \n\n[INFO]: Not loading pre-trained weights\n[INFO]: Freezing hidden layers...\n[INFO]: Loading custom-trained weights...\n\n\n\ndef inference(input):\n# read and preprocess the image\n    image = cv2.imread(input)\n    # get the ground truth class\n    gt_class = input.split('/')[-1].split('.')[0]\n    orig_image = image.copy()\n    # convert to RGB format\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    image = transform(image)\n    # add batch dimension\n    image = torch.unsqueeze(image, 0)\n    with torch.no_grad():\n        outputs = model(image.to(device))\n    output_label = torch.topk(outputs, 1)\n    pred_class = labels[int(output_label.indices)]\n    cv2.putText(orig_image, \n        f\"GT: {gt_class}\",\n        (10, 25),\n        cv2.FONT_HERSHEY_SIMPLEX, \n        1, (0, 255, 0), 2, cv2.LINE_AA\n    )\n    cv2.putText(orig_image, \n        f\"Pred: {pred_class}\",\n        (10, 55),\n        cv2.FONT_HERSHEY_SIMPLEX, \n        1, (0, 0, 255), 2, cv2.LINE_AA\n    )\n    print(f\"GT: {gt_class}, pred: {pred_class}\")\n    #image = cv2.imshow('Result', orig_image)\n    rgb_image = cv2.cvtColor(orig_image,cv2.COLOR_BGR2RGB)\n    fig = plt.figure()\n    plt.axis('off')\n    plt.grid(b=None)\n    plt.imshow(rgb_image)\n    cv2.imwrite(f\"outputs/{gt_class}.png\",\n        orig_image)\n\n\ninference(input='input/test_data/cloudy.jpeg')\n\nGT: cloudy, pred: cloudy\n\n\n\n\n\n\ninference(input='input/test_data/desert.jpeg')\n\nGT: desert, pred: desert"
  },
  {
    "objectID": "computer_vision/image_classification/01_convolution.html",
    "href": "computer_vision/image_classification/01_convolution.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Architecture for Image Classification\n\nA convolution is a NN way to extract and learn a feature through matrix multiplication\nConvolution filters are used to find features such as edges and corners\nThese filters are learned through backpropagation\n\n\nFiltering\n\nConvolution is a mathematical operation on two functions (f and g) that produces a third function (f * g) expressing how the shape of one is modified by the other.\nA filter (or kernel) is defined and is applied to the image.\nThe region the filter is being applied and is called the receptive field.\nAn element-wise multiplication is done between the region and filter and adds everything up\nDoing a convolution produces an image with a reduced size. The bigger the filter, the smaller the resulting image.\nEvery filter will have as many channels as the image it is convolving. convolving a three-channel filter over a three-channel image still produces a single value.\nWe can have more than one filter. \n\n\n\n\nConvolving three-channel Image\n\n\n\n\n\nImage size after convolution\n\n\n\n\n\nConvolutions with square filter\n\n\nThe actual filter, that is, the square matrix used to perform element-wise multiplication is learned using backpropogation\n\nEven if we have only one channel as input, we can have many channels as output. we can also force a convolutional module to use a particular filter by setting its weights\nThe size of the movement, in pixels, is called a stride. When doing the stride, the filter should not move out of the image (big no-no). The bigger the stride, the smaller the resulting image\n\n\n\n\nShape after a convolution with stride\n\n\n\nwe can use padding if we would like to keep the original image size. we can also add asymmetric padding.\n\n\n\nDifferent Padding modes\n\n\n\n\n\n\nImage size after padding\n\n\n\n\n\nCalculating size of feature map after convolution\n\n\n\n\nPooling\n\n\n\n\nMax pooling\n\n\nThe bigger the pooling kernel, the smaller the resulting image\nA pooling kernel of two-by-two results in an image whose dimensions are half of the original. A pooling kernel of threeby-three makes the resulting image one third the size of the original, and so on.\nThe pooling kernel should not go out of the image\nCommon pooling operations - Max pooling, Average pooling,\nNormally the stride will be equal to the dimensions of the square filter. We can also consider other strides as well. In this case there will be overalaps and the pooling works like strides in the convolution layer\nIt is a technique to downsample the output while keeping the most relevant information\n\n\n\nTypical Architecture\n\nconvolution\nActivation function\npooling\nThe number of channels/filters produced by each block is increased as more blocks are added\nImage gets flattened\n\n\n\n\nconvolution architecture\n\n\n\n\nBatch Norm\n\nBatch normalization is the process of normalizing each layer’s inputs by using the mean and variance of the values in the current mini-batch - Benefits in faster convergence with higher learning rates"
  },
  {
    "objectID": "computer_vision/image_classification/classification.html",
    "href": "computer_vision/image_classification/classification.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "classification with pytorch - Functions to use"
  },
  {
    "objectID": "spatial_data_processing/visualization.html",
    "href": "spatial_data_processing/visualization.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Data Visualization using Folium\n\nimport folium\n\n\nfrom pyproj import crs\nimport geopandas as gpd\nimport matplotlib.pyplot as plt\n\n\nCreate a simple Interactive Map\n\nm = folium.Map(location=[20.59,78.96],zoom_start=5,control_scale=True)\n\n\nm\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\n\n\nChange the Style of the Map\n\nm = folium.Map(\n    location = [20.59,78.96],\n    tiles = \"Stamen Toner\",\n    zoom_start = 5,\n    control_scale = True,\n    prefer_canvas = True\n)\n\nm\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\n\n\nAdding layers to the Map - Pin location\n\n#18.5786832,73.7666697\nm = folium.Map(location=[20.59,78.96],\n                zoom_start=5,control_scale=True)\nfolium.Marker(\n    location = [18.5786832,73.7666697],\n    popup='Sai Eshnaya Apartments',\n    icon = folium.Icon(color='green',icon='ok-sign')\n).add_to(m)\n\nm\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\n\n\nMark good resedential areas in Pune\n\npoints_fp = '../data/addresses.shp'\npoints = gpd.read_file(points_fp)\n\n\npoints.head()\n\n\n\n\n\n  \n    \n      \n      id\n      addr\n      geometry\n    \n  \n  \n    \n      0\n      1000\n      Boat Club Road, 411001, Pune, Maharastra\n      POINT (73.87826 18.53937)\n    \n    \n      1\n      1001\n      Koregaon, 415501, Pune, Maharastra\n      POINT (73.89299 18.53772)\n    \n    \n      2\n      1002\n      Kothrud, 411038, Pune, Maharastra\n      POINT (73.80767 18.50389)\n    \n    \n      3\n      1003\n      Balewadi, 411045, Pune, Maharastra\n      POINT (73.76912 18.57767)\n    \n    \n      4\n      1004\n      Baner, 411047, Pune, Maharastra\n      POINT (73.77686 18.56424)\n    \n  \n\n\n\n\n\npoints_gjson = folium.features.GeoJson(points, name='Good Residential Areas')\n\n\nm = folium.Map(location=[18.5786832,73.7666697], tiles=\"cartodbpositron\",\n                zoom_start=8,\n                control_scale=True)\npoints_gjson.add_to(m)\nfolium.LayerControl().add_to(m)\nm\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\n\n\nCreate a Heatmap of the locations\n\npoints[\"x\"] = points[\"geometry\"].apply(lambda geom: geom.x)\npoints[\"y\"] = points[\"geometry\"].apply(lambda geom: geom.y)\n\n# Create a list of coordinate pairs\nlocations = list(zip(points[\"y\"], points[\"x\"]))\n\n\nfrom folium.plugins import HeatMap\n\n# Create a Map instance\nm = folium.Map(\n    location=[18.5786832,73.7666697], tiles=\"stamentoner\", zoom_start=10, control_scale=True\n)\n\n# Add heatmap to map instance\n# Available parameters: HeatMap(data, name=None, min_opacity=0.5, max_zoom=18, max_val=1.0, radius=25, blur=15, gradient=None, overlay=True, control=True, show=True)\nHeatMap(locations).add_to(m)\n\n# Alternative syntax:\n# m.add_child(HeatMap(points_array, radius=15))\n\n# Show map\nm\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\n\n\nCreate a Clustered point Map\n\nfrom folium.plugins import MarkerCluster\n\n\n# Create a Map instance\nm = folium.Map(\n    location=[18.5786832,73.7666697], tiles=\"cartodbpositron\", zoom_start=12, control_scale=True\n)\n\n\n# Get x and y coordinates for each point\npoints[\"x\"] = points[\"geometry\"].apply(lambda geom: geom.x)\npoints[\"y\"] = points[\"geometry\"].apply(lambda geom: geom.y)\n\n# Create a list of coordinate pairs\nlocations = list(zip(points[\"y\"], points[\"x\"]))\n\n\n# Create a folium marker cluster\nmarker_cluster = MarkerCluster(locations)\n\n# Add marker cluster to map\nmarker_cluster.add_to(m)\n\n# Show map\nm\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\n\n\nCreate a Choropleth Map\n\nimport geopandas as gpd\nfrom pyproj import CRS\nimport requests\nimport geojson\n\n# Specify the url for web feature service\nurl = \"https://kartta.hsy.fi/geoserver/wfs\"\n\n# Specify parameters (read data in json format).\n# Available feature types in this particular data source: http://geo.stat.fi/geoserver/vaestoruutu/wfs?service=wfs&version=2.0.0&request=describeFeatureType\nparams = dict(\n    service=\"WFS\",\n    version=\"2.0.0\",\n    request=\"GetFeature\",\n    typeName=\"asuminen_ja_maankaytto:Vaestotietoruudukko_2018\",\n    outputFormat=\"json\",\n)\n\n# Fetch data from WFS using requests\nr = requests.get(url, params=params)\n\n# Create GeoDataFrame from geojson\ndata = gpd.GeoDataFrame.from_features(geojson.loads(r.content))\n\n# Check the data\ndata.head()\n\n\n\n\n\n  \n    \n      \n      geometry\n      index\n      asukkaita\n      asvaljyys\n      ika0_9\n      ika10_19\n      ika20_29\n      ika30_39\n      ika40_49\n      ika50_59\n      ika60_69\n      ika70_79\n      ika_yli80\n    \n  \n  \n    \n      0\n      POLYGON ((25472499.995 6689749.005, 25472499.9...\n      688\n      9\n      28.0\n      99\n      99\n      99\n      99\n      99\n      99\n      99\n      99\n      99\n    \n    \n      1\n      POLYGON ((25472499.995 6685998.998, 25472499.9...\n      703\n      5\n      51.0\n      99\n      99\n      99\n      99\n      99\n      99\n      99\n      99\n      99\n    \n    \n      2\n      POLYGON ((25472499.995 6684249.004, 25472499.9...\n      710\n      8\n      44.0\n      99\n      99\n      99\n      99\n      99\n      99\n      99\n      99\n      99\n    \n    \n      3\n      POLYGON ((25472499.995 6683999.005, 25472499.9...\n      711\n      5\n      90.0\n      99\n      99\n      99\n      99\n      99\n      99\n      99\n      99\n      99\n    \n    \n      4\n      POLYGON ((25472499.995 6682998.998, 25472499.9...\n      715\n      11\n      41.0\n      99\n      99\n      99\n      99\n      99\n      99\n      99\n      99\n      99\n    \n  \n\n\n\n\n\nfrom pyproj import CRS\n\n# Define crs\ndata.crs = CRS.from_epsg(3879)\n\n\n# Re-project to WGS84\ndata = data.to_crs(epsg=4326)\n\n# Check layer crs definition\nprint(data.crs)\n\nEPSG:4326\n\n\n\n# Change the name of a column\ndata = data.rename(columns={\"asukkaita\": \"pop18\"})\n\n\ndata[\"geoid\"] = data.index.astype(str)\n\n\n# Select only needed columns\ndata = data[[\"geoid\", \"pop18\", \"geometry\"]]\n\n# Convert to geojson (not needed for the simple coropleth map!)\n# pop_json = data.to_json()\n\n# check data\ndata.head()\n\n\n\n\n\n  \n    \n      \n      geoid\n      pop18\n      geometry\n    \n  \n  \n    \n      0\n      0\n      9\n      POLYGON ((24.50236 60.31928, 24.50233 60.32152...\n    \n    \n      1\n      1\n      5\n      POLYGON ((24.50287 60.28562, 24.50284 60.28787...\n    \n    \n      2\n      2\n      8\n      POLYGON ((24.50311 60.26992, 24.50308 60.27216...\n    \n    \n      3\n      3\n      5\n      POLYGON ((24.50315 60.26767, 24.50311 60.26992...\n    \n    \n      4\n      4\n      11\n      POLYGON ((24.50328 60.25870, 24.50325 60.26094...\n    \n  \n\n\n\n\n\nm = folium.Map(\n    location=[60.25, 24.8], tiles=\"cartodbpositron\", zoom_start=10, control_scale=True\n)\n\n# Plot a choropleth map\n# Notice: 'geoid' column that we created earlier needs to be assigned always as the first column\nfolium.Choropleth(\n    geo_data=data,\n    name=\"Population in 2018\",\n    data=data,\n    columns=[\"geoid\", \"pop18\"],\n    key_on=\"feature.id\",\n    fill_color=\"YlOrRd\",\n    fill_opacity=0.7,\n    line_opacity=0.2,\n    line_color=\"white\",\n    line_weight=0,\n    highlight=False,\n    smooth_factor=1.0,\n    # threshold_scale=[100, 250, 500, 1000, 2000],\n    legend_name=\"Population in Helsinki\",\n).add_to(m)\n\n# Show map\nm\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\n\n\nCreate Choropleth Map with Interaction\n\n# Convert points to GeoJson\nfolium.features.GeoJson(\n    data,\n    name=\"Labels\",\n    style_function=lambda x: {\n        \"color\": \"transparent\",\n        \"fillColor\": \"transparent\",\n        \"weight\": 0,\n    },\n    tooltip=folium.features.GeoJsonTooltip(\n        fields=[\"pop18\"], aliases=[\"Population\"], labels=True, sticky=False\n    ),\n).add_to(m)\n\nm\n\n\nMake this Notebook Trusted to load map: File -> Trust Notebook"
  },
  {
    "objectID": "spatial_data_processing/plot_buildings_with_area.html",
    "href": "spatial_data_processing/plot_buildings_with_area.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Visualizing Buildings in a location along with its Area\n\nImport the required libraries\n\nimport osmnx as ox\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport geopandas as gpd\nimport plotly.express as px\nimport keplergl\nimport warnings\nwarnings.filterwarnings(action='ignore')\n\n\n\nPlot the map for Pune\n\npune = ox.geocode_to_gdf(\"Pune, India\")\npune.plot(edgecolor=\"0.2\")\nplt.title(\"Pune\")\n\nText(0.5, 1.0, 'Pune')\n\n\n\n\n\n\n\nPlot your location on the Map\n\nmy_location = pd.DataFrame(\n    {\"location\":[\"Baner\"],\n    \"Longitude\":[73.7747862],\n    \"Latitude\":[18.578686]}\n)\n\n\nmy_location\n\n\n\n\n\n  \n    \n      \n      location\n      Longitude\n      Latitude\n    \n  \n  \n    \n      0\n      Baner\n      73.774786\n      18.578686\n    \n  \n\n\n\n\n\nmy_location = gpd.GeoDataFrame(my_location,\n                    crs = \"EPSG:4326\",\n                    geometry=gpd.points_from_xy(my_location[\"Longitude\"],my_location[\"Latitude\"]))\n\n\nmy_location\n\n\n\n\n\n  \n    \n      \n      location\n      Longitude\n      Latitude\n      geometry\n    \n  \n  \n    \n      0\n      Baner\n      73.774786\n      18.578686\n      POINT (73.77479 18.57869)\n    \n  \n\n\n\n\n\nax = pune.plot(edgecolor=\"0.2\")\nmy_location.plot(ax=ax,markersize=60,edgecolor=\"0.2\",color='red')\nplt.title(\"My Location in Pune\")\n\nText(0.5, 1.0, 'My Location in Pune')\n\n\n\n\n\n\n\nGet the Bike Routes for your location\n\nbike_network = ox.graph_from_point(center_point=(18.5584546,73.7852182),dist=400,network_type='bike')\nbike_network\n\n<networkx.classes.multidigraph.MultiDiGraph at 0x7f3d9ae77f40>\n\n\n\nbike_network = (ox.graph_to_gdfs(bike_network, nodes=False)\n                  .reset_index(drop=True)\n                  .loc[:, [\"name\", \"length\", \"geometry\"]]\n               )\nbike_network\n\n\n\n\n\n  \n    \n      \n      name\n      length\n      geometry\n    \n  \n  \n    \n      0\n      Gopal Hari Deshmukh Marg\n      52.331\n      LINESTRING (73.78688 18.56143, 73.78638 18.56145)\n    \n    \n      1\n      NaN\n      127.050\n      LINESTRING (73.78688 18.56143, 73.78683 18.560...\n    \n    \n      2\n      Pancard Clubs Road\n      8.998\n      LINESTRING (73.78638 18.56153, 73.78638 18.56145)\n    \n    \n      3\n      Gopal Hari Deshmukh Marg\n      58.852\n      LINESTRING (73.78638 18.56153, 73.78689 18.561...\n    \n    \n      4\n      Pancard Clubs Road\n      8.998\n      LINESTRING (73.78638 18.56145, 73.78638 18.56153)\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      201\n      NaN\n      32.554\n      LINESTRING (73.78206 18.55943, 73.78175 18.55946)\n    \n    \n      202\n      NaN\n      47.738\n      LINESTRING (73.78206 18.55943, 73.78204 18.55986)\n    \n    \n      203\n      NaN\n      19.034\n      LINESTRING (73.78206 18.55943, 73.78224 18.55944)\n    \n    \n      204\n      NaN\n      32.554\n      LINESTRING (73.78175 18.55946, 73.78206 18.55943)\n    \n    \n      205\n      NaN\n      19.034\n      LINESTRING (73.78224 18.55944, 73.78206 18.55943)\n    \n  \n\n206 rows × 3 columns\n\n\n\n\nbike_network.plot()\n\n<AxesSubplot: >\n\n\n\n\n\n\ntotal_length = bike_network[\"length\"].sum()\nprint(f\"Total bike lane length: {total_length / 1000:.0f}km\")\n\nTotal bike lane length: 16km\n\n\n\n\nPlot Bike routes on the Map\n\nimport contextily as ctx\n\nax = (bike_network.to_crs(\"EPSG:3857\")\n         .plot(figsize=(10, 8), legend=True,\n               edgecolor=\"0.2\", markersize=200, cmap=\"rainbow\")\n     )\nctx.add_basemap(ax, source=ctx.providers.OpenStreetMap.Mapnik)  # I'm using OSM as the source. See all provides with ctx.providers\nplt.axis(\"off\")\nplt.title(\"Baner\")\n\nText(0.5, 1.0, 'Baner')\n\n\n\n\n\n\n\nGet the building details in your area\n\ntags = {'building':True}\n\n\nbaner_buildings = ox.geometries_from_point(center_point=(18.5584546,73.7852182),dist=400,tags=tags)\n\n\nbaner_buildings = baner_buildings.assign(label='Building Footprints').reset_index()\n\n\n(baner_buildings.head(10).T)\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n      9\n    \n  \n  \n    \n      element_type\n      node\n      way\n      way\n      way\n      way\n      way\n      way\n      way\n      way\n      way\n    \n    \n      osmid\n      1432130601\n      264286363\n      359568513\n      359568520\n      359568545\n      359568551\n      359568562\n      359684077\n      359684097\n      359684099\n    \n    \n      building\n      yes\n      yes\n      yes\n      yes\n      yes\n      yes\n      yes\n      yes\n      yes\n      yes\n    \n    \n      name\n      UBICS\n      Baneshwar Temple\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      geometry\n      POINT (73.7860015 18.5612441)\n      POLYGON ((73.7869388 18.5589314, 73.7869469 18...\n      POLYGON ((73.7889321 18.5605767, 73.7890453 18...\n      POLYGON ((73.7876847 18.56149, 73.7877554 18.5...\n      POLYGON ((73.78786 18.5613415, 73.7880387 18.5...\n      POLYGON ((73.7881551 18.559907, 73.7882733 18....\n      POLYGON ((73.7828324 18.5618882, 73.782974 18....\n      POLYGON ((73.7813494 18.5612504, 73.7814597 18...\n      POLYGON ((73.781994 18.5613356, 73.7821481 18....\n      POLYGON ((73.7814951 18.5606079, 73.7817434 18...\n    \n    \n      nodes\n      NaN\n      [2699768401, 2699768402, 2699768403, 269976840...\n      [3642483644, 3642483643, 3642483641, 364248364...\n      [3642483654, 3642483653, 3642483648, 364248365...\n      [3642483649, 3642483647, 3642483645, 364248364...\n      [3642483640, 3642483639, 3642483637, 364248363...\n      [3642483660, 3642483659, 3642483657, 364248365...\n      [3643589874, 3643589877, 3643589867, 364358986...\n      [3643589882, 3643589883, 3643589873, 364358987...\n      [3643589826, 3643589830, 3643589824, 364358981...\n    \n    \n      amenity\n      NaN\n      place_of_worship\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      religion\n      NaN\n      hindu\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      addr:city\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      addr:postcode\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      addr:street\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      ways\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      type\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      label\n      Building Footprints\n      Building Footprints\n      Building Footprints\n      Building Footprints\n      Building Footprints\n      Building Footprints\n      Building Footprints\n      Building Footprints\n      Building Footprints\n      Building Footprints\n    \n  \n\n\n\n\n\nbaner_buildings.name.fillna(value='not_known',inplace=True)\n\n\nbaner_buildings.shape\n\n(255, 14)\n\n\n\n\nVisualize the buildings on the map\n\nax = (baner_buildings.to_crs(\"EPSG:3857\")\n         .plot(figsize=(10, 12),column=\"name\",legend=True,\n               edgecolor=\"0.2\", markersize=200, cmap=\"rainbow\")\n     )\nctx.add_basemap(ax, source=ctx.providers.OpenStreetMap.Mapnik)  # I'm using OSM as the source. See all provides with ctx.providers\nplt.axis(\"off\")\nplt.title(\"Baner\")\n\nText(0.5, 1.0, 'Baner')\n\n\n\n\n\n\nbaner_buildings = baner_buildings.to_crs(epsg=3347)\nbaner_buildings = baner_buildings.assign(area=baner_buildings.area)\n\n\nbaner_buildings= baner_buildings[['geometry','area']]\n\n\n\nPlot the buildings on the Map along with Area\n\nbaner_map = keplergl.KeplerGl(height=500)\nbaner_map.add_data(data=baner_buildings.copy(), name=\"Building area\")\n#baner_map.add_data(data=baner_buildings.copy(), name=\"height\")\n#baner_map\n\nUser Guide: https://docs.kepler.gl/docs/keplergl-jupyter\n\n\n\nbaner_map.save_to_html(file_name='first_map.html')\n\nMap saved to first_map.html!\n\n\n\n%%html\n<iframe src=\"first_map.html\" width=\"80%\" height=\"500\"></iframe>\n\n\n\n\n\n#from IPython.display import IFrame\n#IFrame(src='first_map.html', width=700, height=600)"
  },
  {
    "objectID": "spatial_data_processing/osm_processing copy.html",
    "href": "spatial_data_processing/osm_processing copy.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "OpenStreetMap\n\nIt is an crowd-sourced dataset\nIt contains data about streets, buildings, services, landuse etc.\nOSMnx is a package used to retrieve, construct, analyze and visualize street networks from OpenStreetMap and also retrieve data about points of interest such as restaurants, schools and lots of different kind of services.\nIt is also easy to conduct network routing based on walking, cycling or driving by combining OSMnx functionalities with a package called NetworkX\n\n\nimport osmnx as ox\nimport matplotlib.pyplot as plt\n\n\n#place_name = \"Togo, Africa\"\nplace_name = {18.5786832,73.7666697}\n\n\ngraph = ox.graph_from_point(place_name,dist=750,dist_type='bbox',network_type=\"drive\")\n\nEmptyOverpassResponse: There are no data elements in the response JSON\n\n\n\ntype(graph)\n\nnetworkx.classes.multidigraph.MultiDiGraph\n\n\n\nfig, ax = ox.plot_graph(graph)\n\n\n\n\n\nnodes, edges = ox.graph_to_gdfs(graph)\n\n\nnodes.head()\n\n\n\n\n\n  \n    \n      \n      y\n      x\n      street_count\n      geometry\n    \n    \n      osmid\n      \n      \n      \n      \n    \n  \n  \n    \n      652724178\n      18.574935\n      73.763832\n      4\n      POINT (73.76383 18.57493)\n    \n    \n      652724182\n      18.574981\n      73.764610\n      3\n      POINT (73.76461 18.57498)\n    \n    \n      763423062\n      18.571967\n      73.764768\n      3\n      POINT (73.76477 18.57197)\n    \n    \n      871491336\n      18.574828\n      73.770821\n      4\n      POINT (73.77082 18.57483)\n    \n    \n      1377773005\n      18.574932\n      73.763664\n      3\n      POINT (73.76366 18.57493)\n    \n  \n\n\n\n\n\nedges.head()\n\n\n\n\n\n  \n    \n      \n      \n      \n      osmid\n      oneway\n      highway\n      reversed\n      length\n      name\n      geometry\n      access\n      lanes\n      ref\n      maxspeed\n    \n    \n      u\n      v\n      key\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      652724178\n      7984103956\n      0\n      669050753\n      False\n      primary\n      False\n      13.448\n      NaN\n      LINESTRING (73.76383 18.57493, 73.76387 18.57482)\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      6262990166\n      0\n      669050753\n      False\n      primary\n      True\n      60.618\n      NaN\n      LINESTRING (73.76383 18.57493, 73.76364 18.57545)\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      6305085563\n      0\n      73533877\n      True\n      secondary\n      False\n      24.596\n      Moze College Road\n      LINESTRING (73.76383 18.57493, 73.76402 18.574...\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      652724182\n      4676484316\n      0\n      73533877\n      True\n      secondary\n      False\n      69.380\n      Moze College Road\n      LINESTRING (73.76461 18.57498, 73.76493 18.575...\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      7983557257\n      0\n      [250171874, 223437750]\n      False\n      residential\n      [False, True]\n      306.812\n      Echinus Court Road\n      LINESTRING (73.76461 18.57498, 73.76460 18.575...\n      yes\n      NaN\n      NaN\n      NaN\n    \n  \n\n\n\n\n\narea = ox.geocode_to_gdf(place_name)\n\nValueError: each query must be a dict or a string\n\n\n\ntype(area)\n\nNameError: name 'area' is not defined\n\n\n\narea\n\n\n\n\n\n  \n    \n      \n      geometry\n      bbox_north\n      bbox_south\n      bbox_east\n      bbox_west\n      place_id\n      osm_type\n      osm_id\n      lat\n      lon\n      display_name\n      class\n      type\n      importance\n    \n  \n  \n    \n      0\n      POLYGON ((74.91434 26.83563, 74.91534 26.83465...\n      27.860562\n      26.440461\n      76.285428\n      74.914344\n      298175590\n      relation\n      1950062\n      27.150677\n      75.747016\n      Jaipur, Rajasthan, India\n      boundary\n      administrative\n      0.671968\n    \n  \n\n\n\n\n\narea.plot()\n\n<AxesSubplot: >\n\n\n\n\n\n\ntags = {\"building\":True}\n\n\nbuildings = ox.geometries_from_place(place_name,tags)\n\nTypeError: query must be dict, string, or list of strings\n\n\n\nlen(buildings)\n\n32708\n\n\n\nbuildings.head()\n\n\n\n\n\n  \n    \n      \n      \n      nodes\n      building\n      geometry\n      area\n      barrier\n      currency:INR\n      layer\n      name\n      payment:cash\n      payment:fasttag\n      ...\n      name:tg\n      name:fr\n      motor_vehicle\n      architect\n      historic:civilization\n      outdoor_seating\n      location\n      parking\n      changing_table\n      toilets:disposal\n    \n    \n      element_type\n      osmid\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      way\n      383032803\n      [3862350688, 3862350689, 3862350690, 386235069...\n      residential\n      POLYGON ((75.82021 26.78322, 75.82020 26.78289...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      383032804\n      [3862350692, 3862350693, 3862350694, 386235069...\n      residential\n      POLYGON ((75.82045 26.78350, 75.82042 26.78332...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      383032805\n      [3862350696, 3862350697, 3862350698, 386235069...\n      residential\n      POLYGON ((75.82068 26.78322, 75.82084 26.78320...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      383032806\n      [3862350700, 3862350701, 3862350702, 386235070...\n      residential\n      POLYGON ((75.82069 26.78321, 75.82085 26.78319...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      383032807\n      [3862350704, 3862350705, 3862350706, 386235070...\n      residential\n      POLYGON ((75.82068 26.78297, 75.82079 26.78295...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n  \n\n5 rows × 132 columns\n\n\n\n\nbuildings.shape\n\n(32708, 132)\n\n\n\n# List key-value pairs for tags\ntags = {\"amenity\":\"restaurant\"}\n\n\n# Retrieve restaurants\nrestaurants = ox.geometries_from_place(place_name, tags)\n\n# How many restaurants do we have?\nlen(restaurants)\n\n127\n\n\n\nfig, ax = plt.subplots(figsize=(12, 8))\n\n# Plot the footprint\narea.plot(ax=ax, facecolor=\"black\")\n\n# Plot street edges\nedges.plot(ax=ax, linewidth=1, edgecolor=\"dimgray\",alpha=0.9)\n\n# Plot buildings\nbuildings.plot(ax=ax, facecolor=\"yellow\", markersize=20)\n\n# Plot restaurants\nrestaurants.plot(ax=ax, color=\"red\", markersize=20)\nplt.tight_layout()"
  },
  {
    "objectID": "spatial_data_processing/spatial_modelling.html",
    "href": "spatial_data_processing/spatial_modelling.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "import warnings\nimport keplergl\nimport numpy as np\nimport osmnx as ox\nimport pandas as pd\nimport geopandas as gpd\nimport plotly.express as px\nfrom skgstat import Variogram\nimport matplotlib.pyplot as plt\nfrom shapely.geometry import Point\nfrom pykrige.ok import OrdinaryKriging\nfrom scipy.interpolate import NearestNDInterpolator\nfrom tobler.area_weighted import area_interpolate\n# Custom functions\nfrom scripts.utils import pixel2poly\n# Plotting defaults\nplt.style.use('ggplot')\npx.defaults.height = 400; px.defaults.width = 620\nplt.rcParams.update({'font.size': 16, 'axes.labelweight': 'bold', 'figure.figsize': (6, 6), 'axes.grid': False})"
  },
  {
    "objectID": "spatial_data_processing/geographic_data_formats.html",
    "href": "spatial_data_processing/geographic_data_formats.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Geographic information can be represented in two forms vector or raster\nVector representations are constructed from points in geographical space which are connected to each other forming lines and polygons\nRasters are constructed from rectangular cells that form a uniform grid. Each cell of the grid contains a value representing some information such as elevation, temperature or presence / absence\nSpatio-temporal data incorporates time as additional dimension to the geographic dimension\n\n\n\n\nGeometric objects like points, lines and polygons are used\n\n\n\n\nVector data representation\n\n\n\n\n\nAttribute data is typically attached to the geometries that describe the given entity with various possible characteristics. Attributes are always linked to the geometries in one way or another.\n\n\n\n\n\nGDAL (Geospatial Data Abstraction Library) is a library for reading and writing raster and vector data formats which is used by most of the software libraries\n\n\n\n\nIntroduced by ESRI\nFilename extension is .shp\nIt is made of multiple separate files\nA valid shapefile dataset consist of:\n\n.shp - Feature geometries\n.shx - Positional index for the feature geometries\n.dbf - Attribute information\n.prj - Information about CRS of the dataset\n\n\n\n\n\n\nOpen format for encoding variety of geographic data structures along with their attribute information\nFilename extension is .geojson\nFile is not compressed\nAn example of GeoJSON data\n\n{\"type\": \"FeatureCollection\", \n    \"features\": [\n        {\"type\": \"Feature\", \"properties\": {\"id\": 75553155, \"timestamp\": 1494181812},\n        \"geometry\": {\"type\": \"MultiLineString\", \"coordinates\": [[[26.938, 60.520], [26.938, 60.520]], [[26.937, 60.521], [26.937, 60.521]], [[26.937, 60.521], [26.936, 60.522]]]}\n        }, \n        {\"type\": \"Feature\", \"properties\": {\"id\": 424099695, \"timestamp\": 1465572910}, \n        \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [[[26.935, 60.521], [26.935, 60.521], [26.935, 60.521], [26.935, 60.521], [26.935, 60.521]]]}\n        }\n    ]\n}\n\n\n\n\nIt uses SQLite database container to store the data\nFilename extension is .gpkg\n\n\n\n\n\nGeography Markup Language (GML) is an XML based format\nIt serves as a modeling language for geographic systems as well as an open interchange format for geographic transactions on the Internet\nFile extension is .gml\n\n\n\n\n\n\n\nData is represented as arrays of cells (called pixels) to represent real-world objects or continuous phenomena Ex- Digital photos with RGB channels\nWe can store other information to pixels, such as elevation or temperature data or more detailed spectral information that capture how the light reflects from objects on earth at different wave-lengths\n\n\n\n\nRaster data representation\n\n\n\n\n\nRaster Bit Depth\n\n\n\n\n\nxarray Data Format\n\n\n\n\n\n\n\nBased on TIFF format developed by NASA\nFile extension is .tif\n\n\n\n\n\nCloud Optimized GeoTIFF (COG)\nFile extension is .tif\n\n\n\n\n\nNetwork Common Data Form\nVariables stored in NetCDF are often measured multiple times per day over large (e.g. continental) areas\nThe file extension of NetCDF is .nc4\n\n\n\n\n\nUsed to transfer Raster files between applications\nThe file extension of ASCII Raster File is .asc\n\n\n\n\n\nThe ERDAS Imagine file format (IMG) is proprietary file format that was originally created by an image processing software company called ERDAS. The file can be accompanied with an .xml file which stores metadata information about the raster layer\nThe file extension of Imagine file format is .img\n\n\n\n\n\n\n\nNetworkx is used to store graph objects\npysal rely on sparse adjacency matrix"
  },
  {
    "objectID": "spatial_data_processing/osm_processing.html",
    "href": "spatial_data_processing/osm_processing.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "OpenStreetMap\n\nIt is an crowd-sourced dataset\nIt contains data about streets, buildings, services, landuse etc.\nOSMnx is a package used to retrieve, construct, analyze and visualize street networks from OpenStreetMap and also retrieve data about points of interest such as restaurants, schools and lots of different kind of services.\nIt is also easy to conduct network routing based on walking, cycling or driving by combining OSMnx functionalities with a package called NetworkX\n\n\nGet Street Network Graph for Tirupathi\n\nimport osmnx as ox\nimport matplotlib.pyplot as plt\n\n\nplace_name = \"Tirupathi, Andhra Pradesh, India\"\n\n\ngraph = ox.graph_from_place(place_name)\n\n\ntype(graph)\n\nnetworkx.classes.multidigraph.MultiDiGraph\n\n\n\nfig, ax = ox.plot_graph(graph)\n\n\n\n\n\nnodes, edges = ox.graph_to_gdfs(graph)\n\n\nnodes.head()\n\n\n\n\n\n  \n    \n      \n      y\n      x\n      street_count\n      geometry\n    \n    \n      osmid\n      \n      \n      \n      \n    \n  \n  \n    \n      3726004217\n      13.626082\n      79.391887\n      3\n      POINT (79.39189 13.62608)\n    \n    \n      3726082024\n      13.624080\n      79.381771\n      3\n      POINT (79.38177 13.62408)\n    \n    \n      3726082625\n      13.624315\n      79.383015\n      3\n      POINT (79.38302 13.62431)\n    \n    \n      3726082626\n      13.624330\n      79.383098\n      3\n      POINT (79.38310 13.62433)\n    \n    \n      3726082627\n      13.624499\n      79.393360\n      3\n      POINT (79.39336 13.62450)\n    \n  \n\n\n\n\n\nedges.head()\n\n\n\n\n\n  \n    \n      \n      \n      \n      osmid\n      highway\n      oneway\n      reversed\n      length\n      geometry\n      tunnel\n      bridge\n    \n    \n      u\n      v\n      key\n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      3726004217\n      3727759169\n      0\n      368755785\n      service\n      False\n      True\n      102.448\n      LINESTRING (79.39189 13.62608, 79.39096 13.62589)\n      NaN\n      NaN\n    \n    \n      3726082692\n      0\n      368765668\n      service\n      True\n      False\n      11.398\n      LINESTRING (79.39189 13.62608, 79.39189 13.626...\n      NaN\n      NaN\n    \n    \n      3726082024\n      3726082625\n      0\n      368755785\n      service\n      False\n      False\n      136.948\n      LINESTRING (79.38177 13.62408, 79.38302 13.62431)\n      NaN\n      NaN\n    \n    \n      3726082653\n      0\n      368765683\n      service\n      False\n      False\n      247.024\n      LINESTRING (79.38177 13.62408, 79.38146 13.625...\n      NaN\n      NaN\n    \n    \n      3726082625\n      3726082626\n      0\n      368755785\n      service\n      False\n      False\n      9.070\n      LINESTRING (79.38302 13.62431, 79.38310 13.62433)\n      NaN\n      NaN\n    \n  \n\n\n\n\n\narea = ox.geocode_to_gdf(place_name)\n\n\ntype(area)\n\ngeopandas.geodataframe.GeoDataFrame\n\n\n\narea\n\n\n\n\n\n  \n    \n      \n      geometry\n      bbox_north\n      bbox_south\n      bbox_east\n      bbox_west\n      place_id\n      osm_type\n      osm_id\n      lat\n      lon\n      display_name\n      class\n      type\n      importance\n    \n  \n  \n    \n      0\n      POLYGON ((79.37901 13.62928, 79.38167 13.62409...\n      13.634066\n      13.623569\n      79.39373\n      79.379014\n      191306545\n      way\n      369041142\n      13.626914\n      79.386643\n      Sri Venkateshwara Veterinary University, Tirup...\n      amenity\n      university\n      0.718072\n    \n  \n\n\n\n\n\narea.plot()\n\n<AxesSubplot: >\n\n\n\n\n\n\n\nGet Building information\n\ntags = {\"building\":True}\n\n\nbuildings = ox.geometries_from_place(place_name,tags)\n\n\nlen(buildings)\n\n103\n\n\n\nbuildings.head()\n\n\n\n\n\n  \n    \n      \n      \n      nodes\n      building\n      geometry\n      layer\n      name\n      ways\n      type\n    \n    \n      element_type\n      osmid\n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      way\n      368754138\n      [3725992120, 3725992118, 3725992325, 372599232...\n      yes\n      POLYGON ((79.38556 13.62655, 79.38560 13.62650...\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      368765639\n      [3727711369, 3727711372, 3727711373, 372771137...\n      yes\n      POLYGON ((79.38762 13.62865, 79.38763 13.62865...\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      368765640\n      [3726082631, 3726082641, 3726082638, 372608263...\n      yes\n      POLYGON ((79.38655 13.62537, 79.38662 13.62548...\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      368765641\n      [3726082662, 3726082681, 3726082697, 372608269...\n      yes\n      POLYGON ((79.38255 13.62598, 79.38244 13.62610...\n      1\n      Admin Office (Dr. Y.S.R. Bhavan)\n      NaN\n      NaN\n    \n    \n      368765644\n      [3726083018, 3726083020, 3726082991, 372608298...\n      yes\n      POLYGON ((79.38190 13.62782, 79.38196 13.62783...\n      NaN\n      NaN\n      NaN\n      NaN\n    \n  \n\n\n\n\n\nbuildings.shape\n\n(103, 7)\n\n\n\n# List key-value pairs for tags\ntags = {\"railway\":True}\n\n\n# Retrieve restaurants\nrailway = ox.geometries_from_place(place_name, tags)\n\n# How many restaurants do we have?\nlen(railway)\n\n1\n\n\n\nfig, ax = plt.subplots(figsize=(12, 8))\n\n# Plot the footprint\narea.plot(ax=ax, facecolor=\"black\")\n\n# Plot street edges\nedges.plot(ax=ax, linewidth=1, edgecolor=\"dimgray\")\n\n# Plot buildings\nbuildings.plot(ax=ax, facecolor=\"silver\", alpha=0.7)\n\n# Plot restaurants\nrailway.plot(ax=ax, color=\"red\", alpha=0.7, markersize=20)\nplt.tight_layout()\n\n\n\n\n\n\nGet Park Information\n\ntags = {\"leisure\": \"park\", \"landuse\": \"grass\"}\n\n\nparks = ox.geometries_from_place(place_name, tags)\nprint(\"Retrieved\", len(parks), \"objects\")\n\nRetrieved 5 objects\n\n\n\nparks.head(3)\n\n\n\n\n\n  \n    \n      \n      \n      nodes\n      landuse\n      geometry\n    \n    \n      element_type\n      osmid\n      \n      \n      \n    \n  \n  \n    \n      way\n      368765686\n      [3726082943, 3726082942, 3726082954, 372608295...\n      grass\n      POLYGON ((79.38144 13.62756, 79.38141 13.62755...\n    \n    \n      368765687\n      [3726082995, 3726082981, 3726082971, 372608299...\n      grass\n      POLYGON ((79.38149 13.62773, 79.38150 13.62770...\n    \n    \n      368765688\n      [3726083023, 3726083010, 3726082983, 372608300...\n      grass\n      POLYGON ((79.38150 13.62785, 79.38152 13.62778...\n    \n  \n\n\n\n\n\nparks.plot(color='green')\n\n<AxesSubplot: >\n\n\n\n\n\n\nfig, ax = plt.subplots(figsize=(12, 8))\n\n# Plot the footprint\narea.plot(ax=ax, facecolor=\"black\")\n\n# Plot the parks\nparks.plot(ax=ax, facecolor=\"green\")\n\n# Plot street edges\nedges.plot(ax=ax, linewidth=1, edgecolor=\"dimgray\")\n\n# Plot buildings\nbuildings.plot(ax=ax, facecolor=\"silver\", alpha=0.7)\n\n# Plot restaurants\nrailway.plot(ax=ax, color=\"red\", alpha=0.7, markersize=20)\nplt.tight_layout()"
  },
  {
    "objectID": "spatial_data_processing/spatial_analysis.html",
    "href": "spatial_data_processing/spatial_analysis.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "import geopandas as gpd\nfrom pathlib import Path\n\n\ninput_path = '/home/thulasiram/personal/going_deep_and_wide/togo/togo-targeting-replication/data/shapefiles/cantons.geojson'\ndata = gpd.read_file(input_path)\n\n\ntype(data)\n\ngeopandas.geodataframe.GeoDataFrame\n\n\n\ndata.head()\n\n\n\n\n\n  \n    \n      \n      canton\n      poverty\n      geometry\n    \n  \n  \n    \n      0\n      1\n      3.738084\n      MULTIPOLYGON (((0.75228 6.83786, 0.75137 6.840...\n    \n    \n      1\n      2\n      7.096286\n      MULTIPOLYGON (((0.69026 6.80602, 0.69627 6.806...\n    \n    \n      2\n      3\n      0.824586\n      MULTIPOLYGON (((0.63102 6.74430, 0.63295 6.747...\n    \n    \n      3\n      4\n      3.983729\n      MULTIPOLYGON (((0.67259 6.85123, 0.67714 6.849...\n    \n    \n      4\n      5\n      7.708810\n      MULTIPOLYGON (((0.75269 6.84116, 0.75137 6.840...\n    \n  \n\n\n\n\n\nprint(\"Number of rows\",len(data[\"canton\"]))\nprint(\"Number of classes\",data[\"canton\"].nunique())\n\nNumber of rows 387\nNumber of classes 387\n\n\n\n\n\n\ndata.plot()\n\n<AxesSubplot: >\n\n\n\n\n\nChecking the shape and area of the first Multipolygon in the data\n\ndata.at[0,\"geometry\"]\n\n\n\n\n\n# Calculating area with lat and long is wrong, to calculate \n# area correctly we need to change the co-ordinate reference system\nround(data.at[0,\"geometry\"].area,3)\n\n0.014\n\n\n\n\n\n\ndata.plot(\"poverty\",legend=True)\n\n<AxesSubplot: >\n\n\n\n\n\n\ndata.explore(\"poverty\",legend=True)\n\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\n\n\n\nGeocoding is the process of transforming place names or addresses into coordinates\n\n# Import necessary modules\nimport pandas as pd\nimport geopandas as gpd\nfrom shapely.geometry import Point\n\n\n# Filepath\nfp = '../data/addresses.txt'\n\n# Read the data\ndata = pd.read_csv(fp,sep=\";\")\n\n\nlen(data)\n\n5\n\n\n\ndata.head()\n\n\n\n\n\n  \n    \n      \n      id\n      addr\n    \n  \n  \n    \n      0\n      1000\n      Boat Club Road, 411001, Pune, Maharastra\n    \n    \n      1\n      1001\n      Koregaon, 415501, Pune, Maharastra\n    \n    \n      2\n      1002\n      Kothrud, 411038, Pune, Maharastra\n    \n    \n      3\n      1003\n      Balewadi, 411045, Pune, Maharastra\n    \n    \n      4\n      1004\n      Baner, 411047, Pune, Maharastra\n    \n  \n\n\n\n\n\n# Import the geocoding tool\nfrom geopandas.tools import geocode\n\n# Geocode addresses using Nominatim. Remember to provide a custom \"application name\" in the user_agent parameter!\ngeo = geocode(data[\"addr\"], provider=\"nominatim\", user_agent=\"autogis_xx\", timeout=4)\n\n\ngeo.head()\n\n\n\n\n\n  \n    \n      \n      geometry\n      address\n    \n  \n  \n    \n      0\n      POINT (73.87826 18.53937)\n      Boat Club Road, Pune City, Pune, Maharashtra, ...\n    \n    \n      1\n      POINT (73.89299 18.53772)\n      Koregaon Park, Suyojan Society, Ghorpuri, Pune...\n    \n    \n      2\n      POINT (73.80767 18.50389)\n      Kothrud, Pune City, Maharashtra, 411038, India\n    \n    \n      3\n      POINT (73.76912 18.57767)\n      Prakashgad Society, Balewadi, Perfect 10 Inter...\n    \n    \n      4\n      POINT (73.77686 18.56424)\n      Baner, Pune City, Maharashtra, 511045, India\n    \n  \n\n\n\n\n\n# Joining the original dataframe with geoencoded dataframe\njoin = geo.join(data)\njoin = join.drop(columns=['address'])\n\n\njoin.head()\n\n\n\n\n\n  \n    \n      \n      geometry\n      id\n      addr\n    \n  \n  \n    \n      0\n      POINT (73.87826 18.53937)\n      1000\n      Boat Club Road, 411001, Pune, Maharastra\n    \n    \n      1\n      POINT (73.89299 18.53772)\n      1001\n      Koregaon, 415501, Pune, Maharastra\n    \n    \n      2\n      POINT (73.80767 18.50389)\n      1002\n      Kothrud, 411038, Pune, Maharastra\n    \n    \n      3\n      POINT (73.76912 18.57767)\n      1003\n      Balewadi, 411045, Pune, Maharastra\n    \n    \n      4\n      POINT (73.77686 18.56424)\n      1004\n      Baner, 411047, Pune, Maharastra\n    \n  \n\n\n\n\n\n\n\n\nfrom shapely.geometry import box\n\nminx = 73.76\nminy = 18.537\nmaxx = 73.89\nmaxy = 18.56\ngeom = box(minx, miny, maxx, maxy)\nclipping_gdf = gpd.GeoDataFrame({\"geometry\": [geom]}, index=[0], crs=\"epsg:4326\")\n\n# Explore the extent on a map\nclipping_gdf.explore()\n\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\n\n# Write the data to shape file\noutfp = r\"../data/addresses.shp\"\njoin.to_file(outfp)"
  },
  {
    "objectID": "spatial_data_processing/crs.html",
    "href": "spatial_data_processing/crs.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Coordinate Reference Systems (CRS)\nA CRS tells python how coordinates are related to places on the Earth. A map projection (or a projected coordinate system) is a systematic transformation of the latitudes and longitudes into a plain surface where units are quite commonly represented as meters (instead of decimal degrees). This transformation is used to represent the three dimensional earth on a flat, two dimensional map.\nThere is no perfect projection and we should know the strength and weaknesses of projection systems and choose a projection system that best fits our purpose.\nWe can reproject the geometries from crs to another using to_crs() function from GeoPandas.\nWe can define the coordinate system in different formats using pyproj CRS\n\nImport and view the data\n\nimport geopandas as gpd\n\n\n# Read the data\nworld = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n\n\nworld.head(4)\n\n\n\n\n\n  \n    \n      \n      pop_est\n      continent\n      name\n      iso_a3\n      gdp_md_est\n      geometry\n    \n  \n  \n    \n      0\n      889953.0\n      Oceania\n      Fiji\n      FJI\n      5496\n      MULTIPOLYGON (((180.00000 -16.06713, 180.00000...\n    \n    \n      1\n      58005463.0\n      Africa\n      Tanzania\n      TZA\n      63177\n      POLYGON ((33.90371 -0.95000, 34.07262 -1.05982...\n    \n    \n      2\n      603253.0\n      Africa\n      W. Sahara\n      ESH\n      907\n      POLYGON ((-8.66559 27.65643, -8.66512 27.58948...\n    \n    \n      3\n      37589262.0\n      North America\n      Canada\n      CAN\n      1736425\n      MULTIPOLYGON (((-122.84000 49.00000, -122.9742...\n    \n  \n\n\n\n\n\n\nView the CRS of the data\n\n# Check the CRS of the data.\n# Lat Long data should have EPSG 4326 and WGS 84\nworld.crs\n\n<Geographic 2D CRS: EPSG:4326>\nName: WGS 84\nAxis Info [ellipsoidal]:\n- Lat[north]: Geodetic latitude (degree)\n- Lon[east]: Geodetic longitude (degree)\nArea of Use:\n- name: World.\n- bounds: (-180.0, -90.0, 180.0, 90.0)\nDatum: World Geodetic System 1984 ensemble\n- Ellipsoid: WGS 84\n- Prime Meridian: Greenwich\n\n\n\n\nChange the CRS and visualize the data\n\nax = world.plot()\nax.set_title(\"WGS84 (lat/lon)\")\nworld = world[(world.name != \"Antarctica\") & (world.name != \"Fr. S. Antarctic Lands\")]\n# Data in Mercator Projection\nworld = world.to_crs(\"EPSG:3395\")\nax = world.plot()\nax.set_title(\"Mercator\")\n\nText(0.5, 1.0, 'Mercator')\n\n\n\n\n\n\n\n\n\n\nOrthographic Projection\n\n# Orthographic projection\nfrom pyproj import CRS\n\n\n# Define an orthographic projection, from: http://www.statsmapsnpix.com/2019/09/globe-projections-and-insets-in-qgis.html\northo = CRS.from_proj4(\n    \"+proj=ortho +lat_0=60.00 +lon_0=23.0000 +x_0=0 +y_0=0 +a=6370997 +b=6370997 +units=m +no_defs\"\n)\n\n# Re-project and plot\nax = world.to_crs(ortho).plot()\n\n# Remove x and y axis\nax.axis(\"off\")\nax.set_title(\"Orthographic\")\n\nText(0.5, 1.0, 'Orthographic')"
  },
  {
    "objectID": "spatial_data_processing/gee_timelapse.html",
    "href": "spatial_data_processing/gee_timelapse.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Steps to create a Landsat timelapse:\n\nPan and zoom to your area of interest, or click the globe icon at the upper left corner to search for a location.\nUse the drawing tool to draw a rectangle anywhere on the map.\nAdjust the parameters (e.g., start year, end year, title) if needed.\nClick the Create timelapse button to create a timelapse.\nOnce the timelapse has been added to the map, click the hyperlink at the end if you want to download the GIF.\n\n\nimport os\nimport ee\nimport geemap\nimport ipywidgets as widgets\n\n\nMap = geemap.Map()\nMap.add_basemap('HYBRID')\nMap\n\n\n\n\nTraitError: The 'value' trait of a Label instance expected a unicode string, not the int 10.\n\n\nTraitError: The 'value' trait of a Label instance expected a unicode string, not the int 1984.\n\n\nTraitError: The 'value' trait of a Label instance expected a unicode string, not the int 2020.\n\n\nTraitError: The 'value' trait of a Label instance expected a unicode string, not the int 5.\n\n\nTraitError: The 'value' trait of a Label instance expected a unicode string, not the int 10.\n\n\nTraitError: The 'value' trait of a Label instance expected a unicode string, not the int 30.\n\n\nTraitError: The 'value' trait of a Label instance expected a unicode string, not the int 0.\n\n\n\n\n\nGenerating URL...\nDownloading GIF image from https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/videoThumbnails/8557d6ed3c0cdb0ffa673666569b4260-0e20ead6bdef0b1d973c2259323cb508:getPixels\nPlease wait ...\nAn error occurred while downloading.\nUser memory limit exceeded.\nThe input gif file does not exist.\nThe input gif file does not exist.\nAdding GIF to the map ...\nThe provided file does not exist.\nThe timelapse has been added to the map.\n\n\nGenerating URL...\nDownloading GIF image from https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/videoThumbnails/8557d6ed3c0cdb0ffa673666569b4260-0ed6f7095071ae8dea998b8fcb895851:getPixels\nPlease wait ...\nAn error occurred while downloading.\nUser memory limit exceeded.\nThe input gif file does not exist.\nThe input gif file does not exist.\nAdding GIF to the map ...\nThe provided file does not exist.\nThe timelapse has been added to the map.\n\n\nGenerating URL...\nDownloading GIF image from https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/videoThumbnails/8557d6ed3c0cdb0ffa673666569b4260-be14a4d9b69711ca1550db81d8fda1a8:getPixels\nPlease wait ...\nAn error occurred while downloading.\nUser memory limit exceeded.\nThe input gif file does not exist.\nThe input gif file does not exist.\nAdding GIF to the map ...\nThe provided file does not exist.\nThe timelapse has been added to the map."
  },
  {
    "objectID": "spatial_data_processing/raster_data_processing.html",
    "href": "spatial_data_processing/raster_data_processing.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Converting Data from Raster to Tabular (Geometry) format\n\nImport the libraries\n\nimport pandas\nimport osmnx\nimport geopandas \nimport rioxarray\nimport xarray\nimport datashader as ds\nimport contextily as cx\nfrom shapely import geometry\nimport matplotlib.pyplot as plt\nimport folium\n\n\nimport warnings\nwarnings.filterwarnings(action='ignore')\n\n\n\nDownload Geopackage\n\n# URL for the geopackage\nurl = (\"https://jeodpp.jrc.ec.europa.eu/ftp/\"\\\n       \"jrc-opendata/GHSL/\"\\\n       \"GHS_FUA_UCDB2015_GLOBE_R2019A/V1-0/\"\\\n       \"GHS_FUA_UCDB2015_GLOBE_R2019A_54009_1K_V1_0.zip\"\n      )\nurl\n\n'https://jeodpp.jrc.ec.europa.eu/ftp/jrc-opendata/GHSL/GHS_FUA_UCDB2015_GLOBE_R2019A/V1-0/GHS_FUA_UCDB2015_GLOBE_R2019A_54009_1K_V1_0.zip'\n\n\n\n\nVisualize the map\n\n# Visualize the Map for Sao Paulo\np = f\"zip+{url}!GHS_FUA_UCDB2015_GLOBE_R2019A_54009_1K_V1_0.gpkg\"\nfuas = geopandas.read_file(p)\nsao_paulo = fuas.query(\"eFUA_name == 'São Paulo'\").to_crs(\"EPSG:4326\")\n\n\nax = sao_paulo.plot(alpha=0.5, figsize=(9, 9))\ncx.add_basemap(ax, crs=sao_paulo.crs);\n\n\n\n\n\n\nDownload the population data\n\nurl = (\"https://cidportal.jrc.ec.europa.eu/ftp/\"\\\n       \"jrc-opendata/GHSL/GHS_POP_MT_GLOBE_R2019A/\"\\\n       \"GHS_POP_E2015_GLOBE_R2019A_54009_250/V1-0/\"\\\n       \"tiles/\"\\\n       \"GHS_POP_E2015_GLOBE_R2019A_54009_250_V1_0_13_11.zip\"\n      )\nurl\n\n'https://cidportal.jrc.ec.europa.eu/ftp/jrc-opendata/GHSL/GHS_POP_MT_GLOBE_R2019A/GHS_POP_E2015_GLOBE_R2019A_54009_250/V1-0/tiles/GHS_POP_E2015_GLOBE_R2019A_54009_250_V1_0_13_11.zip'\n\n\n\n# Population data in raster format\n%%time\np = f\"zip+{url}!GHS_POP_E2015_GLOBE_R2019A_54009_250_V1_0_13_11.tif\"\nghsl = rioxarray.open_rasterio(p)\nghsl\n\nCPU times: user 35.6 ms, sys: 4.12 ms, total: 39.8 ms\nWall time: 7.12 s\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.DataArray (band: 1, y: 4000, x: 4000)>\n[16000000 values with dtype=float32]\nCoordinates:\n  * band         (band) int64 1\n  * x            (x) float64 -5.041e+06 -5.041e+06 ... -4.041e+06 -4.041e+06\n  * y            (y) float64 -2e+06 -2e+06 -2.001e+06 ... -3e+06 -3e+06\n    spatial_ref  int64 0\nAttributes:\n    AREA_OR_POINT:  Area\n    _FillValue:     -200.0\n    scale_factor:   1.0\n    add_offset:     0.0xarray.DataArrayband: 1y: 4000x: 4000...[16000000 values with dtype=float32]Coordinates: (4)band(band)int641array([1])x(x)float64-5.041e+06 ... -4.041e+06array([-5040875., -5040625., -5040375., ..., -4041625., -4041375., -4041125.])y(y)float64-2e+06 -2e+06 ... -3e+06 -3e+06array([-2000125., -2000375., -2000625., ..., -2999375., -2999625., -2999875.])spatial_ref()int640crs_wkt :PROJCS[\"World_Mollweide\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0],UNIT[\"Degree\",0.0174532925199433]],PROJECTION[\"Mollweide\"],PARAMETER[\"central_meridian\",0],PARAMETER[\"false_easting\",0],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]spatial_ref :PROJCS[\"World_Mollweide\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0],UNIT[\"Degree\",0.0174532925199433]],PROJECTION[\"Mollweide\"],PARAMETER[\"central_meridian\",0],PARAMETER[\"false_easting\",0],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]GeoTransform :-5041000.0 250.0 0.0 -2000000.0 0.0 -250.0array(0)Indexes: (3)bandPandasIndexPandasIndex(Int64Index([1], dtype='int64', name='band'))xPandasIndexPandasIndex(Float64Index([-5040875.0, -5040625.0, -5040375.0, -5040125.0, -5039875.0,\n              -5039625.0, -5039375.0, -5039125.0, -5038875.0, -5038625.0,\n              ...\n              -4043375.0, -4043125.0, -4042875.0, -4042625.0, -4042375.0,\n              -4042125.0, -4041875.0, -4041625.0, -4041375.0, -4041125.0],\n             dtype='float64', name='x', length=4000))yPandasIndexPandasIndex(Float64Index([-2000125.0, -2000375.0, -2000625.0, -2000875.0, -2001125.0,\n              -2001375.0, -2001625.0, -2001875.0, -2002125.0, -2002375.0,\n              ...\n              -2997625.0, -2997875.0, -2998125.0, -2998375.0, -2998625.0,\n              -2998875.0, -2999125.0, -2999375.0, -2999625.0, -2999875.0],\n             dtype='float64', name='y', length=4000))Attributes: (4)AREA_OR_POINT :Area_FillValue :-200.0scale_factor :1.0add_offset :0.0\n\n\n\n\nVisualize the population on raster data\n\ncvs = ds.Canvas(plot_width=600, plot_height=600)\nagg = cvs.raster(ghsl.where(ghsl>0).sel(band=1))\n\n\nf, ax = plt.subplots(1, figsize=(9, 7))\nagg.plot.imshow(ax=ax, alpha=0.5, cmap=\"cividis_r\")\ncx.add_basemap(\n    ax, \n    crs=ghsl.rio.crs, \n    zorder=-1, \n    source=cx.providers.CartoDB.Voyager\n)\n\n\n\n\n\n# Clip the data for Sao Paulo\nghsl_sp = ghsl.rio.clip(sao_paulo.to_crs(ghsl.rio.crs).geometry.iloc[0])\nghsl_sp\n\n/home/thulasiram/miniconda3/envs/geopy/lib/python3.9/site-packages/rasterio/features.py:290: ShapelyDeprecationWarning: Iteration over multi-part geometries is deprecated and will be removed in Shapely 2.0. Use the `geoms` property to access the constituent parts of a multi-part geometry.\n  for index, item in enumerate(shapes):\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.DataArray (band: 1, y: 416, x: 468)>\narray([[[-200., -200., -200., ..., -200., -200., -200.],\n        [-200., -200., -200., ..., -200., -200., -200.],\n        [-200., -200., -200., ..., -200., -200., -200.],\n        ...,\n        [-200., -200., -200., ..., -200., -200., -200.],\n        [-200., -200., -200., ..., -200., -200., -200.],\n        [-200., -200., -200., ..., -200., -200., -200.]]], dtype=float32)\nCoordinates:\n  * band         (band) int64 1\n  * x            (x) float64 -4.482e+06 -4.482e+06 ... -4.365e+06 -4.365e+06\n  * y            (y) float64 -2.822e+06 -2.822e+06 ... -2.926e+06 -2.926e+06\n    spatial_ref  int64 0\nAttributes:\n    AREA_OR_POINT:  Area\n    scale_factor:   1.0\n    add_offset:     0.0\n    _FillValue:     -200.0xarray.DataArrayband: 1y: 416x: 468-200.0 -200.0 -200.0 -200.0 -200.0 ... -200.0 -200.0 -200.0 -200.0array([[[-200., -200., -200., ..., -200., -200., -200.],\n        [-200., -200., -200., ..., -200., -200., -200.],\n        [-200., -200., -200., ..., -200., -200., -200.],\n        ...,\n        [-200., -200., -200., ..., -200., -200., -200.],\n        [-200., -200., -200., ..., -200., -200., -200.],\n        [-200., -200., -200., ..., -200., -200., -200.]]], dtype=float32)Coordinates: (4)band(band)int641array([1])x(x)float64-4.482e+06 ... -4.365e+06axis :Xlong_name :x coordinate of projectionstandard_name :projection_x_coordinateunits :metrearray([-4481875., -4481625., -4481375., ..., -4365625., -4365375., -4365125.])y(y)float64-2.822e+06 ... -2.926e+06axis :Ylong_name :y coordinate of projectionstandard_name :projection_y_coordinateunits :metrearray([-2822125., -2822375., -2822625., ..., -2925375., -2925625., -2925875.])spatial_ref()int640crs_wkt :PROJCS[\"World_Mollweide\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0],UNIT[\"Degree\",0.0174532925199433]],PROJECTION[\"Mollweide\"],PARAMETER[\"central_meridian\",0],PARAMETER[\"false_easting\",0],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]spatial_ref :PROJCS[\"World_Mollweide\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0],UNIT[\"Degree\",0.0174532925199433]],PROJECTION[\"Mollweide\"],PARAMETER[\"central_meridian\",0],PARAMETER[\"false_easting\",0],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]GeoTransform :-4482000.0 250.0 0.0 -2822000.0 0.0 -250.0array(0)Indexes: (3)bandPandasIndexPandasIndex(Int64Index([1], dtype='int64', name='band'))xPandasIndexPandasIndex(Float64Index([-4481875.0, -4481625.0, -4481375.0, -4481125.0, -4480875.0,\n              -4480625.0, -4480375.0, -4480125.0, -4479875.0, -4479625.0,\n              ...\n              -4367375.0, -4367125.0, -4366875.0, -4366625.0, -4366375.0,\n              -4366125.0, -4365875.0, -4365625.0, -4365375.0, -4365125.0],\n             dtype='float64', name='x', length=468))yPandasIndexPandasIndex(Float64Index([-2822125.0, -2822375.0, -2822625.0, -2822875.0, -2823125.0,\n              -2823375.0, -2823625.0, -2823875.0, -2824125.0, -2824375.0,\n              ...\n              -2923625.0, -2923875.0, -2924125.0, -2924375.0, -2924625.0,\n              -2924875.0, -2925125.0, -2925375.0, -2925625.0, -2925875.0],\n             dtype='float64', name='y', length=416))Attributes: (4)AREA_OR_POINT :Areascale_factor :1.0add_offset :0.0_FillValue :-200.0\n\n\nout_p = “../data/ghsl_sao_paulo.tif” ! rm $out_p ghsl_sp.rio.to_raster(out_p)\n\n\nConvert Raster to geometry\n\n# Read the raster data\nsurface = xarray.open_rasterio(\"../data/ghsl_sao_paulo.tif\")\n\n\n# Convert raster to geometry\nt_surface = surface.to_series()\n\n\nt_surface.head()\n\nband  y           x         \n1     -2822125.0  -4481875.0   -200.0\n                  -4481625.0   -200.0\n                  -4481375.0   -200.0\n                  -4481125.0   -200.0\n                  -4480875.0   -200.0\ndtype: float32\n\n\n\nt_surface = t_surface.reset_index().rename(columns={0: \"Value\"})\n\n\nt_surface.query(\"Value > 1000\").info()\n\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 7734 entries, 3785 to 181296\nData columns (total 4 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   band    7734 non-null   int64  \n 1   y       7734 non-null   float64\n 2   x       7734 non-null   float64\n 3   Value   7734 non-null   float32\ndtypes: float32(1), float64(2), int64(1)\nmemory usage: 271.9 KB\n\n\n\ntype(t_surface)\n\npandas.core.frame.DataFrame\n\n\n\n# Calculate the polygon based on resolution values\ndef row2cell(row, res_xy):\n    res_x, res_y = res_xy  # Extract resolution for each dimension\n    # XY Coordinates are centered on the pixel\n    minX = row[\"x\"] - (res_x / 2)\n    maxX = row[\"x\"] + (res_x / 2)\n    minY = row[\"y\"] + (res_y / 2)\n    maxY = row[\"y\"] - (res_y / 2)\n    poly = geometry.box(\n        minX, minY, maxX, maxY\n    )  # Build squared polygon\n    return poly\n\n\n# Get the polygons\nmax_polys = (\n    t_surface.query(\n        \"Value > 1000\"\n    )  # Keep only cells with more than 1k people\n    .apply(  # Build polygons for selected cells\n        row2cell, res_xy=surface.attrs[\"res\"], axis=1\n    )\n    .pipe(  # Pipe result from apply to convert into a GeoSeries\n        geopandas.GeoSeries, crs=surface.attrs[\"crs\"]\n    )\n)\n\n\n# Plot polygons on the map\nax = max_polys.plot(edgecolor=\"red\", figsize=(9, 9))\n# Add basemap\ncx.add_basemap(\n    ax, crs=surface.attrs[\"crs\"], source=cx.providers.CartoDB.Voyager\n)\n\n\n\n\n\n\nConvert Geometry to Raster\n\nnew_da = xarray.DataArray.from_series(\n    t_surface.set_index([\"band\", \"y\", \"x\"])[\"Value\"]\n)\nnew_da\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.DataArray 'Value' (band: 1, y: 416, x: 468)>\narray([[[-200., -200., -200., ..., -200., -200., -200.],\n        [-200., -200., -200., ..., -200., -200., -200.],\n        [-200., -200., -200., ..., -200., -200., -200.],\n        ...,\n        [-200., -200., -200., ..., -200., -200., -200.],\n        [-200., -200., -200., ..., -200., -200., -200.],\n        [-200., -200., -200., ..., -200., -200., -200.]]], dtype=float32)\nCoordinates:\n  * band     (band) int64 1\n  * y        (y) float64 -2.926e+06 -2.926e+06 ... -2.822e+06 -2.822e+06\n  * x        (x) float64 -4.482e+06 -4.482e+06 ... -4.365e+06 -4.365e+06xarray.DataArray'Value'band: 1y: 416x: 468-200.0 -200.0 -200.0 -200.0 -200.0 ... -200.0 -200.0 -200.0 -200.0array([[[-200., -200., -200., ..., -200., -200., -200.],\n        [-200., -200., -200., ..., -200., -200., -200.],\n        [-200., -200., -200., ..., -200., -200., -200.],\n        ...,\n        [-200., -200., -200., ..., -200., -200., -200.],\n        [-200., -200., -200., ..., -200., -200., -200.],\n        [-200., -200., -200., ..., -200., -200., -200.]]], dtype=float32)Coordinates: (3)band(band)int641array([1])y(y)float64-2.926e+06 ... -2.822e+06array([-2925875., -2925625., -2925375., ..., -2822625., -2822375., -2822125.])x(x)float64-4.482e+06 ... -4.365e+06array([-4481875., -4481625., -4481375., ..., -4365625., -4365375., -4365125.])Indexes: (3)bandPandasIndexPandasIndex(Int64Index([1], dtype='int64', name='band'))yPandasIndexPandasIndex(Float64Index([-2925875.0, -2925625.0, -2925375.0, -2925125.0, -2924875.0,\n              -2924625.0, -2924375.0, -2924125.0, -2923875.0, -2923625.0,\n              ...\n              -2824375.0, -2824125.0, -2823875.0, -2823625.0, -2823375.0,\n              -2823125.0, -2822875.0, -2822625.0, -2822375.0, -2822125.0],\n             dtype='float64', name='y', length=416))xPandasIndexPandasIndex(Float64Index([-4481875.0, -4481625.0, -4481375.0, -4481125.0, -4480875.0,\n              -4480625.0, -4480375.0, -4480125.0, -4479875.0, -4479625.0,\n              ...\n              -4367375.0, -4367125.0, -4366875.0, -4366625.0, -4366375.0,\n              -4366125.0, -4365875.0, -4365625.0, -4365375.0, -4365125.0],\n             dtype='float64', name='x', length=468))Attributes: (0)"
  },
  {
    "objectID": "3D_deep_learning/02_coordination_systems.html",
    "href": "3D_deep_learning/02_coordination_systems.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "The first coordination system we frequently use is called the world coordination system. This coordinate system is a 3D coordination system chosen with respect to all the 3D objects, such that the locations of the 3D objects can be easy to determine. Usually, the axis of the world coordination system does not agree with the object orientation or camera orientation.\n\n\n\nIn PyTorch3D, the camera view coordination system is defined such that the origin is at the projection point of the camera, the x axis points to the left, the y axis points upward, and the z axis points to the front.\n\n\n\ncamera view coordination system in pytorch3d\n\n\n\n\n\nThe normalized device coordinate (NDC) confines the volume that a camera can render. The x coordinate values in the NDC space range from -1 to +1, as do the y coordinate values. The z coordinate values range from znear to zfar, where znear is the nearest depth and zfar is the farthest depth. Any object out of this znear to zfar range would not be rendered by the camera.\n\n\n\nIt is defined in terms of how the rendered images are shown on our screens. The coordinate system contains the x coordinate as the columns of the pixels, the y coordinate as the rows of the pixels, and the z coordinate corresponding to the depth of the object.\n\n\n\nCamera models are to create a correspondence between 2D space and the 3D world. In pytorch3D there are two major camera models\n\n\n\nPytorch3D camera projections\n\n\n\n\nThe orthographic projections map objects to 2D images, disregarding the object depth. For example, just as shown in the figure, two objects with the same geometric size at different depths would be mapped to 2D images of the same size.\n\n\n\nIf an object moved far away from the camera, it would be mapped to a smaller size on the 2D images."
  },
  {
    "objectID": "3D_deep_learning/resources.html",
    "href": "3D_deep_learning/resources.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Resources\n\nMega Github resource\npytorch3d\nmeshrcnn\n3D Deep learning youtube videos\nNvidia Kaolin"
  },
  {
    "objectID": "3D_deep_learning/01_data_formats.html",
    "href": "3D_deep_learning/01_data_formats.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Depth images\nPoint clouds\nVoxels\nMeshes\n\n\n\nDepth images contain the depth values of a scene in the form of distance from the camera in meters for each pixel in the image frame.\n\n\n\nA Point cloud is a collection of three-dimensional points distributed in a 3D space. Each of these 3D points has a deterministic position denoted by a certain (x, y, z) coordinate along with other attributes like RGB colour values. Unlike depth images, point cloud representation preserves more high-quality geometric information of the three-dimensional space without any discretization.\nPoint clouds do not have grid-like structures, thus convolutions cannot be directly used for them. They are one of the unordered and irregular data types. There are no clear and regular definitions for neighboring points for each point in a point cloud.special types of deep learning models need to be used for processing point clouds, such as PointNet\nAnother issue for point clouds as training data for 3D deep learning is the heterogeneous data issue – that is, for one training dataset, different point clouds may contain different numbers of 3D points.\n\n\n\nA voxel is just a pixel in a three-dimensional space. A voxel is defined by dividing a 3D cube into smaller-sized cubes and each cube is called one voxel.\nVoxel usually use Truncated Signed Distance Functions (TSDFs) to represent 3D surfaces.\nA Signed Distance Function (SDF) can be defined at each voxel as the (signed) distance between the center of the voxel to the closest point on the surface. A positive sign in an SDF indicates that the voxel center is outside an object. The only difference between a TSDF and an SDF is that the values of a TSDF are truncated, such that the values of a TSDF always range from -1 to +1.\n#### Polygon Mesh A Polygon Mesh is a collection of edges, vertices and faces that together defines the shape and volume of a polyhedral object.\nEach mesh contains a set of 3D points called vertices. Each mesh also contains a set of polygons called faces, which are defined on vertices.\nMeshes also have heterogeneous data issues.\n\n\n\n\nply\nobj\n\n\n\n\n\nLink to Blog post\nBook - 3D deep learning with python"
  },
  {
    "objectID": "3D_deep_learning/07_gan_based_image_synthesis.html",
    "href": "3D_deep_learning/07_gan_based_image_synthesis.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Compositional 3D Aware Image Synthesis"
  },
  {
    "objectID": "3D_deep_learning/06_Differentiable_volumetric_rendering.html",
    "href": "3D_deep_learning/06_Differentiable_volumetric_rendering.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Volumetric rendering is a collection of techniques used to generate a 2D view of discrete 3D data.The projections generated from this method are without any explicit conversion to a geometric representation. Volumetric rendering is typically used when generating surfaces is difficult or can lead to errors. It can also be used when the content (and not just the geometry and surface) of the volume is important. It is typically used for data visualization.\n\n\n\nVolumetric Rendering\n\n\n\n\nTo determine the RGB values at each pixel, a ray is generated from the projection center going through each image pixel of the cameras. We need to check the probability of occupancy or opacity and colors along this ray to determine RGB values for the pixel. Note there are infinitely many points on each such ray. Thus, we need to have a sampling scheme to select a certain number of points along this ray. This sampling operation is called ray sampling.\n\n\n\nwe have densities and colors defined on the nodes of the volume but not on the points on the rays. Thus, we need to have a way to convert densities and colors of volumes to points on rays. This operation is called volume sampling.\nThe points defined in the ray sampling step might not fall exactly on a point. The nodes of the volume grids and points on rays typically have different spatial locations. We need to use an interpolation scheme to interpolate the densities and colors at points of rays from the densities and colors at volumes.\n\n\n\nFrom the densities and colors of the rays, we need to determine the RGB values of each pixel. In this process, we need to compute how many incident lights can arrive at each point along the ray and how many lights are reflected to the image pixel. We call this process ray marching.\n\n\n\nWhile standard volumetric rendering is used to render 2D projections of 3D data, differentiable volume rendering is used to do the opposite: construct 3D data from 2D images.\n\n\nWe will have ground-truth images. From the intial 3D model, 2D images are rendered and compared to the ground truth images. As this process is differentiable, we run an optimization algorithm to get the final resulting volumetric model. This model can be used to render images from new angles.\nwe represent the shape and texture of the object as a parametric function. This function can be used to generate 2D projections. But, given 2D projections (this is typically multiple views of the 3D scene), we can optimize the parameters of these implicit shape and texture functions so that its projections are the multi-view 2D images. This optimization is possible since the rendering process is completely differentiable, and the implicit functions used are also differentiable."
  },
  {
    "objectID": "3D_deep_learning/05_differentiable_rendering.html",
    "href": "3D_deep_learning/05_differentiable_rendering.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "The physical process of image formation is a mapping from 3D models to 2D images.\n\n\n\nThe image formation process is a mapping from the 3D models to 2D images\n\n\nMany 3D computer vision problems are a reversal of image formation. In these problems, we are usually given 2D images and need to estimate the 3D models from the 2D images.\n\n\n\nMany 3D computer vision problems are based on 2D images given to estimate 3D models\n\n\nwe can cast this as an optimization problem. We can minimize the distance between rendered image and the original image. To do this the rendering should be differentiable.\n\n\n\n\n\nBy considering a weighted average of all the relevant mesh faces instead of single mesh face per ray we can make the rendering differentiable"
  },
  {
    "objectID": "3D_deep_learning/04_fitting_deformable_mesh_to_point.html",
    "href": "3D_deep_learning/04_fitting_deformable_mesh_to_point.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Real-world depth cameras, such as LiDAR, time-of-flight cameras, and stereo vision cameras, usually output either depth images or point clouds. These devices do not give any direct measurements of surfaces. However, in many real-world applications, surface information is sought.\n\n\nThe cost function should be chosen such that it is a good measurement of how similar the point cloud is to the mesh. One such cost function is Chamfer set distance.\n\n\n\nChamfer distance\n\n\nFor fitting meshes to point clouds, we first randomly sample some points from a mesh model and then optimize the Chamfer distances between the sampled points from the mesh model and the input point cloud.\n\n\n\nThere may exist multiple mesh models that can be good fits to the same point cloud.These mesh models that are good fits may include some mesh models that are far away from smooth meshes.\nRegularization methods:- * Mesh laplacian smoothing loss * Mesh normal consistency loss * Mesh edge loss\n\n\n\n\n\nMesh Laplacian Smoothing loss\n\n\nIn the preceding definition, the Laplacian at the i-th vertex is just a sum of differences, where each difference is between the coordinates of the current vertex and those of a neighboring vertex.\nThe Laplacian is a measurement for smoothness. If the i-th vertex and its neighbors lie all within one plane, then the Laplacian should be zero.\n\n\n\nThe mesh normal consistency loss is a loss function for penalizing the distances between adjacent normal vectors on the mesh.\n\n\n\n\nIt is sum of all the edge lengths in the mesh. Longer edge lenghts will not capture the fine details of slowly varying surfaces.\n\n\n\n\nAn Example of 3D point cloud of a Pedestrian\n\n\n\n\n\nOptimized deformed mesh model. We have far more points than the original input point cloud. All the different losses are weighted"
  },
  {
    "objectID": "3D_deep_learning/03_rendering.html",
    "href": "3D_deep_learning/03_rendering.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Rendering is the process of projecting 3D physical models into 2D images\nIn the below example, the world model contains one 3D sphere, which is represented by a mesh model. To form the image of the 3D sphere, for each image pixel, we generate one ray, starting from the camera origin and going through the image pixel. If one ray intersects with one mesh face, then we know the mesh face can project its color to the image pixel. We also need to trace the depth of each intersection because a face with a smaller depth would occlude faces with larger depths.\n\n\n\nRendering\n\n\nRendering usually is divided into two stages * Rasterization * Shading\n\n\nIt is the process of finding relevant geometric objects for each image pixel\n\n\n\nIt is the process of taking the outputs of the rasterization and computing the pixel value for each image pixel.\n\n\nLambertian surfaces are types of objects that are not shiny at all, such as paper, unfinished wood, and unpolished stones.\n\n\n\nLambertian shading model\n\n\nOne basic idea of the Lambertian cosine law is that for Lambertian surfaces, the amplitude of the reflected light does not depend on the viewer’s angle, but only depends on the angle between the surface normal and the direction of the incident light.\n\n\n\nThe Phong lighting model is a frequently used model for these glossy components.\nOne basic principle of the Phong lighting model is that the shiny light component should be strongest in the direction of reflection of the incoming light. The component would become weaker as the angle between the direction of reflection and the viewing angle becomes larger.\n\n\n\nPhong model"
  },
  {
    "objectID": "3D_deep_learning/06_nerf.html",
    "href": "3D_deep_learning/06_nerf.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Differentiable volume Rendering will require a lot of storage. This is undesirable if we want to transmit this information over the network.\nNeRF is one of the first techniques to model a 3D scene that requires less disk space and also captures the fine geometry and texture of complex scenes.\n\n\nThe challenge is to synthesize new views of a 3D scene using a small number of available 2D snapshots of the scene. The challenge is to construct complete information about the world given incomplete and noisy information.\n## Radiance Fields Radiance is the intensity of a point in space when viewed in a particular direction. When capturing this information in RGB, the radiance will have three components corresponding to the colors Red, Green, and Blue. The radiance of a point in space can depend on many factors, including the following:\n\nLight sources illuminating that point\nThe existence of a surface (or volume density) that can reflect light at that point\nThe texture properties of the surface\n\n\n\n\nRadiance (r,g,b) at a point (x,y,z) when viewed from certain viewing angles\n\n\nIf we know the radiance of all the points in a scene in all directions then it constitutes a radiance field.\n\n\n\nNeRF uses a neural network to represent a volumetric scene function. This neural network takes a 5-dimensional input. These are the three spatial locations (x, y, z) and two viewing angles (θ, ∅). Its output is the volume density σ at (x, y, z) and the emitted color (r, g, b) of the point (x, y, z) when viewed from the viewing angle (θ, ∅).\nThe model therefore maps any point in the 3D scene and a viewing angle to the volume density and radiance at that point. You can then use this model to synthesize views by querying the 5D coordinates along camera rays and using the volume rendering technique to project the output colors and volume densities into an image.\n\n\n\nNeRF Architecture\n\n\nA single NeRF model is optimized on a set of images from a single scene. Therefore, each model only knows the scene on which it is optimized. NeRF is optimized to generalize unseen viewpoints well for a particular scene."
  },
  {
    "objectID": "pytorch/03_activation_functions.html",
    "href": "pytorch/03_activation_functions.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Activation Functions\n\nCreating our own activation functions in pytorch\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass ActivationFunction(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.name = self.__class__.__name__\n        self.config = {\"name\":self.name}\n\n\nSigmoid\nSigmoid activation function does not work well in the inner layers. The gradients of the inner layers becomes very small and it is very difficult to update them. The gradients of the outer layers are large compared to the gradients of the inner layers. A high learning rate is suitable for the inner layers and low learning rate would suit the outer layers. Also the gradients are not centered around zero which is not good for a neural network.\n\nclass Sigmoid(ActivationFunction):\n    def forward(self,x):\n        return 1/(1+torch.exp(-x))\n\n\n\n\nTanh\nTanh value range from -1 to 1. The gradients will be centered around zero which is good for the network.\n\nclass Tanh(ActivationFunction):\n    def forward(self,x):\n        x_exp, neg_x_exp = torch.exp(x), torch.exp(-x)\n        return (x_exp - neg_x_exp)/(x_exp + neg_x_exp)\n\n\n\nRELU\nThis activation function replaces the negative values with zero. The gradient of this activation function is either zero or 1 and it will look like a step function. The neurons whose activation is zero will not propogate the gradients and hence will not get updated. They are considered as dead neurons. Two ways to deal with this are to use higher learning rates or use a bias value which will avoid getting zero after activation. (Read about these methods). In practice RELU is the most commonly used activation function.\n\nclass ReLU(ActivationFunction):\n    def forward(self,x):\n        return x * (x > 0).float()\n        # return x.clamp(min=0)\n        # return max(x,0)\n\n\n\nLeaky ReLU\nThis function adds a small value (alpha) when the activation is negative. This will help overcome the dead neuron challenge. However we will have an additional hyperparameter to tune. The gradient instead of 0 and 1 will be alpha and 1.\n\nclass LeakyReLU(ActivationFunction):\n    def __init__(self, alpha=0.01):\n        super().__init__()        \n        self.config['alpha'] = alpha\n\n    def forward(self,x):\n        return torch.where(x > 0, x, x * self.config['alpha'])\n\n\n\nELU\nThe negative values are exponentially reduced.\n\nclass ELU(ActivationFunction):\n    def forward(self,x):\n        return torch.where(x > 0, x, torch.exp(x)-1)\n\n\n\nSwish\nExperimentation was done on alot of datasets and activation functions. In the test, Swish was found to perform very well. For a neural network with many layers, Swish can perform better. This results after activation is not monotonic.\n\nclass Swish(ActivationFunction):\n    def forward(self,x):\n        return x * torch.sigmoid(x)"
  },
  {
    "objectID": "pytorch/04_intialization.html",
    "href": "pytorch/04_intialization.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Initialization\n\n\nProperties for Initializing a neural network\n\nThe variance of the input should be propagated through the model to the last layer, so that we have a similar standard deviation for the output neurons\nIf the variance vanish in deeper layers then it is difficult to optimize\nGradient distribution with equal variance across layers. If this is not the case then we will have difficulties in selecting the learning rate"
  },
  {
    "objectID": "pytorch/01_introduction.html",
    "href": "pytorch/01_introduction.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "import os\nimport math\nimport numpy as np\nimport time\n\n\n# libraries for plotting\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom IPython.display import set_matplotlib_formats\nset_matplotlib_formats('svg','pdf')\nfrom matplotlib.colors import to_rgba\nimport seaborn as sns\nsns.set()\nfrom tqdm.notebook import tqdm\n\n/tmp/ipykernel_50982/195479827.py:4: DeprecationWarning: `set_matplotlib_formats` is deprecated since IPython 7.23, directly use `matplotlib_inline.backend_inline.set_matplotlib_formats()`\n  set_matplotlib_formats('svg','pdf')\n\n\n\n# Import and check torch version\nimport torch\nprint(\"using torch\", torch.__version__)\n\nusing torch 1.12.0+cu113\n\n\n/home/thulasiram/miniconda3/envs/pytorch_learn/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n\n\n\n# Set seed for reproducability\ntorch.manual_seed(42)\n\n<torch._C.Generator at 0x7feee50aa850>\n\n\n\n\n\n\n# Create a tensor\nx = torch.Tensor(2,3,4)\n\n\nprint(x)\n\ntensor([[[0., 0., 0., 0.],\n         [0., 0., 0., 0.],\n         [0., 0., 0., 0.]],\n\n        [[0., 0., 0., 0.],\n         [0., 0., 0., 0.],\n         [0., 0., 0., 0.]]])\n\n\n\n# Tensor filled with zeros\ntorch.zeros(2,3)\n\ntensor([[0., 0., 0.],\n        [0., 0., 0.]])\n\n\n\ntorch.ones(2,3)\n\ntensor([[1., 1., 1.],\n        [1., 1., 1.]])\n\n\n\n# Random values uniformly sampled between 0 and 1\ntorch.rand(2,3,4)\n\ntensor([[[0.2566, 0.7936, 0.9408, 0.1332],\n         [0.9346, 0.5936, 0.8694, 0.5677],\n         [0.7411, 0.4294, 0.8854, 0.5739]],\n\n        [[0.2666, 0.6274, 0.2696, 0.4414],\n         [0.2969, 0.8317, 0.1053, 0.2695],\n         [0.3588, 0.1994, 0.5472, 0.0062]]])\n\n\n\n# Create tensor in a given range with steps\ntorch.arange(0,20,2)\n\ntensor([ 0,  2,  4,  6,  8, 10, 12, 14, 16, 18])\n\n\n\n# Create tensor from a list\ntorch.Tensor([[1,2],[3,4]])\n\ntensor([[1., 2.],\n        [3., 4.]])\n\n\n\nx = torch.rand(2,3,4)\n\n\n# Getting the shape, size of a tensor\n# Shape is an attribute of the tensor\n# size() is a method of the tensor\nshape = x.shape\nprint(\"shape\", x.shape)\n\nsize = x.size()\nprint(\"size\", size)\n\ndim1, dim2, dim3 = x.size()\nprint(\"Size:\", dim1, dim2, dim3)\n\nshape torch.Size([2, 3, 4])\nsize torch.Size([2, 3, 4])\nSize: 2 3 4\n\n\n\n\n\n\n# convert numpy array to torch tensor\nnp_arr = np.array([[1,2],[3,4]])\ntensor = torch.from_numpy(np_arr)\nprint(\"Numpy array:\", np_arr)\nprint(\"pytorch tensor:\", tensor)\n\nNumpy array: [[1 2]\n [3 4]]\npytorch tensor: tensor([[1, 2],\n        [3, 4]])\n\n\n\n# convert torch tensor to numpy array\ntensor = torch.arange(10)\nnp_arr = tensor.numpy()\n\nprint(\"pytorch tensor:\", tensor)\nprint(\"Numpy array:\", np_arr)\n\npytorch tensor: tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\nNumpy array: [0 1 2 3 4 5 6 7 8 9]\n\n\n\n# set device as gpu\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Device:\", device)\n\nDevice: cuda:0\n\n\n\n# send the tensor to the gpu\ntensor = tensor.to(device)\n\n\n# We cannot convert a tensor on GPU to Numpy array\nnp_arr = tensor.numpy()\n\nTypeError: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.\n\n\n\n# Get the tensor on GPU to cpu and convert to numpy\nnp_arr = tensor.cpu().numpy()\nnp_arr\n\narray([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n\n\n\n\n\n\nx1 = torch.rand(2,3)\nx2 = torch.rand(2,3,)\ny = x1 + x2\n\nprint(\"x1\", x1)\nprint(\"x2\", x2)\nprint(\"y\", y)\n\nx1 tensor([[0.6440, 0.7071, 0.6581],\n        [0.4913, 0.8913, 0.1447]])\nx2 tensor([[0.5315, 0.1587, 0.6542],\n        [0.3278, 0.6532, 0.3958]])\ny tensor([[1.1755, 0.8658, 1.3123],\n        [0.8191, 1.5445, 0.5406]])\n\n\n\n# In place operation on x2 \n# x2 values will be changed in-place\nx1 = torch.rand(2,3)\nx2 = torch.rand(2,3)\nprint(\"x1(before)\",x1)\nprint(\"x2(before)\",x2)\n\n# In-place operations are usually marked with underscore postfix \"add_\"\nx2.add_(x1)\nprint(\"x1(after)\",x1)\nprint(\"x2(after)\",x2)\n\nx1(before) tensor([[0.9147, 0.2036, 0.2018],\n        [0.2018, 0.9497, 0.6666]])\nx2(before) tensor([[0.9811, 0.0874, 0.0041],\n        [0.1088, 0.1637, 0.7025]])\nx1(after) tensor([[0.9147, 0.2036, 0.2018],\n        [0.2018, 0.9497, 0.6666]])\nx2(after) tensor([[1.8958, 0.2910, 0.2059],\n        [0.3106, 1.1134, 1.3691]])\n\n\n\nx = torch.arange(8)\nprint(\"X\",x)\n\nX tensor([0, 1, 2, 3, 4, 5, 6, 7])\n\n\n\nx = x.view(2,4)\n\n\nx\n\ntensor([[0, 1, 2, 3],\n        [4, 5, 6, 7]])\n\n\n\n# swap the dimension 0 and 1\nx = x.permute(1,0)\nx\n\ntensor([[0, 4],\n        [1, 5],\n        [2, 6],\n        [3, 7]])\n\n\n\n# Matrix multiplication\n# we can also use x@y\nx = torch.arange(6).view(2,3)\nw = torch.arange(9).view(3,3)\nh = torch.matmul(x,w)\nprint(\"X\",x)\nprint(\"Y\",w)\nprint(\"h\",h)\n\nX tensor([[0, 1, 2],\n        [3, 4, 5]])\nY tensor([[0, 1, 2],\n        [3, 4, 5],\n        [6, 7, 8]])\nh tensor([[15, 18, 21],\n        [42, 54, 66]])\n\n\n\n\n\n\n# Indexing wrks like in numpy\nx = torch.arange(12).view(3,4)\nprint(\"X\",x)\n\nX tensor([[ 0,  1,  2,  3],\n        [ 4,  5,  6,  7],\n        [ 8,  9, 10, 11]])\n\n\n\n# Get second column\nx[:,1]\n\ntensor([1, 5, 9])\n\n\n\n# Get First Row\nx[0,:]\n\ntensor([0, 1, 2, 3])\n\n\n\n# Get First two rows and last column\nx[:2,-1]\n\ntensor([3, 7])\n\n\n\n# Get middle two rows\nx[1:3,:]\n\ntensor([[ 4,  5,  6,  7],\n        [ 8,  9, 10, 11]])\n\n\n\n\n\nPytorch provides the gradients/derivatives of functions that we define. Using pytorch we compute the output and ask pytorch to automatically gets the gradients.\n\n# we need to specify which tensors require gradients\nx = torch.ones((3,2))\nprint(x.requires_grad)\n\nFalse\n\n\nEither pass te argument requires_grad=True or change it for an existing tensor using the function requires_grad_()\n\nx.requires_grad_(True)\nprint(x.requires_grad)\n\nTrue\n\n\nwe will calculate the gradient for the function: \\[y = \\frac{1}{|x|}\\sum_i \\left[(x_i + 2)^2 + 3\\right]\\] We will imagine \\(x\\) as our parameters, and we want to optimize the output \\(y\\). For this, we want to obtain the gradients \\(\\partial y / \\partial \\mathbf{x}\\). For our example, we’ll use \\(\\mathbf{x}=[0,1,2]\\) as our input.\n\n# Only float tensors can have gradients\nx = torch.arange(3, dtype=torch.float32, requires_grad=True) \nprint(\"X\", x)\n\nX tensor([0., 1., 2.], requires_grad=True)\n\n\n\n# computation graph for the function\na = x+2\nb = a**2\nc = b+3\ny = c.mean()\nprint(\"y\",y)\n\ny tensor(12.6667, grad_fn=<MeanBackward0>)\n\n\nEach node of the computation graph has automatically defined a function for calculating the gradients with respect to its inputs, grad_fn. You can see this when we printed the output tensor \\(y\\). This is why the computation graph is usually visualized in the reverse direction (arrows point from the result to the inputs). We can perform backpropagation on the computation graph by calling the function backward() on the last output, which effectively calculates the gradients for each tensor that has the property requires_grad=True:\nUsing the statements above, we have created a computation graph that looks similar to the figure below:-\n\n\n\nPytorch Computation Graph\n\n\n\ny.backward()\n\n\nprint(x.grad)\n\ntensor([1.3333, 2.0000, 2.6667])\n\n\nWe can also verify these gradients by hand. We will calculate the gradients using the chain rule, in the same way as PyTorch did it:\n\\[\\frac{\\partial y}{\\partial x_i} = \\frac{\\partial y}{\\partial c_i}\\frac{\\partial c_i}{\\partial b_i}\\frac{\\partial b_i}{\\partial a_i}\\frac{\\partial a_i}{\\partial x_i}\\]\nNote that we have simplified this equation to index notation, and by using the fact that all operation besides the mean do not combine the elements in the tensor. The partial derivatives are:\n\\[\n\\frac{\\partial a_i}{\\partial x_i} = 1,\\hspace{1cm}\n\\frac{\\partial b_i}{\\partial a_i} = 2\\cdot a_i\\hspace{1cm}\n\\frac{\\partial c_i}{\\partial b_i} = 1\\hspace{1cm}\n\\frac{\\partial y}{\\partial c_i} = \\frac{1}{3}\n\\]\nHence, with the input being \\(\\mathbf{x}=[0,1,2]\\), our gradients are \\(\\partial y/\\partial \\mathbf{x}=[4/3,2,8/3]\\). The previous code cell should have printed the same result.\n\n\n\n\ngpu_avail = torch.cuda.is_available()\nprint(f\"Is the GPU available? {gpu_avail}\")\n\nIs the GPU available? True\n\n\n\n# Create a object device which will be assigned to GPU if available otherwise to cpu\ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\nprint(\"Device\",device)\n\nDevice cuda\n\n\n\nx = torch.zeros(2,3)\nx = x.to(device)\nprint(x)\n\ntensor([[0., 0., 0.],\n        [0., 0., 0.]], device='cuda:0')\n\n\n\n# comparing cpu vs gpu execution\nx = torch.randn(5000, 5000)\n\n## CPU version\nstart_time = time.time()\n_ = torch.matmul(x, x)\nend_time = time.time()\nprint(f\"CPU time: {(end_time - start_time):6.5f}s\")\n\n## GPU version\nx = x.to(device)\n_ = torch.matmul(x, x)  # First operation to 'burn in' GPU\n# CUDA is asynchronous, so we need to use different timing functions\nstart = torch.cuda.Event(enable_timing=True)\nend = torch.cuda.Event(enable_timing=True)\nstart.record()\n_ = torch.matmul(x, x)\nend.record()\ntorch.cuda.synchronize()  # Waits for everything to finish running on the GPU\nprint(f\"GPU time: {0.001 * start.elapsed_time(end):6.5f}s\")  # Milliseconds to seconds\n\nCPU time: 0.40145s\nGPU time: 0.07050s\n\n\n\n# The seed between CPU and GPU is not synchronized.\n# we need to set the seed on GPU separately\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(42)\n    torch.cuda.manual_seed_all(42)\n\n# Additionally, some operations on a GPU are implemented stochastic for efficiency\n# We want to ensure that all operations are deterministic on GPU (if used) for reproducibility\ntorch.backends.cudnn.deterministic = True\n# benchmark is to find the best algorithm to use the hardware when input does not vary\ntorch.backends.cudnn.benchmark = False"
  },
  {
    "objectID": "pytorch/02_continous_xor.html",
    "href": "pytorch/02_continous_xor.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "import os\nimport math\nimport numpy as np\nimport time\n\n\n# libraries for plotting\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom IPython.display import set_matplotlib_formats\nset_matplotlib_formats('svg','pdf')\nfrom matplotlib.colors import to_rgba\nimport seaborn as sns\nsns.set()\nfrom tqdm.notebook import tqdm\n\n/tmp/ipykernel_69450/3532747965.py:5: DeprecationWarning: `set_matplotlib_formats` is deprecated since IPython 7.23, directly use `matplotlib_inline.backend_inline.set_matplotlib_formats()`\n  set_matplotlib_formats('svg','pdf')\n\n\n\n# Import and check torch version\nimport torch\nprint(\"using torch\", torch.__version__)\n\nusing torch 1.12.0+cu113\n\n\n\n# Set seed for reproducability\ntorch.manual_seed(42)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(42)\n    torch.cuda.manual_seed_all(42)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False  \n\n\n# `torch.nn` contains classes for building neural networks. This is implemented as modules\n# 'torch.nn.functional` are implemented as functions. torch.nn uses functionalities from torch.nn.functional\n\n\n\n\n\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\n# In pytorch, a neural network is built up of modules\n# Modules can contain other modules\n# A neural network is considered a module in itself\n\nclass MyModule(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, x):\n        # calculate the forward pass\n        pass\n\nWe will train a simple neural network to create a XOR gate. The output of XOR will be true if either one of the inputs is true, otherwise it will be false. We will add noise to the input to make it harder to learn. The model we will be creating is visualized below. * \n\nclass SimpleClassifier(nn.Module):\n    def __init__(self, num_inputs,num_hidden,num_outputs):\n        super().__init__()\n        self.linear1 = nn.Linear(num_inputs,num_hidden)\n        self.act_fn = nn.Tanh()\n        self.linear2 = nn.Linear(num_hidden,num_outputs)\n\n    def forward(self,x):\n        x = self.linear1(x)\n        x = self.act_fn(x)\n        x = self.linear2(x)\n        return x\n\n\nmodel = SimpleClassifier(num_inputs=2,num_hidden=4,num_outputs=1)\nprint(model)\n\nSimpleClassifier(\n  (linear1): Linear(in_features=2, out_features=4, bias=True)\n  (act_fn): Tanh()\n  (linear2): Linear(in_features=4, out_features=1, bias=True)\n)\n\n\n\n# Print the names and shape of the parameters\nfor name, param in model.named_parameters():\n    print(f\"Parameter:{name}, shape: {param.shape}\")\n\nParameter:linear1.weight, shape: torch.Size([4, 2])\nParameter:linear1.bias, shape: torch.Size([4])\nParameter:linear2.weight, shape: torch.Size([1, 4])\nParameter:linear2.bias, shape: torch.Size([1])\n\n\nEach linear layer will have the weight matrix of shape [output, input], and a bias of shape [output]\n\n\n\ntorch.utils.data provides utilites for handling data efficiently. data.Dataset class is the interface to access training/test data. data.DataLoader class is to efficiently prepare batches from dataset\nTo define a dataset class, we need to specify two functions __getitem__ and __len__. The get-item function will return the ith data point in a dataset. The len function will return the size of the dataset\n\nimport torch.utils.data as data\n\n\n\n\nclass XORDataset(data.Dataset):\n    \"\"\"\n        Inputs:\n            size - Number of data points we want to generate\n            std - Standard deviation of the noise (see generate_continuous_xor function)\n        \"\"\"\n    def __init__(self, size, std=0.1):\n        super().__init__()\n        self.size = size\n        self.std = std\n        self.generate_continuous_xor()\n\n    def generate_continuous_xor(self):\n        data = torch.randint(low=0, high=2, size=(self.size, 2), dtype=torch.float32)\n        label = (data.sum(dim=1)==1).to(torch.long)\n        data += self.std * torch.randn(data.shape)\n        self.data = data\n        self.label = label\n\n    def __len__(self):\n        return self.data.shape[0]\n\n    def __getitem__(self, idx):\n        data_point = self.data[idx]\n        data_label = self.label[idx]\n        return data_point, data_label\n\n\ndataset = XORDataset(size=200, std=0.1)\nprint(\"size of the dataset:\", len(dataset))\nprint(\"Data Point 0:\", dataset[0])\n\nsize of the dataset: 200\nData Point 0: (tensor([0.8675, 0.9484]), tensor(0))\n\n\n\ndef visualize_samples(data,label):\n    if isinstance(data, torch.Tensor):\n        data = data.cpu().numpy()\n    if isinstance(label, torch.Tensor):\n        label = label.cpu().numpy()\n\n    data_0 = data[label==0]\n    data_1 = data[label==1]\n\n    plt.figure(figsize=(4,4))\n    plt.scatter(data_0[:,0],data_0[:,1],label=\"class 0\",edgecolor=\"#333\")\n    plt.scatter(data_1[:,0],data_1[:,1],label=\"class 1\",edgecolor=\"#333\")\n    plt.title(\"Dataset samples\")\n    plt.ylabel(\"$x_2$\")\n    plt.xlabel(\"$x_1$\")\n    plt.legend()\n\n\nvisualize_samples(dataset.data, dataset.label)\n\n\n\n\n\n\n\ntorch.utils.DataLoader class provides a python iterable over a dataset with support for batching, multi-process data loading and many more features.\n\ndata_loader = data.DataLoader(dataset, batch_size=8, shuffle=True)\n\n\n# First batch of the data loader\ndata_inputs, data_labels = next(iter(data_loader))\n# The shape of output are [batch_size,dimensions_of_input]\nprint(\"Data inputs\", data_inputs.shape,\"\\n\", data_inputs)\nprint(\"Data labels\", data_labels.shape,\"\\n\", data_labels)\n\nData inputs torch.Size([8, 2]) \n tensor([[ 1.1953,  0.2049],\n        [-0.1459,  0.8506],\n        [-0.1253,  0.1119],\n        [ 0.0531, -0.1361],\n        [ 0.1345,  0.0127],\n        [-0.1449,  0.9395],\n        [ 1.0506,  0.9082],\n        [ 1.0080,  0.0745]])\nData labels torch.Size([8]) \n tensor([1, 1, 0, 0, 0, 1, 0, 1])\n\n\n\n\n\n\n\n\n\nnn.BCEWithLogitsLoss - BCE using logits - More stable\nnn.BCELoss - BCE using labels (afer applying sigmoid) - Not stable \\[\\mathcal{L}_{BCE} = -\\sum_i \\left[ y_i \\log x_i + (1 - y_i) \\log (1 - x_i) \\right]\\]\n\n\nloss_module = nn.BCEWithLogitsLoss()\n\ntorch.optim consists of popular optimizers. stochastic Gradient Descent updates parameters by multiplying the gradients with a small constant, called learning rate, and subtracting them from the parameters.\n\n# Input to the optimizer are the parameters of the model\noptimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n\n\noptimizer.step() updates the parameters based on the gradients\noptimizer.zero_grad() resets the gradients to zero (otherwise the gradients will be added to the previous ones)\n\n\n\n\n\n\ntrain_dataset = XORDataset(size=2500)\ntrain_data_loader = data.DataLoader(train_dataset,batch_size=128,shuffle=True)\n\n\ndevice = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n\n\nmodel.to(device)\n\nSimpleClassifier(\n  (linear1): Linear(in_features=2, out_features=4, bias=True)\n  (act_fn): Tanh()\n  (linear2): Linear(in_features=4, out_features=1, bias=True)\n)\n\n\n\ndef train_model(model, optimizer,data_loader,loss_module, num_epochs=100):\n    # set model to train mode\n    model.train()\n\n    # Training loop\n    for epoch in tqdm(range(num_epochs)):\n        for data_inputs, data_labels in data_loader:\n\n            #step 1: Move input data to GPU\n            data_inputs = data_inputs.to(device)\n            data_labels = data_labels.to(device)\n\n            #step 2: Forward pass\n            preds = model(data_inputs)\n            # output is [Batch size, 1] but we want [Batch size]\n            preds = preds.squeeze(dim=1)  \n\n            #step 3: Calculate the loss\n            loss = loss_module(preds, data_labels.float())\n\n            #step 4: Backpropagate\n            # Ensure all the gradients are zero\n            # otherwise they will be added to the existing ones\n            optimizer.zero_grad()\n            # Perform backpropagation\n            loss.backward()\n\n            #step 5: Update the parameters\n            optimizer.step()\n\n\ntrain_model(model, optimizer, train_data_loader, loss_module)\n\n\n\n\n\n\n\nstate_dict contains the parameters of the model.\n\nstate_dict = model.state_dict()\nprint(state_dict)\n\nOrderedDict([('linear1.weight', tensor([[ 2.2680,  2.2159],\n        [-3.4127,  2.4905],\n        [-0.2947, -0.1366],\n        [-2.2528,  3.2508]], device='cuda:0')), ('linear1.bias', tensor([-0.4080, -0.9470,  0.7039,  0.8016], device='cuda:0')), ('linear2.weight', tensor([[ 3.2733,  4.3679,  0.5540, -4.3394]], device='cuda:0')), ('linear2.bias', tensor([1.0851], device='cuda:0'))])\n\n\n\ntorch.save(state_dict,\"our_model.tar\")\n\n\n# Load the model\nstate_dict = torch.load(\"our_model.tar\")\n\n# create a new model and load the state\nnew_model = SimpleClassifier(num_inputs=2,num_hidden=4,num_outputs=1)\nnew_model.load_state_dict(state_dict)\n\n# verify if the parameters are same\nprint(\"original model\\n\", model.state_dict())\nprint(\"\\nnew model\\n\", new_model.state_dict())\n\noriginal model\n OrderedDict([('linear1.weight', tensor([[ 2.2680,  2.2159],\n        [-3.4127,  2.4905],\n        [-0.2947, -0.1366],\n        [-2.2528,  3.2508]], device='cuda:0')), ('linear1.bias', tensor([-0.4080, -0.9470,  0.7039,  0.8016], device='cuda:0')), ('linear2.weight', tensor([[ 3.2733,  4.3679,  0.5540, -4.3394]], device='cuda:0')), ('linear2.bias', tensor([1.0851], device='cuda:0'))])\n\nnew model\n OrderedDict([('linear1.weight', tensor([[ 2.2680,  2.2159],\n        [-3.4127,  2.4905],\n        [-0.2947, -0.1366],\n        [-2.2528,  3.2508]])), ('linear1.bias', tensor([-0.4080, -0.9470,  0.7039,  0.8016])), ('linear2.weight', tensor([[ 3.2733,  4.3679,  0.5540, -4.3394]])), ('linear2.bias', tensor([1.0851]))])\n\n\n\n\n\n\ntest_dataset = XORDataset(size=500)\n# drop_last -> Don't drop the last batch although it is smaller than 128\ntest_data_loader = data.DataLoader(test_dataset, batch_size=128, shuffle=False, drop_last=False)\n\n\ndef eval_model(model, data_loader):\n    # set model to eval mode\n    model.eval()\n    true_preds, num_preds = 0., 0.\n\n    # No need to calculate gradients for the test data\n    with torch.no_grad():\n        for data_inputs, data_labels in data_loader:\n            data_inputs, data_labels = data_inputs.to(device), data_labels.to(device)\n            preds = model(data_inputs)\n            preds = preds.squeeze(dim=1)\n            preds = torch.sigmoid(preds)\n            preds_labels = (preds > 0.5).long()\n\n            true_preds += (preds_labels == data_labels).sum()\n            num_preds += data_labels.shape[0]\n\n    accuracy = true_preds / num_preds\n    print(f\"Accuracy: {100.0*accuracy:4.2f}%\")\n\n\neval_model(model,test_data_loader)\n\nAccuracy: 100.00%\n\n\n\n\n\n\n@torch.no_grad() # Decorator, same effect as \"with torch.no_grad(): ...\" over the whole function.\ndef visualize_classification(model, data, label):\n    if isinstance(data, torch.Tensor):\n        data = data.cpu().numpy()\n    if isinstance(label, torch.Tensor):\n        label = label.cpu().numpy()\n    data_0 = data[label == 0]\n    data_1 = data[label == 1]\n\n    fig = plt.figure(figsize=(4,4), dpi=500)\n    plt.scatter(data_0[:,0], data_0[:,1], edgecolor=\"#333\", label=\"Class 0\")\n    plt.scatter(data_1[:,0], data_1[:,1], edgecolor=\"#333\", label=\"Class 1\")\n    plt.title(\"Dataset samples\")\n    plt.ylabel(r\"$x_2$\")\n    plt.xlabel(r\"$x_1$\")\n    plt.legend()\n\n    # Let's make use of a lot of operations we have learned above\n    model.to(device)\n    c0 = torch.Tensor(to_rgba(\"C0\")).to(device)\n    c1 = torch.Tensor(to_rgba(\"C1\")).to(device)\n    x1 = torch.arange(-0.5, 1.5, step=0.01, device=device)\n    x2 = torch.arange(-0.5, 1.5, step=0.01, device=device)\n    xx1, xx2 = torch.meshgrid(x1, x2, indexing='ij')  # Meshgrid function as in numpy\n    model_inputs = torch.stack([xx1, xx2], dim=-1)\n    preds = model(model_inputs)\n    preds = torch.sigmoid(preds)\n    output_image = (1 - preds) * c0[None,None] + preds * c1[None,None]  # Specifying \"None\" in a dimension creates a new one\n    output_image = output_image.cpu().numpy()  # Convert to numpy array. This only works for tensors on CPU, hence first push to CPU\n    plt.imshow(output_image, origin='lower', extent=(-0.5, 1.5, -0.5, 1.5))\n    plt.grid(False)\n    return fig\n\n_ = visualize_classification(model, dataset.data, dataset.label)\nplt.show()\n\n\n\n\n\n\n\n\nfrom torch.utils.tensorboard import SummaryWriter\n%load_ext tensorboard\n\n\ndef train_model_with_logger(model, optimizer, data_loader, loss_module, val_dataset, num_epochs=100,logging_dir = 'runs/our_experiment'):\n    writer = SummaryWriter(logging_dir)\n    model_plotted = False\n\n    # set the model to train mode\n    model.train()\n\n    # Training loop\n    for epoch in tqdm(range(num_epochs)):\n        epoch_loss = 0.0\n        for data_inputs, data_labels in data_loader:\n\n            # step 1: Move the data to device\n            data_inputs = data_inputs.to(device)\n            data_labels = data_labels.to(device)\n\n            # For the first batch, we visualize the computation graph in TensorBoard\n            if not model_plotted:\n                writer.add_graph(model, data_inputs)\n                model_plotted = True\n\n            # step 2: Forward pass\n            preds = model(data_inputs)\n            # output is [Batch size, 1] but we want [Batch size]\n            preds = preds.squeeze(dim=1)\n\n            # step 3: Calculate the loss\n            loss = loss_module(preds, data_labels.float())\n\n            # step 4: Backpropagate\n            # Ensure all the gradients are zero\n            # otherwise they will be added to the existing ones\n            optimizer.zero_grad()\n            # Perform backpropagation\n            loss.backward()\n\n            # step 5: Update the parameters\n            optimizer.step()\n\n            # step 6: Take the running average of the loss\n            epoch_loss += loss.item()\n\n    # step 7: Add average loss to TensorBoard\n    epoch_loss /= len(data_loader)\n\n    writer.add_scalar('training_loss', epoch_loss, \n                      global_step = epoch + 1)\n\n    # Visualize prediction and add figure to TensorBoard\n    # Since matplotlib figures can be slow in rendering, we only do it every 10th epoch\n\n    if (epoch + 1) % 10 == 0:\n       fig = visualize_classification(model, val_dataset.data,val_dataset.label)\n       writer.add_figure('predictions',fig, global_step = epoch + 1)\n\n    writer.close()\n\n\nmodel = SimpleClassifier(num_inputs=2, num_hidden=4, num_outputs=1).to(device)\noptimizer = torch.optim.SGD(model.parameters(), lr=0.1)\ntrain_model_with_logger(model, optimizer, train_data_loader, loss_module, val_dataset=test_dataset)\n\n\n\n\n\n%tensorboard --logdir runs/our_experiment"
  },
  {
    "objectID": "about_me/my_portfolio.html",
    "href": "about_me/my_portfolio.html",
    "title": "Project Portfolio",
    "section": "",
    "text": "Why\n\nAt an OTT company, on average 20% of paid subscribers were churning every month and the company wanted to reduce the churn and retain the customers.\n\nRole\n\nAs a Data Science scientist, my target was to reduce the churn of customers from 20% to 17% in six months.\n\nAction\n\nWe started with talking with different teams and understanding their perspective on churn, gathering as much domain knowledge as possible.\nAnalyzing the patterns of churn and searching for the various data sources available. Talking to different data owners and understanding the data\n\nExploratory data analysis\nFeature engineering\nCreating a model to predict churn\nEvaluating results\nModel deployment\nMeasuring the impact of the predictions and making changes for improvement\n\nResult\n\nWe are able to reduce churn by 4%, there by increasing the customer rentention and revenue for the organisation.\n\nTools Used\n\nAWS S3, Athena, AWS Glue, EC2, ECR\nDatabricks\nSisense / Tableau for Dashboards\n\n\n\n\nwhy\n\nAt a social media company, we wanted to keep the platform clean and safe to increase the satisfaction of all our customers. We wanted to increase their enjoyment by recommending videos which users will prefer and are difficult to discover on the platform due to plethora of choices.\n\nRole\n\nAs a Data Science manager, my target was to blacklist the NSFW content and reduce the workload of manual moderators by 60%. Tag the videos and improve the number of video views and watch time by 5%\n\nAction\n\nBrainstorming what is meant by NSFW, reading the content moderation guidelines\nUnderstanding what should not be published on the platform.\nWe created computer vision models and also used open source models to detect NSFW and category of the content.\n\nEvaluate and deploy the models\nUsed an open source tool Fiftyone to automate samping videos, getting the ground truth, calculating the ML metrics and publishing on the dashboard.\n\n\n\n\nVisualizing Results in Fiftyone\n\n\nResult\n\nWe reduced the effort of moderation and tagging by more than 80%. Increased the watchtime and video views on average by 3%\n\nTools Used\n\nSnowflake, EC2, Ray Serve, Grafana, Fiftyone\n\n\n\n\nWhy\n\nAt an Adtech organization, partner companies have the data for survey participants. The number of survey participants are very less. Find the customers in our company’s database who are similar to survery participants for ad targeting and monetisation.\n\nRole\n\nAs a Data science manager, develop a framework for data integration with partners and automation the pipeline for building look-alike models, segment the customers and send the data for campaigning.\n\nAction\n\nCo-ordination with different teams for data integration\nDesigning a framework for data ingestion, feature engineering, model creation, segmenting the customers and evaluation of results\n\nResult\n\nCreating a framework to automate data ingestion, creation of hundereds of look-alike models and segmenting the customers and deploying them. Using the framework for integration with different partners. Increasing the campaign revenue by more than 2%. The complete deployment was automated using Mlops\n\nTools Used\n\nAWS S3, Glue, Sagemaker, EMR, EC2, Mlflow\nSnowflake\nAirflow\nApache Superset for Dashboarding\n\n\n\n\nWhy\n\nAt an Edtech organization, students enrollment for a course is high, if he is informed in advance the placement outcomes of the course.\n\nRole\n\nAs a Data scientist, I need to create a model which will help increase the enrollment by 10%\n\nAction\n\nCollecting the data on student’s past educational performance, work experience etc.\nCreating and evaluating the model\n\nResult\n\nStudent enrollment increased 12% when they were informed in advance their placement outcomes\n\nTools Used\n\nAWS & Streamlit\n\n\n\n\nWhy\n\nAt a social media company, enabling content discovery and providing informed choices for content consumption as per the user preferences\n\nRole\n\nAs a Data Science manager, my role is to design a framework for recommendation engines, executing the project and evaluating the results. Recommendation engines should increase the video views and watch time by 5%\n\nAction\n\nDesigning a recommendation system based on candidate retrieval and ranking system\nCreating the pipelines for data processing\nExecution of candidte retrieval using elasticsearch\nModeling and ranking the videos for the user\nEvaluation of the recommendation system\n\nResult\n\nWe are currently deployment stage\n\nTools Used\n\nElasticsearch, FastAPI, AWS, Snowflake"
  },
  {
    "objectID": "about_me/my_portfolio.html#distributed-computing-data-cleaning-and-data-quality",
    "href": "about_me/my_portfolio.html#distributed-computing-data-cleaning-and-data-quality",
    "title": "Project Portfolio",
    "section": "Distributed computing, Data cleaning and Data Quality",
    "text": "Distributed computing, Data cleaning and Data Quality\n\nBuilding Datalake\nwhy\n\nAt an OTT company, data is generated from numerous tools which is used by different departments in an organisation. Accessing data generated by different tools was becoming difficult and different numbers were being reported at different levels.\n\nRole\n\nAs a Data Science manager my role was to create a single point of truth for the data. Automate the data processing requirements of the organization by 50% and reduce the infrastructure cost by 30%\n\nAction\n\nCollecting all the data generated by different tools at a central location.\nDesigning a framework for automation, cleaning and processing of the data.\nScheduling of workflows and creating alerts for failures\nCreating different marts for the data\n\nBronze tables - Cleaned and transformed data\nSilver tables - Joining different tables, calculating and storing the features required for analysis and different machine learning models\nGold tables - Calculating and storing metrics for business reporting and dashboards\n\nSetting up of expectations for data quality, monitoring of validations and alerts in case of any discrepancy.\n\n\n\n\nEnsuring Data Quality with Great Expectations\n\n\nResult\n\nAchieved more than 50% of organisations data processing requirements and automation using Data engineering and MLOps\n\nTools Used\n\nAWS S3, EMR, Athena, Glue, Databricks, Pyspark, Hive metastore, Airflow etc\nGreat Expectations for monitoring Data quality\nDashboards for monitoring the data processed and results of quality checks\n\n\n\nData Discovery\nwhy\n\nData Scientists in the organization was facing challenges to know about existing data sources, understanding their definition and meaning. Knowledge existed in silos as the information generated by one data scientist was not shared with others\n\nRole\n\nTo create a interactive platform where information about data sources are searchable, definitions of features available. Creating a knowledge repository to share all the analysis and reports.\n\nAction\n\nSearch for a data discovery platform which can easily integrate with AWS\nStarting and executing a POC for a data discovery platform\nProviding access to the platform to all data scientists in the organization and collecting feedback.\nImplementation in production\n\nResult\n\nIncrease in work satisfaction and productivity of all the data scientists in the organization.\n\n\n\n\nData discovery using Quiltdata\n\n\nTools Used\n\nQuilt, Elasticsearch"
  },
  {
    "objectID": "about_me/my_portfolio.html#visualization",
    "href": "about_me/my_portfolio.html#visualization",
    "title": "Project Portfolio",
    "section": "Visualization",
    "text": "Visualization\n\nBuilding dashboards\nWhy\nAt an OTT company, Business teams wanted to see a few metrics with dynamic date ranges which were extremely difficult to show on the dashboards. An example, unique users on the platform with a dynamic data range.\nRole\nDo a POC on how to show these metrics on the dashboard which can be rendered very fast and are computationally cheap.\nAction\n\nConducting a study on how to process the data at scale to calculate these metrics and provide the information to the dashboard quickly\nReading about sketching algorithms which can provide approximate results very quickly\nUndertaking a POC\nCreating a dashboard and presenting the results\n\nResult\n\nAble to provide metrics required by the business team on the dashboard for a dynamic date range\n\n\n\n\nCreating Dashboard on raw data using Dremio\n\n\nTools Used\n\nDremio, Druid, Apache Superset"
  },
  {
    "objectID": "about_me/why_me.html",
    "href": "about_me/why_me.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "The International Social Service (ISS) is an international NGO founded in 1924 which assists children and families confronted with complex social problems as a result of migration. This project was undertaken in collaboration with 30 data scientists around the world.\n\n\n\n\nA Goodwill project I did for The International Social Service\n\n\n\n\n\n\n\n\nI am a team player\n\n\n\nAn example - I initiated and built a data discovery platform understanding the difficulties faced by other data scientists in the team\nI initiated and built a automated system for video moderation and tagging after knowing about the challenges faced by a different team in the company\n\n\n\n\n\nAfter serving in military for 20 years, I pivoted to a corporate environment and a different industry\nI love making small upgrades on a daily basis\n\n\n\nI upgrade my version everyday\n\n\n\n\n\n\n\nBooks on ML I read the previous year \nCourse I am doing currently"
  },
  {
    "objectID": "causal_inference/resources.html",
    "href": "causal_inference/resources.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "causal Inference Book\nBrady Neal course website\nBrady Neal causal Book\nBrady Neal youtube course\nJoshua Angrist videos\nwhat If - Book\nwhat If Python code\nMixtape - Book\nstatistical Rethinking book code in python and pymc"
  },
  {
    "objectID": "causal_inference/resources.html#packages",
    "href": "causal_inference/resources.html#packages",
    "title": "My Datascience Journey",
    "section": "Packages",
    "text": "Packages\n\nCausalinference\nCausallib\nCausalimpact\nDoWhy\nEconml"
  },
  {
    "objectID": "causal_inference/03_stats_revision.html",
    "href": "causal_inference/03_stats_revision.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Stats Revisited\n\nWith standard error, we can create an interval that will contain the true mean 95% of the time (error is for estimating true mean from the mean of the means of experiment).\nConfidence interval is calculated when we don’t have the luxury of simulating the same experiment with multiple datasets.To calculate confidence interval, we use the central limit theorem.As 95% of the mass of normal distribution is between 1.96 (close to 2) standard errors, we multiply standard error by 2 and add and subtract it from the mean of one of our experiments, we will construct a 95% confidence interval for the true mean. (In practice we multiple by Z which is a cumulative density function instead of 2). If sample size is small, the larger the standard error and wider the confidence interval.\nThe sum or difference of 2 independent normal distributions is also normal distribution. The resulting mean will be the sum or difference between the two distributions, while the variance will always be the sum of the variance. \nP-values - It measures how unlikely is the measurement given that null hypothesis is true. p-values is P(data|Null Hypothesis is True) \n\n\nCode to do AB Testing in python\ndef AB_test(test: pd.Series, control: pd.Series, confidence=0.95, h0=0):\n    mu1, mu2 = test.mean(), control.mean()\n    se1, se2 = test.std() / np.sqrt(len(test)), control.std() / np.sqrt(len(control))\n    \n    diff = mu1 - mu2\n    se_diff = np.sqrt(test.var()/len(test) + control.var()/len(control))\n    \n    z_stats = (diff-h0)/se_diff\n    p_value = stats.norm.cdf(z_stats)\n    \n    def critial(se): return -se*stats.norm.ppf((1 - confidence)/2)\n    \n    print(f\"Test {confidence*100}% CI: {mu1} +- {critial(se1)}\")\n    print(f\"Control {confidence*100}% CI: {mu2} +- {critial(se2)}\")\n    print(f\"Test-Control {confidence*100}% CI: {diff} +- {critial(se_diff)}\")\n    print(f\"Z Statistic {z_stats}\")\n    print(f\"P-Value {p_value}\")"
  },
  {
    "objectID": "causal_inference/01_intro.html",
    "href": "causal_inference/01_intro.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Average Treatment Effect and Average Treatment Effect on the treated\n\n\n\n\n\nAssociation Vs ATT\n\n\nIn the above equation the first term is association. If there is no Bias, then association will be causation. Bias is given by how treated and control group differ before the treatment, in case neither of them has received the treatment.\nCausal Inference is all about finding clever ways to removing bias and making the treated and untreated comparable so that all the difference we see is only the average treatment effect.\n\n\n\nIt happens because of unequal weights given to different conditions for different treatments. If we stratify the condition then we will know the causal impact\nCorrleation is not causation becuase correlation = confounding effect + causal effect."
  },
  {
    "objectID": "causal_inference/02_randomised_exp.html",
    "href": "causal_inference/02_randomised_exp.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Randomised Experiments\n\nRandomised experiments can make bias vanish\nThey tend to be very expensive or plain unethical. Sometimes it is not practical to control the assignment.\nRandomised experiments are the simplest and most effective way to uncover causal impact. It does this by making the treatment and control groups comparable."
  },
  {
    "objectID": "causal_inference/05_potential_outcomes.html",
    "href": "causal_inference/05_potential_outcomes.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Intro - Potential Outcomes"
  },
  {
    "objectID": "causal_inference/04_graphical_causal_models.html",
    "href": "causal_inference/04_graphical_causal_models.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Graphical Causal Models\n\nMy understanding of conditioning in causality is filtering on a value of the variable. \n\n\nDirect path\n\n\n\nDirect Path\n\n\nIf we are conditioning on the intermediary variable i,e problem solving, then the dependence between causal knowledge and promotion is broken - They become independent.\n\n\n\nDirect Path Rule\n\n\n\n\nFork\nIn the case of a fork, the dependence flows backward through the arrows, and we have a Backdoor path\n\n\n\nFork\n\n\nWe can close the backdoor path and shut down dependence by conditioning on the common cause. Two variables that share a common cause are dependent, but independent when we condition on the common cause.\n\n\n\nFork Rule\n\n\n\n\nCollider\n\nA collider is when two arrows collide on a single variable. Both the causes share a common effect.\n\n\n\n\ncollider\n\n\nConditioning on the collider opens the dependence path. Not conditioning on it leaves it closed.\n\n\n\ncollider rule\n\n\n\nBased on the above, we can have a more general rule. A path is blocked if and only if:\n\nIt contains a non-collider that has been conditioned on\nIt contains a collider and that has not been conditioned on and has no descendants that have been conditioned on\n\n\n\n\n\ncausality rules\n\n\n\n\nDescendent\n\n\n\nDescendent\n\n\n\nExcercises for causal rules\n \n\n\nImportant points\n\n\n\nUnderstanding the causal effects\n\n\n\n\n\nConfounding\nIt is caused when the treatment and the outcome share a common cause. If we close all the backdoor paths between the treatment and the outcome, we can identify the causal effect.\n\n\n\nconfounding Bias\n\n\nTo fix confounding bias, we need to control all common causes of the treatment and the outcome\nSometimes confounders are not measurable. we may have other measured variables that can act as a proxy for the confounder. controlling for them will help lower the bias. Those variables are sometimes referred to as surrogate confounders. Controlling for the surrogate variables is not sufficient to eliminate bias, but it helps\n\n\nSelection Bias\nSelection bias arises when we control for more variables than we should. It might be the case that the treatment and the potential outcome are marginally independent but become dependent once we condition on a collider.\nSelection bias can also happen due to excessive controlling of mediator variables. This excessive controlling could lead to bias even if the treatment was randomly assigned.\nSelection bias can often be fixed by simply doing nothing, which is why it is dangerous. Since we are biased toward action, we tend to see ideas that control things as clever when they can be doing more harm than good."
  },
  {
    "objectID": "blogs/ideas.html",
    "href": "blogs/ideas.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "How stable diffusion works\nHow to train stable diffusion on your own data\nWhat is textual inversion fine-tuning for stable diffusion\nComputer vision use case for the construction industry\nDeep learning architectures for computer vision\nTransformers for computer vision\nTransformers Vs Convolutions for computer vision\nSegmentation architectures for computer vision\nImage classification architectures for computer vision\nObject detection architectures for computer vision\nGenerative models for computer vision\nHow reverse image search works\nHow computer vision is used in Insurance sector"
  },
  {
    "objectID": "blogs/ideas.html#nlp",
    "href": "blogs/ideas.html#nlp",
    "title": "My Datascience Journey",
    "section": "NLP",
    "text": "NLP\n\nHow Question Answering NLP systems work\nHow NLP is used for Summarization\nHOW Named Entity Recognition Models are trained\nHow Keyword Extraction models are trained using NLP\nPopular Transformer Architectures for NLP\nHow to use NLP for cleaning social media data\nHow open domain chatbots like chatgpt are trained\nHow Rasa trains their models for intent detection and predicting next action\nHow to create your own LLM powered applications"
  },
  {
    "objectID": "blogs/ideas.html#recommendation-systems",
    "href": "blogs/ideas.html#recommendation-systems",
    "title": "My Datascience Journey",
    "section": "Recommendation Systems",
    "text": "Recommendation Systems\n\nHow Deep Knowledge-aware Networks (DKN) is used for News Recommendation System\nHow Tiktok recommenders work\nHow Neural Collobarative Filtering (NCF) works\nAlternative Least Square (ALS) for recommender systems\nAutoencoders for Recommender sytems\nBuilding Graph Based Recommender systems\nPopular architectures for recommender systems\nWhat is Matrix Factorization? How is it used in recommendation systems?\nwhat are session based recommendation systems and how do they work?"
  },
  {
    "objectID": "blogs/ideas.html#data-engineering",
    "href": "blogs/ideas.html#data-engineering",
    "title": "My Datascience Journey",
    "section": "Data Engineering",
    "text": "Data Engineering\n\nData engineering life cycle\nHow to design good data architecture\nwhat is a data mesh\nData pipeline patterns\nHow to orchestrate your data pipelines\nCreating datapiplines in Jupyter\nwhat is Fugue - Distributed compute engine"
  },
  {
    "objectID": "blogs/ideas.html#python",
    "href": "blogs/ideas.html#python",
    "title": "My Datascience Journey",
    "section": "Python",
    "text": "Python\n\nPandas 2.0 release is expected soon. Blog post on the advantages of Pandas 2.0\nUsing Python dictionaries effectively\nUsing Decorators for Data Science\nGenerators and Iterators"
  },
  {
    "objectID": "blogs/ideas.html#data-science-machine-learning-and-deep-learning-concepts-demystified",
    "href": "blogs/ideas.html#data-science-machine-learning-and-deep-learning-concepts-demystified",
    "title": "My Datascience Journey",
    "section": "Data Science, Machine Learning and Deep Learning Concepts Demystified",
    "text": "Data Science, Machine Learning and Deep Learning Concepts Demystified\nExplaining the Jargon which we come across in our day to day life. For example:- * What is Reinforcement Learning with Human Feedback * What is label smoothing * What is KL divergence"
  },
  {
    "objectID": "blogs/ideas.html#content-publishing",
    "href": "blogs/ideas.html#content-publishing",
    "title": "My Datascience Journey",
    "section": "Content Publishing",
    "text": "Content Publishing\n\nHow to create your Data Science Portfolio\nHow to create your personal website without any coding skills\nHow to write and maintain your own Data Science blog"
  },
  {
    "objectID": "blogs/rlhf.html",
    "href": "blogs/rlhf.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Loss functions for ethical, safe?\nTo encode these values into the model\n\n\n\nRHFL for decision making\n\n\nscaler reward\n\n\n\n\nIt came in 2017\n\npolicy gradient methods\n\n\n\nPPO"
  },
  {
    "objectID": "personal/todo.html",
    "href": "personal/todo.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Call with Ujjyaini and Anish\nCall with steven\nPrepare for tomorrow - Packing and other things required\nWithdraw money\nPre-schedule the cab\nWeb-check in\nGet school information from neel"
  },
  {
    "objectID": "telecom_churn_prediction/telecom_churn_prediction.html",
    "href": "telecom_churn_prediction/telecom_churn_prediction.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Telecom Churn Prediction\nThe objectives of this project are:-\n1. Perform exploratory analysis and extract insights from the dataset.\n2. Split the dataset into train/test sets and explain your reasoning.\n3. Build a predictive model to predict which customers are going to churn and discuss the reason why you choose a particular algorithm.\n4. Establish metrics to evaluate model performance.\n5. Discuss the potential issues with deploying the model into production\n\nImport the required libraries\n\n# python version # 3.8.2\nimport pandas as pd \nimport numpy as np \nfrom pandas_profiling import ProfileReport \nfrom pycaret.classification import * \nfrom sklearn import metrics \nimport os \nfrom sklearn.model_selection import train_test_split \n\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n# option to display all columns\npd.set_option('display.max_columns', None)\n\n\n# Read the data\ntelecom_churn = pd.read_csv('data science challenge.csv')\n\n\ntelecom_churn.head(10)\n\n\n\n\n\n  \n    \n      \n      state\n      account length\n      area code\n      phone number\n      international plan\n      voice mail plan\n      number vmail messages\n      total day minutes\n      total day calls\n      total day charge\n      total eve minutes\n      total eve calls\n      total eve charge\n      total night minutes\n      total night calls\n      total night charge\n      total intl minutes\n      total intl calls\n      total intl charge\n      customer service calls\n      churn\n    \n  \n  \n    \n      0\n      KS\n      128\n      415\n      382-4657\n      no\n      yes\n      25\n      265.1\n      110\n      45.07\n      197.4\n      99\n      16.78\n      244.7\n      91\n      11.01\n      10.0\n      3\n      2.70\n      1\n      False\n    \n    \n      1\n      OH\n      107\n      415\n      371-7191\n      no\n      yes\n      26\n      161.6\n      123\n      27.47\n      195.5\n      103\n      16.62\n      254.4\n      103\n      11.45\n      13.7\n      3\n      3.70\n      1\n      False\n    \n    \n      2\n      NJ\n      137\n      415\n      358-1921\n      no\n      no\n      0\n      243.4\n      114\n      41.38\n      121.2\n      110\n      10.30\n      162.6\n      104\n      7.32\n      12.2\n      5\n      3.29\n      0\n      False\n    \n    \n      3\n      OH\n      84\n      408\n      375-9999\n      yes\n      no\n      0\n      299.4\n      71\n      50.90\n      61.9\n      88\n      5.26\n      196.9\n      89\n      8.86\n      6.6\n      7\n      1.78\n      2\n      False\n    \n    \n      4\n      OK\n      75\n      415\n      330-6626\n      yes\n      no\n      0\n      166.7\n      113\n      28.34\n      148.3\n      122\n      12.61\n      186.9\n      121\n      8.41\n      10.1\n      3\n      2.73\n      3\n      False\n    \n    \n      5\n      AL\n      118\n      510\n      391-8027\n      yes\n      no\n      0\n      223.4\n      98\n      37.98\n      220.6\n      101\n      18.75\n      203.9\n      118\n      9.18\n      6.3\n      6\n      1.70\n      0\n      False\n    \n    \n      6\n      MA\n      121\n      510\n      355-9993\n      no\n      yes\n      24\n      218.2\n      88\n      37.09\n      348.5\n      108\n      29.62\n      212.6\n      118\n      9.57\n      7.5\n      7\n      2.03\n      3\n      False\n    \n    \n      7\n      MO\n      147\n      415\n      329-9001\n      yes\n      no\n      0\n      157.0\n      79\n      26.69\n      103.1\n      94\n      8.76\n      211.8\n      96\n      9.53\n      7.1\n      6\n      1.92\n      0\n      False\n    \n    \n      8\n      LA\n      117\n      408\n      335-4719\n      no\n      no\n      0\n      184.5\n      97\n      31.37\n      351.6\n      80\n      29.89\n      215.8\n      90\n      9.71\n      8.7\n      4\n      2.35\n      1\n      False\n    \n    \n      9\n      WV\n      141\n      415\n      330-8173\n      yes\n      yes\n      37\n      258.6\n      84\n      43.96\n      222.0\n      111\n      18.87\n      326.4\n      97\n      14.69\n      11.2\n      5\n      3.02\n      0\n      False\n    \n  \n\n\n\n\n\n\nCheck the Shape and Column types of the Dataframe\n\ntelecom_churn.shape\n\n(3333, 21)\n\n\n\ntelecom_churn.dtypes\n\nstate                      object\naccount length              int64\narea code                   int64\nphone number               object\ninternational plan         object\nvoice mail plan            object\nnumber vmail messages       int64\ntotal day minutes         float64\ntotal day calls             int64\ntotal day charge          float64\ntotal eve minutes         float64\ntotal eve calls             int64\ntotal eve charge          float64\ntotal night minutes       float64\ntotal night calls           int64\ntotal night charge        float64\ntotal intl minutes        float64\ntotal intl calls            int64\ntotal intl charge         float64\ncustomer service calls      int64\nchurn                        bool\ndtype: object\n\n\n\n\nExploratory Analysis\n\n# No missing values in the data.\n# Scaling of numeric columns is required\ntelecom_churn.describe()\n\n\n\n\n\n  \n    \n      \n      account length\n      area code\n      number vmail messages\n      total day minutes\n      total day calls\n      total day charge\n      total eve minutes\n      total eve calls\n      total eve charge\n      total night minutes\n      total night calls\n      total night charge\n      total intl minutes\n      total intl calls\n      total intl charge\n      customer service calls\n    \n  \n  \n    \n      count\n      3333.000000\n      3333.000000\n      3333.000000\n      3333.000000\n      3333.000000\n      3333.000000\n      3333.000000\n      3333.000000\n      3333.000000\n      3333.000000\n      3333.000000\n      3333.000000\n      3333.000000\n      3333.000000\n      3333.000000\n      3333.000000\n    \n    \n      mean\n      101.064806\n      437.182418\n      8.099010\n      179.775098\n      100.435644\n      30.562307\n      200.980348\n      100.114311\n      17.083540\n      200.872037\n      100.107711\n      9.039325\n      10.237294\n      4.479448\n      2.764581\n      1.562856\n    \n    \n      std\n      39.822106\n      42.371290\n      13.688365\n      54.467389\n      20.069084\n      9.259435\n      50.713844\n      19.922625\n      4.310668\n      50.573847\n      19.568609\n      2.275873\n      2.791840\n      2.461214\n      0.753773\n      1.315491\n    \n    \n      min\n      1.000000\n      408.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      23.200000\n      33.000000\n      1.040000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n    \n    \n      25%\n      74.000000\n      408.000000\n      0.000000\n      143.700000\n      87.000000\n      24.430000\n      166.600000\n      87.000000\n      14.160000\n      167.000000\n      87.000000\n      7.520000\n      8.500000\n      3.000000\n      2.300000\n      1.000000\n    \n    \n      50%\n      101.000000\n      415.000000\n      0.000000\n      179.400000\n      101.000000\n      30.500000\n      201.400000\n      100.000000\n      17.120000\n      201.200000\n      100.000000\n      9.050000\n      10.300000\n      4.000000\n      2.780000\n      1.000000\n    \n    \n      75%\n      127.000000\n      510.000000\n      20.000000\n      216.400000\n      114.000000\n      36.790000\n      235.300000\n      114.000000\n      20.000000\n      235.300000\n      113.000000\n      10.590000\n      12.100000\n      6.000000\n      3.270000\n      2.000000\n    \n    \n      max\n      243.000000\n      510.000000\n      51.000000\n      350.800000\n      165.000000\n      59.640000\n      363.700000\n      170.000000\n      30.910000\n      395.000000\n      175.000000\n      17.770000\n      20.000000\n      20.000000\n      5.400000\n      9.000000\n    \n  \n\n\n\n\n\n# Format the column names, remove space and special characters in column names\ntelecom_churn.columns =  telecom_churn.columns.str.strip().str.lower().str.replace(' ', '_').str.replace('(', '').str.replace(')', '')\n\n\ntelecom_churn\n\n\n\n\n\n  \n    \n      \n      state\n      account_length\n      area_code\n      phone_number\n      international_plan\n      voice_mail_plan\n      number_vmail_messages\n      total_day_minutes\n      total_day_calls\n      total_day_charge\n      total_eve_minutes\n      total_eve_calls\n      total_eve_charge\n      total_night_minutes\n      total_night_calls\n      total_night_charge\n      total_intl_minutes\n      total_intl_calls\n      total_intl_charge\n      customer_service_calls\n      churn\n    \n  \n  \n    \n      0\n      KS\n      128\n      415\n      382-4657\n      no\n      yes\n      25\n      265.1\n      110\n      45.07\n      197.4\n      99\n      16.78\n      244.7\n      91\n      11.01\n      10.0\n      3\n      2.70\n      1\n      False\n    \n    \n      1\n      OH\n      107\n      415\n      371-7191\n      no\n      yes\n      26\n      161.6\n      123\n      27.47\n      195.5\n      103\n      16.62\n      254.4\n      103\n      11.45\n      13.7\n      3\n      3.70\n      1\n      False\n    \n    \n      2\n      NJ\n      137\n      415\n      358-1921\n      no\n      no\n      0\n      243.4\n      114\n      41.38\n      121.2\n      110\n      10.30\n      162.6\n      104\n      7.32\n      12.2\n      5\n      3.29\n      0\n      False\n    \n    \n      3\n      OH\n      84\n      408\n      375-9999\n      yes\n      no\n      0\n      299.4\n      71\n      50.90\n      61.9\n      88\n      5.26\n      196.9\n      89\n      8.86\n      6.6\n      7\n      1.78\n      2\n      False\n    \n    \n      4\n      OK\n      75\n      415\n      330-6626\n      yes\n      no\n      0\n      166.7\n      113\n      28.34\n      148.3\n      122\n      12.61\n      186.9\n      121\n      8.41\n      10.1\n      3\n      2.73\n      3\n      False\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      3328\n      AZ\n      192\n      415\n      414-4276\n      no\n      yes\n      36\n      156.2\n      77\n      26.55\n      215.5\n      126\n      18.32\n      279.1\n      83\n      12.56\n      9.9\n      6\n      2.67\n      2\n      False\n    \n    \n      3329\n      WV\n      68\n      415\n      370-3271\n      no\n      no\n      0\n      231.1\n      57\n      39.29\n      153.4\n      55\n      13.04\n      191.3\n      123\n      8.61\n      9.6\n      4\n      2.59\n      3\n      False\n    \n    \n      3330\n      RI\n      28\n      510\n      328-8230\n      no\n      no\n      0\n      180.8\n      109\n      30.74\n      288.8\n      58\n      24.55\n      191.9\n      91\n      8.64\n      14.1\n      6\n      3.81\n      2\n      False\n    \n    \n      3331\n      CT\n      184\n      510\n      364-6381\n      yes\n      no\n      0\n      213.8\n      105\n      36.35\n      159.6\n      84\n      13.57\n      139.2\n      137\n      6.26\n      5.0\n      10\n      1.35\n      2\n      False\n    \n    \n      3332\n      TN\n      74\n      415\n      400-4344\n      no\n      yes\n      25\n      234.4\n      113\n      39.85\n      265.9\n      82\n      22.60\n      241.4\n      77\n      10.86\n      13.7\n      4\n      3.70\n      0\n      False\n    \n  \n\n3333 rows × 21 columns\n\n\n\n\n#telecom_churn[\"area_code\"] = telecom_churn[\"area_code\"].astype('category')\n\n\nprofile = ProfileReport(telecom_churn, title = \"Telecom Churn Report\")\n\n\n# create report for EDA\nprofile.to_widgets()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#save the profile report\nprofile.to_file(\"telecom_churn_eda.html\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n[telecom_churn.churn.value_counts()]\n\n[False    2850\n True      483\n Name: churn, dtype: int64]\n\n\n\npd.crosstab(telecom_churn.churn, telecom_churn. customer_service_calls,margins=True, margins_name=\"Total\")\n\n\n\n\n\n  \n    \n      customer_service_calls\n      0\n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n      9\n      Total\n    \n    \n      churn\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      False\n      605\n      1059\n      672\n      385\n      90\n      26\n      8\n      4\n      1\n      0\n      2850\n    \n    \n      True\n      92\n      122\n      87\n      44\n      76\n      40\n      14\n      5\n      1\n      2\n      483\n    \n    \n      Total\n      697\n      1181\n      759\n      429\n      166\n      66\n      22\n      9\n      2\n      2\n      3333\n    \n  \n\n\n\n\n\nimport matplotlib.pyplot as plt\nct = pd.crosstab(telecom_churn.churn, telecom_churn.customer_service_calls)\nct.plot.bar(stacked=True)\nplt.legend(title='churn vs Number of calls')\nplt.show()\n\n\n\n\n\ntelecom_churn['area_code'].value_counts().plot.bar()\n\n<AxesSubplot:>\n\n\n\n\n\n\ntelecom_churn['state'].value_counts().head(10).plot.bar()\n\n<AxesSubplot:>\n\n\n\n\n\n\ntelecom_churn['international_plan'].value_counts().plot.bar()\n\n<AxesSubplot:>\n\n\n\n\n\n\ntelecom_churn['voice_mail_plan'].value_counts().plot.bar()\n\n<AxesSubplot:>\n\n\n\n\n\n\n\nObservations from EDA are:-\n\nDataset is imbalanced - 85.5% customers did not churn and 14.5% customers churned\nState consist of 51 distinct values with high cardinality\nNumeric variables are in different ranges and needs to be scaled\nThree distinct area codes - Area code ‘415’ is 49.7%, rest two area codes are equally distributed\nDistinct values of phone number is equal to the length of the dataset. This will be equivalent to primary key of the dataset. Not be included in modelling\n72.3% customers did not activate their voicemail plan. This is verified by equal number of customers with zero number of voice messages\nTotal international calls and customer service calls data is skewed. This is verified by high kurtosis and skewness.\nAll the other numeric variables are following normal distribution as verified by kurtosis / skewness values and histogram\n\n\n\nSplit the Data for Training and Testing\nThe Machine learning algorithm should not be exposed to test data. The performance of the learning algorithm can only be measured by testing on unseen data. To achieve the same a train and test split with 95% and 5% is created. It is ensured that the sampling is stratified so that the proportion of churn and not churn customers are equal in train and test data. As the amount of data is very less, only 5% of the data is kept aside for testing.Further the train data is further split into train and validation set with 90% and 10%. Validation set is required for hyperparameter tuning.As validation set is also exposed to training algorithm, it is also should not be used for model validation. Model validation is done on test set only.\n\n# convert the target value to integers. \ntelecom_churn['churn'] = telecom_churn['churn'] * 1\n\n\ntrain, test = train_test_split(telecom_churn, test_size = 0.05, stratify = telecom_churn['churn']) \nprint('Data for Modeling: ' + str(train.shape))\nprint('Unseen Data For Predictions: ' + str(test.shape))\n\nData for Modeling: (3166, 21)\nUnseen Data For Predictions: (167, 21)\n\n\n\n# Test the proportion of churn in train and test sets\ntrain.churn.value_counts()\n\n0    2707\n1     459\nName: churn, dtype: int64\n\n\n\n# 16.5% of the customers churned in train data\n(459/2707)*100\n\n16.956039896564462\n\n\n\ntest.churn.value_counts()\n\n0    143\n1     24\nName: churn, dtype: int64\n\n\n\n# 16.7% of the customers churned in test data\n(24/143)*100\n# customers churned proportionally from train and test data\n\n16.783216783216783\n\n\n\n\nModelling with Pycaret\n\nTrain and validation sets are created with 90 % and 10 % data.\nThe random seed selected for the modeling is 786\nIn this step we are normalizing the data, ignoring the variable ‘phone number’ for analysis\nFixing the imbalance in the data using SMOTE method\nWe are transforming the features - Changing the distribution of variables to a normal or approximate normal distribution\nIgnorning features with low variance - This will ignore variables (multi level categorical) where a single level dominates and there is not much variation in the information provided by the feature\nThe setup is inferring the customer_service_calls as numeric (as there are only ten distinct values). Hence explicitly mentioning it as numeric\n\n\nexp_clf =    setup(data = train, target = 'churn', session_id = 786, \n                   train_size = 0.9,\n                   normalize = True,\n                   transformation = True,\n                   ignore_low_variance = True,               \n                   ignore_features = ['phone_number'],\n                   fix_imbalance = True,\n                   high_cardinality_features = ['state'],\n                   numeric_features = ['customer_service_calls'])               \n\nSetup Succesfully Completed!\n\n\n\n                    Description        Value    \n                \n                        0\n                        session_id\n                        786\n            \n            \n                        1\n                        Target Type\n                        Binary\n            \n            \n                        2\n                        Label Encoded\n                        None\n            \n            \n                        3\n                        Original Data\n                        (3166, 21)\n            \n            \n                        4\n                        Missing Values \n                        False\n            \n            \n                        5\n                        Numeric Features \n                        15\n            \n            \n                        6\n                        Categorical Features \n                        5\n            \n            \n                        7\n                        Ordinal Features \n                        False\n            \n            \n                        8\n                        High Cardinality Features \n                        True\n            \n            \n                        9\n                        High Cardinality Method \n                        frequency\n            \n            \n                        10\n                        Sampled Data\n                        (3166, 21)\n            \n            \n                        11\n                        Transformed Train Set\n                        (2849, 22)\n            \n            \n                        12\n                        Transformed Test Set\n                        (317, 22)\n            \n            \n                        13\n                        Numeric Imputer \n                        mean\n            \n            \n                        14\n                        Categorical Imputer \n                        constant\n            \n            \n                        15\n                        Normalize \n                        True\n            \n            \n                        16\n                        Normalize Method \n                        zscore\n            \n            \n                        17\n                        Transformation \n                        True\n            \n            \n                        18\n                        Transformation Method \n                        yeo-johnson\n            \n            \n                        19\n                        PCA \n                        False\n            \n            \n                        20\n                        PCA Method \n                        None\n            \n            \n                        21\n                        PCA Components \n                        None\n            \n            \n                        22\n                        Ignore Low Variance \n                        True\n            \n            \n                        23\n                        Combine Rare Levels \n                        False\n            \n            \n                        24\n                        Rare Level Threshold \n                        None\n            \n            \n                        25\n                        Numeric Binning \n                        False\n            \n            \n                        26\n                        Remove Outliers \n                        False\n            \n            \n                        27\n                        Outliers Threshold \n                        None\n            \n            \n                        28\n                        Remove Multicollinearity \n                        False\n            \n            \n                        29\n                        Multicollinearity Threshold \n                        None\n            \n            \n                        30\n                        Clustering \n                        False\n            \n            \n                        31\n                        Clustering Iteration \n                        None\n            \n            \n                        32\n                        Polynomial Features \n                        False\n            \n            \n                        33\n                        Polynomial Degree \n                        None\n            \n            \n                        34\n                        Trignometry Features \n                        False\n            \n            \n                        35\n                        Polynomial Threshold \n                        None\n            \n            \n                        36\n                        Group Features \n                        False\n            \n            \n                        37\n                        Feature Selection \n                        False\n            \n            \n                        38\n                        Features Selection Threshold \n                        None\n            \n            \n                        39\n                        Feature Interaction \n                        False\n            \n            \n                        40\n                        Feature Ratio \n                        False\n            \n            \n                        41\n                        Interaction Threshold \n                        None\n            \n            \n                        42\n                        Fix Imbalance\n                        True\n            \n            \n                        43\n                        Fix Imbalance Method\n                        SMOTE\n            \n    \n\n\n\ncompare_models(fold = 5)\n\n\n                    Model        Accuracy        AUC        Recall        Prec.        F1        Kappa        MCC        TT (Sec)    \n                \n                        0\n                        Light Gradient Boosting Machine\n                        0.9512\n                        0.9127\n                        0.7771\n                        0.8732\n                        0.8213\n                        0.7932\n                        0.7957\n                        0.1908\n            \n            \n                        1\n                        Extreme Gradient Boosting\n                        0.9488\n                        0.9145\n                        0.7675\n                        0.8652\n                        0.8124\n                        0.7829\n                        0.7854\n                        0.2130\n            \n            \n                        2\n                        CatBoost Classifier\n                        0.9466\n                        0.9140\n                        0.7577\n                        0.8583\n                        0.8041\n                        0.7734\n                        0.7759\n                        4.3314\n            \n            \n                        3\n                        Extra Trees Classifier\n                        0.9333\n                        0.9032\n                        0.6535\n                        0.8544\n                        0.7392\n                        0.7018\n                        0.7109\n                        0.1456\n            \n            \n                        4\n                        Random Forest Classifier\n                        0.9330\n                        0.9082\n                        0.7045\n                        0.8147\n                        0.7529\n                        0.7145\n                        0.7186\n                        0.0366\n            \n            \n                        5\n                        Gradient Boosting Classifier\n                        0.9308\n                        0.9078\n                        0.7674\n                        0.7618\n                        0.7635\n                        0.7230\n                        0.7238\n                        1.1674\n            \n            \n                        6\n                        Decision Tree Classifier\n                        0.8929\n                        0.8227\n                        0.7238\n                        0.6106\n                        0.6616\n                        0.5986\n                        0.6022\n                        0.0356\n            \n            \n                        7\n                        Ada Boost Classifier\n                        0.8645\n                        0.8464\n                        0.6003\n                        0.5325\n                        0.5625\n                        0.4829\n                        0.4853\n                        0.2996\n            \n            \n                        8\n                        Naive Bayes\n                        0.8284\n                        0.7894\n                        0.6439\n                        0.4430\n                        0.5233\n                        0.4237\n                        0.4355\n                        0.0028\n            \n            \n                        9\n                        K Neighbors Classifier\n                        0.7715\n                        0.7897\n                        0.6878\n                        0.3531\n                        0.4664\n                        0.3398\n                        0.3705\n                        0.0160\n            \n            \n                        10\n                        Logistic Regression\n                        0.7375\n                        0.7886\n                        0.6852\n                        0.3152\n                        0.4314\n                        0.2904\n                        0.3273\n                        0.0316\n            \n            \n                        11\n                        Ridge Classifier\n                        0.7371\n                        0.0000\n                        0.6852\n                        0.3149\n                        0.4311\n                        0.2899\n                        0.3270\n                        0.0082\n            \n            \n                        12\n                        Linear Discriminant Analysis\n                        0.7336\n                        0.7844\n                        0.6779\n                        0.3103\n                        0.4252\n                        0.2824\n                        0.3190\n                        0.0144\n            \n            \n                        13\n                        SVM - Linear Kernel\n                        0.7329\n                        0.0000\n                        0.6079\n                        0.3163\n                        0.4021\n                        0.2621\n                        0.2910\n                        0.0198\n            \n            \n                        14\n                        Quadratic Discriminant Analysis\n                        0.5907\n                        0.6505\n                        0.6373\n                        0.2089\n                        0.3106\n                        0.1201\n                        0.1596\n                        0.0110\n            \n    \n\n\nLGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n               importance_type='split', learning_rate=0.1, max_depth=-1,\n               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,\n               random_state=786, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)\n\n\n\n\nCreating the models for top performing algorithms based on Precision and AUC. Tree based models are performing well on this dataset\n\nlightgbm = create_model('lightgbm', fold =5)\n\n\n                    Accuracy        AUC        Recall        Prec.        F1        Kappa        MCC    \n                \n                        0\n                        0.9526\n                        0.9008\n                        0.8171\n                        0.8481\n                        0.8323\n                        0.8047\n                        0.8049\n            \n            \n                        1\n                        0.9561\n                        0.9271\n                        0.8193\n                        0.8718\n                        0.8447\n                        0.8192\n                        0.8197\n            \n            \n                        2\n                        0.9544\n                        0.9241\n                        0.8072\n                        0.8701\n                        0.8375\n                        0.8110\n                        0.8118\n            \n            \n                        3\n                        0.9544\n                        0.9278\n                        0.7470\n                        0.9254\n                        0.8267\n                        0.8007\n                        0.8068\n            \n            \n                        4\n                        0.9385\n                        0.8836\n                        0.6951\n                        0.8507\n                        0.7651\n                        0.7301\n                        0.7351\n            \n            \n                        Mean\n                        0.9512\n                        0.9127\n                        0.7771\n                        0.8732\n                        0.8213\n                        0.7932\n                        0.7957\n            \n            \n                        SD\n                        0.0065\n                        0.0176\n                        0.0488\n                        0.0278\n                        0.0287\n                        0.0321\n                        0.0307\n            \n    \n\n\n\ncatboost = create_model('catboost', fold =5)\n\n\n                    Accuracy        AUC        Recall        Prec.        F1        Kappa        MCC    \n                \n                        0\n                        0.9421\n                        0.9161\n                        0.7439\n                        0.8356\n                        0.7871\n                        0.7537\n                        0.7554\n            \n            \n                        1\n                        0.9456\n                        0.9254\n                        0.7711\n                        0.8421\n                        0.8050\n                        0.7735\n                        0.7745\n            \n            \n                        2\n                        0.9561\n                        0.9198\n                        0.8313\n                        0.8625\n                        0.8466\n                        0.8210\n                        0.8212\n            \n            \n                        3\n                        0.9544\n                        0.9314\n                        0.7470\n                        0.9254\n                        0.8267\n                        0.8007\n                        0.8068\n            \n            \n                        4\n                        0.9350\n                        0.8775\n                        0.6951\n                        0.8261\n                        0.7550\n                        0.7178\n                        0.7214\n            \n            \n                        Mean\n                        0.9466\n                        0.9140\n                        0.7577\n                        0.8583\n                        0.8041\n                        0.7734\n                        0.7759\n            \n            \n                        SD\n                        0.0078\n                        0.0190\n                        0.0443\n                        0.0356\n                        0.0317\n                        0.0360\n                        0.0358\n            \n    \n\n\n\nxgboost = create_model('xgboost', fold = 5)\n\n\n                    Accuracy        AUC        Recall        Prec.        F1        Kappa        MCC    \n                \n                        0\n                        0.9474\n                        0.9080\n                        0.8171\n                        0.8171\n                        0.8171\n                        0.7863\n                        0.7863\n            \n            \n                        1\n                        0.9561\n                        0.9270\n                        0.8072\n                        0.8816\n                        0.8428\n                        0.8173\n                        0.8184\n            \n            \n                        2\n                        0.9474\n                        0.9231\n                        0.7590\n                        0.8630\n                        0.8077\n                        0.7773\n                        0.7795\n            \n            \n                        3\n                        0.9509\n                        0.9393\n                        0.7470\n                        0.8986\n                        0.8158\n                        0.7877\n                        0.7922\n            \n            \n                        4\n                        0.9420\n                        0.8752\n                        0.7073\n                        0.8657\n                        0.7785\n                        0.7455\n                        0.7506\n            \n            \n                        Mean\n                        0.9488\n                        0.9145\n                        0.7675\n                        0.8652\n                        0.8124\n                        0.7829\n                        0.7854\n            \n            \n                        SD\n                        0.0047\n                        0.0220\n                        0.0404\n                        0.0272\n                        0.0206\n                        0.0230\n                        0.0218\n            \n    \n\n\n\nrf = create_model('rf', fold =5)\n\n\n                    Accuracy        AUC        Recall        Prec.        F1        Kappa        MCC    \n                \n                        0\n                        0.9298\n                        0.8986\n                        0.7317\n                        0.7692\n                        0.7500\n                        0.7092\n                        0.7095\n            \n            \n                        1\n                        0.9263\n                        0.9227\n                        0.7349\n                        0.7531\n                        0.7439\n                        0.7009\n                        0.7009\n            \n            \n                        2\n                        0.9281\n                        0.9091\n                        0.6867\n                        0.7917\n                        0.7355\n                        0.6941\n                        0.6965\n            \n            \n                        3\n                        0.9421\n                        0.9291\n                        0.7349\n                        0.8472\n                        0.7871\n                        0.7538\n                        0.7563\n            \n            \n                        4\n                        0.9385\n                        0.8817\n                        0.6341\n                        0.9123\n                        0.7482\n                        0.7145\n                        0.7298\n            \n            \n                        Mean\n                        0.9330\n                        0.9082\n                        0.7045\n                        0.8147\n                        0.7529\n                        0.7145\n                        0.7186\n            \n            \n                        SD\n                        0.0062\n                        0.0170\n                        0.0396\n                        0.0583\n                        0.0178\n                        0.0208\n                        0.0221\n            \n    \n\n\n\net = create_model('et', fold =5)\n\n\n                    Accuracy        AUC        Recall        Prec.        F1        Kappa        MCC    \n                \n                        0\n                        0.9175\n                        0.8940\n                        0.6463\n                        0.7465\n                        0.6928\n                        0.6455\n                        0.6477\n            \n            \n                        1\n                        0.9404\n                        0.9064\n                        0.6988\n                        0.8657\n                        0.7733\n                        0.7394\n                        0.7451\n            \n            \n                        2\n                        0.9404\n                        0.9096\n                        0.6747\n                        0.8889\n                        0.7671\n                        0.7337\n                        0.7428\n            \n            \n                        3\n                        0.9456\n                        0.9307\n                        0.6867\n                        0.9194\n                        0.7862\n                        0.7558\n                        0.7664\n            \n            \n                        4\n                        0.9227\n                        0.8754\n                        0.5610\n                        0.8519\n                        0.6765\n                        0.6347\n                        0.6525\n            \n            \n                        Mean\n                        0.9333\n                        0.9032\n                        0.6535\n                        0.8544\n                        0.7392\n                        0.7018\n                        0.7109\n            \n            \n                        SD\n                        0.0111\n                        0.0183\n                        0.0494\n                        0.0586\n                        0.0453\n                        0.0510\n                        0.0503\n            \n    \n\n\n\n\nTune the created models for selecting the best hyperparameters\n\ntuned_lightgbm = tune_model(lightgbm, optimize = 'F1', n_iter = 30)\n\n\n                    Accuracy        AUC        Recall        Prec.        F1        Kappa        MCC    \n                \n                        0\n                        0.9404\n                        0.9034\n                        0.7805\n                        0.8000\n                        0.7901\n                        0.7554\n                        0.7554\n            \n            \n                        1\n                        0.9439\n                        0.8894\n                        0.8049\n                        0.8049\n                        0.8049\n                        0.7721\n                        0.7721\n            \n            \n                        2\n                        0.9579\n                        0.9525\n                        0.8537\n                        0.8537\n                        0.8537\n                        0.8291\n                        0.8291\n            \n            \n                        3\n                        0.9649\n                        0.9000\n                        0.8049\n                        0.9429\n                        0.8684\n                        0.8483\n                        0.8519\n            \n            \n                        4\n                        0.9474\n                        0.9161\n                        0.8049\n                        0.8250\n                        0.8148\n                        0.7841\n                        0.7842\n            \n            \n                        5\n                        0.9614\n                        0.9198\n                        0.8049\n                        0.9167\n                        0.8571\n                        0.8349\n                        0.8373\n            \n            \n                        6\n                        0.9684\n                        0.9594\n                        0.8810\n                        0.9024\n                        0.8916\n                        0.8731\n                        0.8732\n            \n            \n                        7\n                        0.9649\n                        0.9162\n                        0.7619\n                        1.0000\n                        0.8649\n                        0.8451\n                        0.8554\n            \n            \n                        8\n                        0.9333\n                        0.8673\n                        0.7381\n                        0.7949\n                        0.7654\n                        0.7266\n                        0.7273\n            \n            \n                        9\n                        0.9437\n                        0.9092\n                        0.6829\n                        0.9032\n                        0.7778\n                        0.7462\n                        0.7558\n            \n            \n                        Mean\n                        0.9526\n                        0.9134\n                        0.7918\n                        0.8744\n                        0.8289\n                        0.8015\n                        0.8042\n            \n            \n                        SD\n                        0.0117\n                        0.0259\n                        0.0531\n                        0.0659\n                        0.0414\n                        0.0480\n                        0.0484\n            \n    \n\n\n\ntuned_catboost = tune_model(catboost, optimize = 'F1', n_iter = 30)\n\n\n                    Accuracy        AUC        Recall        Prec.        F1        Kappa        MCC    \n                \n                        0\n                        0.9368\n                        0.8919\n                        0.7317\n                        0.8108\n                        0.7692\n                        0.7328\n                        0.7341\n            \n            \n                        1\n                        0.9474\n                        0.9042\n                        0.7805\n                        0.8421\n                        0.8101\n                        0.7796\n                        0.7804\n            \n            \n                        2\n                        0.9509\n                        0.9510\n                        0.8049\n                        0.8462\n                        0.8250\n                        0.7964\n                        0.7968\n            \n            \n                        3\n                        0.9544\n                        0.8872\n                        0.7561\n                        0.9118\n                        0.8267\n                        0.8007\n                        0.8053\n            \n            \n                        4\n                        0.9474\n                        0.9258\n                        0.8049\n                        0.8250\n                        0.8148\n                        0.7841\n                        0.7842\n            \n            \n                        5\n                        0.9544\n                        0.9058\n                        0.8049\n                        0.8684\n                        0.8354\n                        0.8090\n                        0.8098\n            \n            \n                        6\n                        0.9579\n                        0.9607\n                        0.7619\n                        0.9412\n                        0.8421\n                        0.8181\n                        0.8242\n            \n            \n                        7\n                        0.9439\n                        0.9133\n                        0.6667\n                        0.9333\n                        0.7778\n                        0.7467\n                        0.7605\n            \n            \n                        8\n                        0.9193\n                        0.8823\n                        0.7143\n                        0.7317\n                        0.7229\n                        0.6757\n                        0.6757\n            \n            \n                        9\n                        0.9437\n                        0.8767\n                        0.7073\n                        0.8788\n                        0.7838\n                        0.7518\n                        0.7577\n            \n            \n                        Mean\n                        0.9456\n                        0.9099\n                        0.7533\n                        0.8589\n                        0.8008\n                        0.7695\n                        0.7729\n            \n            \n                        SD\n                        0.0106\n                        0.0270\n                        0.0452\n                        0.0597\n                        0.0351\n                        0.0411\n                        0.0414\n            \n    \n\n\n\ntuned_xgboost = tune_model(xgboost, optimize = 'F1', n_iter = 30)\n\n\n                    Accuracy        AUC        Recall        Prec.        F1        Kappa        MCC    \n                \n                        0\n                        0.9474\n                        0.8786\n                        0.7805\n                        0.8421\n                        0.8101\n                        0.7796\n                        0.7804\n            \n            \n                        1\n                        0.9474\n                        0.8737\n                        0.7805\n                        0.8421\n                        0.8101\n                        0.7796\n                        0.7804\n            \n            \n                        2\n                        0.9509\n                        0.9396\n                        0.8293\n                        0.8293\n                        0.8293\n                        0.8006\n                        0.8006\n            \n            \n                        3\n                        0.9684\n                        0.9017\n                        0.8049\n                        0.9706\n                        0.8800\n                        0.8620\n                        0.8670\n            \n            \n                        4\n                        0.9509\n                        0.9386\n                        0.7805\n                        0.8649\n                        0.8205\n                        0.7921\n                        0.7935\n            \n            \n                        5\n                        0.9544\n                        0.9125\n                        0.7561\n                        0.9118\n                        0.8267\n                        0.8007\n                        0.8053\n            \n            \n                        6\n                        0.9439\n                        0.9686\n                        0.7381\n                        0.8611\n                        0.7949\n                        0.7626\n                        0.7656\n            \n            \n                        7\n                        0.9614\n                        0.9216\n                        0.7857\n                        0.9429\n                        0.8571\n                        0.8350\n                        0.8397\n            \n            \n                        8\n                        0.9263\n                        0.8634\n                        0.6905\n                        0.7838\n                        0.7342\n                        0.6916\n                        0.6935\n            \n            \n                        9\n                        0.9366\n                        0.9118\n                        0.6829\n                        0.8485\n                        0.7568\n                        0.7208\n                        0.7264\n            \n            \n                        Mean\n                        0.9487\n                        0.9110\n                        0.7629\n                        0.8697\n                        0.8120\n                        0.7825\n                        0.7852\n            \n            \n                        SD\n                        0.0113\n                        0.0313\n                        0.0446\n                        0.0533\n                        0.0408\n                        0.0472\n                        0.0476\n            \n    \n\n\n\ntuned_rf = tune_model(rf, optimize = 'F1', n_iter = 30)\n\n\n                    Accuracy        AUC        Recall        Prec.        F1        Kappa        MCC    \n                \n                        0\n                        0.9368\n                        0.8803\n                        0.7561\n                        0.7949\n                        0.7750\n                        0.7383\n                        0.7386\n            \n            \n                        1\n                        0.9368\n                        0.9106\n                        0.8049\n                        0.7674\n                        0.7857\n                        0.7487\n                        0.7490\n            \n            \n                        2\n                        0.9439\n                        0.9287\n                        0.8049\n                        0.8049\n                        0.8049\n                        0.7721\n                        0.7721\n            \n            \n                        3\n                        0.9614\n                        0.9113\n                        0.7805\n                        0.9412\n                        0.8533\n                        0.8313\n                        0.8362\n            \n            \n                        4\n                        0.9439\n                        0.9427\n                        0.8049\n                        0.8049\n                        0.8049\n                        0.7721\n                        0.7721\n            \n            \n                        5\n                        0.9368\n                        0.8999\n                        0.7561\n                        0.7949\n                        0.7750\n                        0.7383\n                        0.7386\n            \n            \n                        6\n                        0.9439\n                        0.9418\n                        0.7143\n                        0.8824\n                        0.7895\n                        0.7575\n                        0.7631\n            \n            \n                        7\n                        0.9509\n                        0.9113\n                        0.7143\n                        0.9375\n                        0.8108\n                        0.7832\n                        0.7927\n            \n            \n                        8\n                        0.9263\n                        0.8532\n                        0.6905\n                        0.7838\n                        0.7342\n                        0.6916\n                        0.6935\n            \n            \n                        9\n                        0.9437\n                        0.8886\n                        0.6829\n                        0.9032\n                        0.7778\n                        0.7462\n                        0.7558\n            \n            \n                        Mean\n                        0.9424\n                        0.9068\n                        0.7509\n                        0.8415\n                        0.7911\n                        0.7579\n                        0.7612\n            \n            \n                        SD\n                        0.0089\n                        0.0265\n                        0.0454\n                        0.0636\n                        0.0293\n                        0.0344\n                        0.0356\n            \n    \n\n\n\ntuned_et = tune_model(et, optimize = 'F1' , n_iter = 30)\n\n\n                    Accuracy        AUC        Recall        Prec.        F1        Kappa        MCC    \n                \n                        0\n                        0.9158\n                        0.8828\n                        0.6341\n                        0.7429\n                        0.6842\n                        0.6360\n                        0.6386\n            \n            \n                        1\n                        0.9298\n                        0.8971\n                        0.6829\n                        0.8000\n                        0.7368\n                        0.6966\n                        0.6996\n            \n            \n                        2\n                        0.9263\n                        0.9369\n                        0.6585\n                        0.7941\n                        0.7200\n                        0.6780\n                        0.6819\n            \n            \n                        3\n                        0.9509\n                        0.8955\n                        0.7317\n                        0.9091\n                        0.8108\n                        0.7830\n                        0.7891\n            \n            \n                        4\n                        0.9368\n                        0.9228\n                        0.6829\n                        0.8485\n                        0.7568\n                        0.7210\n                        0.7266\n            \n            \n                        5\n                        0.9439\n                        0.8852\n                        0.7073\n                        0.8788\n                        0.7838\n                        0.7520\n                        0.7578\n            \n            \n                        6\n                        0.9474\n                        0.9410\n                        0.7381\n                        0.8857\n                        0.8052\n                        0.7751\n                        0.7794\n            \n            \n                        7\n                        0.9439\n                        0.9394\n                        0.6429\n                        0.9643\n                        0.7714\n                        0.7409\n                        0.7607\n            \n            \n                        8\n                        0.9123\n                        0.8558\n                        0.5476\n                        0.7931\n                        0.6479\n                        0.5997\n                        0.6131\n            \n            \n                        9\n                        0.9120\n                        0.8913\n                        0.4878\n                        0.8333\n                        0.6154\n                        0.5695\n                        0.5956\n            \n            \n                        Mean\n                        0.9319\n                        0.9048\n                        0.6514\n                        0.8450\n                        0.7332\n                        0.6952\n                        0.7042\n            \n            \n                        SD\n                        0.0141\n                        0.0273\n                        0.0755\n                        0.0625\n                        0.0629\n                        0.0698\n                        0.0666\n            \n    \n\n\n\n\nCreate an Ensemble, Blended and Stack model to see the performance\n\ndt = create_model('dt' , fold = 5)\n\n\n                    Accuracy        AUC        Recall        Prec.        F1        Kappa        MCC    \n                \n                        0\n                        0.8895\n                        0.8289\n                        0.7439\n                        0.5922\n                        0.6595\n                        0.5945\n                        0.6000\n            \n            \n                        1\n                        0.8912\n                        0.8314\n                        0.7470\n                        0.6019\n                        0.6667\n                        0.6026\n                        0.6076\n            \n            \n                        2\n                        0.8877\n                        0.8094\n                        0.6988\n                        0.5979\n                        0.6444\n                        0.5783\n                        0.5807\n            \n            \n                        3\n                        0.9035\n                        0.8536\n                        0.7831\n                        0.6373\n                        0.7027\n                        0.6458\n                        0.6507\n            \n            \n                        4\n                        0.8928\n                        0.7903\n                        0.6463\n                        0.6235\n                        0.6347\n                        0.5719\n                        0.5721\n            \n            \n                        Mean\n                        0.8929\n                        0.8227\n                        0.7238\n                        0.6106\n                        0.6616\n                        0.5986\n                        0.6022\n            \n            \n                        SD\n                        0.0055\n                        0.0214\n                        0.0471\n                        0.0170\n                        0.0234\n                        0.0260\n                        0.0274\n            \n    \n\n\n\ntuned_dt = tune_model(dt, optimize = 'F1', n_iter = 30)\n\n\n                    Accuracy        AUC        Recall        Prec.        F1        Kappa        MCC    \n                \n                        0\n                        0.9018\n                        0.8782\n                        0.7805\n                        0.6275\n                        0.6957\n                        0.6379\n                        0.6433\n            \n            \n                        1\n                        0.8842\n                        0.9037\n                        0.8049\n                        0.5690\n                        0.6667\n                        0.5991\n                        0.6123\n            \n            \n                        2\n                        0.8982\n                        0.9072\n                        0.8780\n                        0.6000\n                        0.7129\n                        0.6537\n                        0.6712\n            \n            \n                        3\n                        0.9053\n                        0.8717\n                        0.7805\n                        0.6400\n                        0.7033\n                        0.6476\n                        0.6521\n            \n            \n                        4\n                        0.9193\n                        0.8988\n                        0.8049\n                        0.6875\n                        0.7416\n                        0.6941\n                        0.6971\n            \n            \n                        5\n                        0.9018\n                        0.9168\n                        0.8537\n                        0.6140\n                        0.7143\n                        0.6569\n                        0.6699\n            \n            \n                        6\n                        0.9439\n                        0.9188\n                        0.8333\n                        0.7955\n                        0.8140\n                        0.7809\n                        0.7812\n            \n            \n                        7\n                        0.9298\n                        0.8626\n                        0.7381\n                        0.7750\n                        0.7561\n                        0.7151\n                        0.7154\n            \n            \n                        8\n                        0.9053\n                        0.8125\n                        0.6667\n                        0.6829\n                        0.6747\n                        0.6193\n                        0.6193\n            \n            \n                        9\n                        0.9190\n                        0.8680\n                        0.7073\n                        0.7250\n                        0.7160\n                        0.6688\n                        0.6689\n            \n            \n                        Mean\n                        0.9108\n                        0.8838\n                        0.7848\n                        0.6716\n                        0.7195\n                        0.6673\n                        0.6731\n            \n            \n                        SD\n                        0.0164\n                        0.0307\n                        0.0623\n                        0.0715\n                        0.0406\n                        0.0494\n                        0.0469\n            \n    \n\n\n\nbagged_dt = ensemble_model(tuned_dt, n_estimators = 200, optimize = 'F1')\n\n\n                    Accuracy        AUC        Recall        Prec.        F1        Kappa        MCC    \n                \n                        0\n                        0.8807\n                        0.8698\n                        0.7805\n                        0.5614\n                        0.6531\n                        0.5833\n                        0.5949\n            \n            \n                        1\n                        0.9298\n                        0.9043\n                        0.8293\n                        0.7234\n                        0.7727\n                        0.7315\n                        0.7338\n            \n            \n                        2\n                        0.9053\n                        0.9175\n                        0.9024\n                        0.6167\n                        0.7327\n                        0.6776\n                        0.6957\n            \n            \n                        3\n                        0.9404\n                        0.9077\n                        0.8049\n                        0.7857\n                        0.7952\n                        0.7603\n                        0.7604\n            \n            \n                        4\n                        0.9298\n                        0.9073\n                        0.8537\n                        0.7143\n                        0.7778\n                        0.7365\n                        0.7406\n            \n            \n                        5\n                        0.9123\n                        0.9220\n                        0.8293\n                        0.6538\n                        0.7312\n                        0.6796\n                        0.6865\n            \n            \n                        6\n                        0.9509\n                        0.9436\n                        0.8810\n                        0.8043\n                        0.8409\n                        0.8119\n                        0.8131\n            \n            \n                        7\n                        0.9509\n                        0.9128\n                        0.7857\n                        0.8684\n                        0.8250\n                        0.7965\n                        0.7979\n            \n            \n                        8\n                        0.9088\n                        0.8489\n                        0.7381\n                        0.6739\n                        0.7045\n                        0.6507\n                        0.6517\n            \n            \n                        9\n                        0.9401\n                        0.8935\n                        0.7561\n                        0.8158\n                        0.7848\n                        0.7501\n                        0.7508\n            \n            \n                        Mean\n                        0.9249\n                        0.9027\n                        0.8161\n                        0.7218\n                        0.7618\n                        0.7178\n                        0.7225\n            \n            \n                        SD\n                        0.0215\n                        0.0254\n                        0.0504\n                        0.0922\n                        0.0540\n                        0.0664\n                        0.0632\n            \n    \n\n\n\nboosted_dt = ensemble_model(tuned_dt, method = 'Boosting', n_estimators = 50, optimize = 'F1')\n\n\n                    Accuracy        AUC        Recall        Prec.        F1        Kappa        MCC    \n                \n                        0\n                        0.8877\n                        0.8717\n                        0.5610\n                        0.6216\n                        0.5897\n                        0.5249\n                        0.5258\n            \n            \n                        1\n                        0.9123\n                        0.8352\n                        0.6341\n                        0.7222\n                        0.6753\n                        0.6249\n                        0.6266\n            \n            \n                        2\n                        0.9333\n                        0.9076\n                        0.7073\n                        0.8056\n                        0.7532\n                        0.7149\n                        0.7169\n            \n            \n                        3\n                        0.9404\n                        0.8308\n                        0.6829\n                        0.8750\n                        0.7671\n                        0.7335\n                        0.7409\n            \n            \n                        4\n                        0.9053\n                        0.8260\n                        0.6098\n                        0.6944\n                        0.6494\n                        0.5949\n                        0.5965\n            \n            \n                        5\n                        0.9333\n                        0.9336\n                        0.7317\n                        0.7895\n                        0.7595\n                        0.7209\n                        0.7216\n            \n            \n                        6\n                        0.9228\n                        0.8915\n                        0.6429\n                        0.7941\n                        0.7105\n                        0.6666\n                        0.6715\n            \n            \n                        7\n                        0.9018\n                        0.8379\n                        0.4524\n                        0.7917\n                        0.5758\n                        0.5248\n                        0.5512\n            \n            \n                        8\n                        0.9263\n                        0.8179\n                        0.6190\n                        0.8387\n                        0.7123\n                        0.6712\n                        0.6814\n            \n            \n                        9\n                        0.9085\n                        0.8018\n                        0.5610\n                        0.7419\n                        0.6389\n                        0.5876\n                        0.5952\n            \n            \n                        Mean\n                        0.9172\n                        0.8554\n                        0.6202\n                        0.7675\n                        0.6832\n                        0.6364\n                        0.6428\n            \n            \n                        SD\n                        0.0159\n                        0.0411\n                        0.0774\n                        0.0701\n                        0.0654\n                        0.0735\n                        0.0710\n            \n    \n\n\n\n# Train a voting classifier with all models in the library\nblender = blend_models()\n\n\n                    Accuracy        AUC        Recall        Prec.        F1        Kappa        MCC    \n                \n                        0\n                        0.8912\n                        0.0000\n                        0.7561\n                        0.5962\n                        0.6667\n                        0.6028\n                        0.6088\n            \n            \n                        1\n                        0.9404\n                        0.0000\n                        0.7805\n                        0.8000\n                        0.7901\n                        0.7554\n                        0.7554\n            \n            \n                        2\n                        0.9158\n                        0.0000\n                        0.6829\n                        0.7179\n                        0.7000\n                        0.6511\n                        0.6513\n            \n            \n                        3\n                        0.9474\n                        0.0000\n                        0.7805\n                        0.8421\n                        0.8101\n                        0.7796\n                        0.7804\n            \n            \n                        4\n                        0.9263\n                        0.0000\n                        0.7561\n                        0.7381\n                        0.7470\n                        0.7039\n                        0.7039\n            \n            \n                        5\n                        0.9158\n                        0.0000\n                        0.7073\n                        0.7073\n                        0.7073\n                        0.6581\n                        0.6581\n            \n            \n                        6\n                        0.9404\n                        0.0000\n                        0.7857\n                        0.8049\n                        0.7952\n                        0.7603\n                        0.7604\n            \n            \n                        7\n                        0.9474\n                        0.0000\n                        0.7619\n                        0.8649\n                        0.8101\n                        0.7797\n                        0.7818\n            \n            \n                        8\n                        0.9018\n                        0.0000\n                        0.7143\n                        0.6522\n                        0.6818\n                        0.6239\n                        0.6248\n            \n            \n                        9\n                        0.9190\n                        0.0000\n                        0.7317\n                        0.7143\n                        0.7229\n                        0.6755\n                        0.6755\n            \n            \n                        Mean\n                        0.9245\n                        0.0000\n                        0.7457\n                        0.7438\n                        0.7431\n                        0.6990\n                        0.7001\n            \n            \n                        SD\n                        0.0183\n                        0.0000\n                        0.0333\n                        0.0802\n                        0.0520\n                        0.0628\n                        0.0621\n            \n    \n\n\n\nblender_specific = blend_models(estimator_list = [tuned_lightgbm,tuned_xgboost,\n                                                 tuned_rf, tuned_et, tuned_dt], method = 'soft')\n\n\n                    Accuracy        AUC        Recall        Prec.        F1        Kappa        MCC    \n                \n                        0\n                        0.9404\n                        0.8833\n                        0.7805\n                        0.8000\n                        0.7901\n                        0.7554\n                        0.7554\n            \n            \n                        1\n                        0.9404\n                        0.9079\n                        0.7805\n                        0.8000\n                        0.7901\n                        0.7554\n                        0.7554\n            \n            \n                        2\n                        0.9474\n                        0.9345\n                        0.8293\n                        0.8095\n                        0.8193\n                        0.7885\n                        0.7886\n            \n            \n                        3\n                        0.9684\n                        0.8996\n                        0.8049\n                        0.9706\n                        0.8800\n                        0.8620\n                        0.8670\n            \n            \n                        4\n                        0.9509\n                        0.9361\n                        0.8049\n                        0.8462\n                        0.8250\n                        0.7964\n                        0.7968\n            \n            \n                        5\n                        0.9474\n                        0.9149\n                        0.8293\n                        0.8095\n                        0.8193\n                        0.7885\n                        0.7886\n            \n            \n                        6\n                        0.9544\n                        0.9541\n                        0.7857\n                        0.8919\n                        0.8354\n                        0.8091\n                        0.8113\n            \n            \n                        7\n                        0.9579\n                        0.9318\n                        0.7381\n                        0.9688\n                        0.8378\n                        0.8142\n                        0.8241\n            \n            \n                        8\n                        0.9298\n                        0.8511\n                        0.6905\n                        0.8056\n                        0.7436\n                        0.7032\n                        0.7060\n            \n            \n                        9\n                        0.9437\n                        0.8926\n                        0.7073\n                        0.8788\n                        0.7838\n                        0.7518\n                        0.7577\n            \n            \n                        Mean\n                        0.9481\n                        0.9106\n                        0.7751\n                        0.8581\n                        0.8124\n                        0.7824\n                        0.7851\n            \n            \n                        SD\n                        0.0102\n                        0.0289\n                        0.0458\n                        0.0639\n                        0.0354\n                        0.0412\n                        0.0422\n            \n    \n\n\n\nstacked_models = stack_models(estimator_list = [tuned_lightgbm,tuned_catboost,tuned_xgboost,\n                                                 tuned_rf, tuned_et, tuned_dt], meta_model = None, optimize = 'F1', fold = 5)\n\n\n                    Accuracy        AUC        Recall        Prec.        F1        Kappa        MCC    \n                \n                        0\n                        0.9368\n                        0.9035\n                        0.8293\n                        0.7556\n                        0.7907\n                        0.7536\n                        0.7547\n            \n            \n                        1\n                        0.9579\n                        0.9200\n                        0.8434\n                        0.8642\n                        0.8537\n                        0.8291\n                        0.8292\n            \n            \n                        2\n                        0.9509\n                        0.9184\n                        0.8434\n                        0.8235\n                        0.8333\n                        0.8045\n                        0.8046\n            \n            \n                        3\n                        0.9614\n                        0.9347\n                        0.8434\n                        0.8861\n                        0.8642\n                        0.8417\n                        0.8421\n            \n            \n                        4\n                        0.9315\n                        0.8787\n                        0.7561\n                        0.7654\n                        0.7607\n                        0.7207\n                        0.7208\n            \n            \n                        Mean\n                        0.9477\n                        0.9111\n                        0.8231\n                        0.8190\n                        0.8205\n                        0.7899\n                        0.7903\n            \n            \n                        SD\n                        0.0117\n                        0.0189\n                        0.0339\n                        0.0519\n                        0.0391\n                        0.0459\n                        0.0458\n            \n    \n\n\n\n\nOut of all the models created tuned_lightgbm is performing better on validation data\n\nevaluate_model(tuned_lightgbm)\n\n\n\n\n\n\nTest the model on the test data and choose the best performing model\n\n\nEvaluate Model\n\n# create funtion to return evaluation metrics\ndef evaluation_metrics(model):\n    check_model = predict_model(model, data = test)\n    print(metrics.confusion_matrix(check_model.churn,check_model.Label))\n    tn, fp, fn, tp = metrics.confusion_matrix(check_model.churn,check_model.Label).ravel()\n    Accuracy = round((tp+tn)/(tp+tn+fp+fn),3)\n    precision = round(tp/(tp+fp),3)\n    specificity = round(tn/(tn+fp),3)\n    recall = round(tp/(tp+fn),3)\n    print( f\"Accuracy:{Accuracy} , Specificity:{specificity}, Precision:{precision} , Recall:{recall}\")\n\n\ncheck tuned_lightgbm\n\nevaluation_metrics(tuned_lightgbm)\n\n[[142   1]\n [  5  19]]\nAccuracy:0.964 , Specificity:0.993, Precision:0.95 , Recall:0.792\n\n\n\n\ncheck tuned_catboost\n\nevaluation_metrics(tuned_catboost)\n\n[[141   2]\n [  5  19]]\nAccuracy:0.958 , Specificity:0.986, Precision:0.905 , Recall:0.792\n\n\n\n\ncheck tuned_xgboost\n\nevaluation_metrics(tuned_xgboost)\n\n[[141   2]\n [  5  19]]\nAccuracy:0.958 , Specificity:0.986, Precision:0.905 , Recall:0.792\n\n\n\n\ncheck tuned_rf\n\nevaluation_metrics(tuned_rf)\n\n[[141   2]\n [  6  18]]\nAccuracy:0.952 , Specificity:0.986, Precision:0.9 , Recall:0.75\n\n\n\n\ncheck tuned_et\n\nevaluation_metrics(tuned_et)\n\n[[143   0]\n [ 11  13]]\nAccuracy:0.934 , Specificity:1.0, Precision:1.0 , Recall:0.542\n\n\n\n\ncheck tuned_dt\n\nevaluation_metrics(tuned_dt)\n\n[[141   2]\n [  6  18]]\nAccuracy:0.952 , Specificity:0.986, Precision:0.9 , Recall:0.75\n\n\n\n\ncheck boosted_dt\n\nevaluation_metrics(boosted_dt)\n\n[[141   2]\n [ 10  14]]\nAccuracy:0.928 , Specificity:0.986, Precision:0.875 , Recall:0.583\n\n\n\n\ncheck bagged_dt\n\nevaluation_metrics(bagged_dt)\n\n[[139   4]\n [  6  18]]\nAccuracy:0.94 , Specificity:0.972, Precision:0.818 , Recall:0.75\n\n\n\n\ncheck blender\n\nevaluation_metrics(blender)\n\n[[143   0]\n [ 11  13]]\nAccuracy:0.934 , Specificity:1.0, Precision:1.0 , Recall:0.542\n\n\n\n\ncheck blender_specific\n\nevaluation_metrics(blender_specific)\n\n[[142   1]\n [  5  19]]\nAccuracy:0.964 , Specificity:0.993, Precision:0.95 , Recall:0.792\n\n\n\n\ncheck stacked_models\n\nevaluation_metrics(stacked_models)\n\n[[139   4]\n [  4  20]]\nAccuracy:0.952 , Specificity:0.972, Precision:0.833 , Recall:0.833\n\n\n\n\n\nFinalizing the Model and Metrics\n\nCompared multiple models to examine which algorithm is suitable for this dataset\nChose the five best performing algorithms and created models for them\nHyper parameter tuning was done to further improve the model performance\nEnsemble of models were created to check their performance on test data\nAll the tuned and esemble models were tested out on unseen data to finalize a model\n\n\n\nModel finalization\nMy recommendation for the final model is tuned_lightgbm. This is because the models predictions for churned customers is very high. From my experience in media Industry, it was observed that business users usually request for model explainability. This model’s Recall is slightly lower than stacked_models. stacked_model was not selected because it does not provide model interpretation. Also given similar performance it is better to go for simpler model.\n\n# Finalize the model\nfinal_model = finalize_model(tuned_lightgbm)\n\n\n# Feature importance using decision tree models\nplot_model(final_model, plot = 'feature')\n\n\n\n\n\n# Feature importance using Shap\ninterpret_model(final_model, plot = 'summary')\n\n\n\n\n\n# local interpretation\ninterpret_model(final_model, plot = 'reason', observation = 14)\n\n\n\n\n\n\n\n\n  Visualization omitted, Javascript library not loaded!\n  Have you run `initjs()` in this notebook? If this notebook was from another\n  user you must also trust this notebook (File -> Trust notebook). If you are viewing\n  this notebook on github the Javascript has been stripped for security. If you are using\n  JupyterLab this error is because a JupyterLab extension has not yet been written.\n\n \n\n\n\n# save the model\nsave_model(final_model,'tuned_lightgbm_dt_save_20201017')\n\nTransformation Pipeline and Model Succesfully Saved\n\n\n\n# load the model\nloaded_model = load_model('bagged_dt_save_20201017')\n\nTransformation Pipeline and Model Sucessfully Loaded\n\n\n\n\nPotential issues with deploying the model into production are:-\n\nData and model versioning As the amount of data is increasing every day, a mechanism to version data along with code needs to be established. This needs to be done without any cost overhead for storing multiple copies of the same data.\nTracking and storing of experiment results and artifacts efficiently. Data scientist should be able to tell which version of model is presently in production, data used for training, what are the evaluation metrics of the model at any given time.\nMonitoring of models in production for data drift - The behaviour of incoming data may change and will may differ from the data on which it was trained\nTaking care of CI/CD in production - As soon as a better performing model is finalized and commited, it should go to production in an automated fashion\nInstead of full-deployment of the models - Canary or Blue-Green deployment should be done. If model should be exposed to 10% to 15% of the population. If it performs well on a small population, then it should be rolled out for everyone.\nFeedback loops - The data used by the model for prediction going again into the training set\n\nThe following tools which can be used for the production issues mentioned above:- 1. Data and model versioning - Data Version Control (DVC) and MLOps by DVC 2. Tracking experiments - mlflow python package 3. Monitoring of models in production - Bi tools 4. Containerization - Docker and Kubernetes 5. CI/CD - Github, CircleCI, MLops 6. Deployment - Seldon core, Heroku 7. Canary / Bluegreen Deployment - AWS Sagemaker\n\n\nAppendix\nIf the business priorirty is to predict both churners and non-churners accurately then Accuracy. If the business priority is to identify churners then precision and recall. If the business priority is to predict non-churners (which is a very rare scenario) then it is specificity. The final model will be chosen as per business priority. If the business priority is :- 1. Predicting both churn and non-churning customers accurately then the model with highest accuracy will be chosen i.e - tuned_dt 2. Maximizing the proportion of churner identifications which are actually correct, then model with highest precision - tuned_dt 3. Identifying the Maximum proportion of actual churners then model with highest recall - bagged_dt (as this is simpler than blender_specific)\n\n# Data for DOE\ndoe = predict_model(loaded_model, data = telecom_churn)\nprint(metrics.confusion_matrix(doe.churn,doe.Label))\n\n[[2821   29]\n [ 135  348]]\n\n\n\ntn, fp, fn, tp = metrics.confusion_matrix(doe.churn,doe.Label).ravel()\nAccuracy = round((tp+tn)/(tp+tn+fp+fn),3)\nprecision = round(tp/(tp+fp),3)\nspecificity = round(tn/(tn+fp),3)\nrecall = round(tp/(tp+fn),3)\nprint( f\"Accuracy:{Accuracy} , Specificity:{specificity}, Precision:{precision} , Recall:{recall}\")\n\nAccuracy:0.951 , Specificity:0.99, Precision:0.923 , Recall:0.72\n\n\n\ndoe.shape\n\n(3333, 23)\n\n\n\ndoe.churn.value_counts()\n\n0    2850\n1     483\nName: churn, dtype: int64\n\n\n\n93/483\n\n0.19254658385093168"
  },
  {
    "objectID": "telecom_churn_prediction/telecom_churn_eda.html",
    "href": "telecom_churn_prediction/telecom_churn_eda.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "EDA on Telecom Churn Data\nThe objectives of this project are:-\n1. Perform exploratory analysis and extract insights from the dataset.\n2. Split the dataset into train/test sets and explain your reasoning.\n3. Build a predictive model to predict which customers are going to churn and discuss the reason why you choose a particular algorithm.\n4. Establish metrics to evaluate model performance.\n5. Discuss the potential issues with deploying the model into production\n\nImport the required libraries\n\n# python version # 3.8.2\nimport pandas as pd \nimport numpy as np \nimport os \nfrom pandas_profiling import ProfileReport\n\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n# option to display all columns\npd.set_option('display.max_columns', None)\n\n\n# Read the data\ntelecom_churn = pd.read_csv('../data/telecom_data/telecom.csv')\n\n\ntelecom_churn.head(10)\n\n\n\n\n\n  \n    \n      \n      state\n      account length\n      area code\n      phone number\n      international plan\n      voice mail plan\n      number vmail messages\n      total day minutes\n      total day calls\n      total day charge\n      total eve minutes\n      total eve calls\n      total eve charge\n      total night minutes\n      total night calls\n      total night charge\n      total intl minutes\n      total intl calls\n      total intl charge\n      customer service calls\n      churn\n    \n  \n  \n    \n      0\n      KS\n      128\n      415\n      382-4657\n      no\n      yes\n      25\n      265.1\n      110\n      45.07\n      197.4\n      99\n      16.78\n      244.7\n      91\n      11.01\n      10.0\n      3\n      2.70\n      1\n      False\n    \n    \n      1\n      OH\n      107\n      415\n      371-7191\n      no\n      yes\n      26\n      161.6\n      123\n      27.47\n      195.5\n      103\n      16.62\n      254.4\n      103\n      11.45\n      13.7\n      3\n      3.70\n      1\n      False\n    \n    \n      2\n      NJ\n      137\n      415\n      358-1921\n      no\n      no\n      0\n      243.4\n      114\n      41.38\n      121.2\n      110\n      10.30\n      162.6\n      104\n      7.32\n      12.2\n      5\n      3.29\n      0\n      False\n    \n    \n      3\n      OH\n      84\n      408\n      375-9999\n      yes\n      no\n      0\n      299.4\n      71\n      50.90\n      61.9\n      88\n      5.26\n      196.9\n      89\n      8.86\n      6.6\n      7\n      1.78\n      2\n      False\n    \n    \n      4\n      OK\n      75\n      415\n      330-6626\n      yes\n      no\n      0\n      166.7\n      113\n      28.34\n      148.3\n      122\n      12.61\n      186.9\n      121\n      8.41\n      10.1\n      3\n      2.73\n      3\n      False\n    \n    \n      5\n      AL\n      118\n      510\n      391-8027\n      yes\n      no\n      0\n      223.4\n      98\n      37.98\n      220.6\n      101\n      18.75\n      203.9\n      118\n      9.18\n      6.3\n      6\n      1.70\n      0\n      False\n    \n    \n      6\n      MA\n      121\n      510\n      355-9993\n      no\n      yes\n      24\n      218.2\n      88\n      37.09\n      348.5\n      108\n      29.62\n      212.6\n      118\n      9.57\n      7.5\n      7\n      2.03\n      3\n      False\n    \n    \n      7\n      MO\n      147\n      415\n      329-9001\n      yes\n      no\n      0\n      157.0\n      79\n      26.69\n      103.1\n      94\n      8.76\n      211.8\n      96\n      9.53\n      7.1\n      6\n      1.92\n      0\n      False\n    \n    \n      8\n      LA\n      117\n      408\n      335-4719\n      no\n      no\n      0\n      184.5\n      97\n      31.37\n      351.6\n      80\n      29.89\n      215.8\n      90\n      9.71\n      8.7\n      4\n      2.35\n      1\n      False\n    \n    \n      9\n      WV\n      141\n      415\n      330-8173\n      yes\n      yes\n      37\n      258.6\n      84\n      43.96\n      222.0\n      111\n      18.87\n      326.4\n      97\n      14.69\n      11.2\n      5\n      3.02\n      0\n      False\n    \n  \n\n\n\n\n\n\nCheck the Shape and Column types of the Dataframe\n\ntelecom_churn.shape\n\n(3333, 21)\n\n\n\ntelecom_churn.dtypes\n\nstate                      object\naccount length              int64\narea code                   int64\nphone number               object\ninternational plan         object\nvoice mail plan            object\nnumber vmail messages       int64\ntotal day minutes         float64\ntotal day calls             int64\ntotal day charge          float64\ntotal eve minutes         float64\ntotal eve calls             int64\ntotal eve charge          float64\ntotal night minutes       float64\ntotal night calls           int64\ntotal night charge        float64\ntotal intl minutes        float64\ntotal intl calls            int64\ntotal intl charge         float64\ncustomer service calls      int64\nchurn                        bool\ndtype: object\n\n\n\n\nExploratory Analysis\n\n# Format the column names, remove space and special characters in column names\ntelecom_churn.columns =  telecom_churn.columns.str.strip().str.lower().str.replace(' ', '_').str.replace('(', '').str.replace(')', '')\n\n\ntelecom_churn\n\n\n\n\n\n  \n    \n      \n      state\n      account_length\n      area_code\n      phone_number\n      international_plan\n      voice_mail_plan\n      number_vmail_messages\n      total_day_minutes\n      total_day_calls\n      total_day_charge\n      total_eve_minutes\n      total_eve_calls\n      total_eve_charge\n      total_night_minutes\n      total_night_calls\n      total_night_charge\n      total_intl_minutes\n      total_intl_calls\n      total_intl_charge\n      customer_service_calls\n      churn\n    \n  \n  \n    \n      0\n      KS\n      128\n      415\n      382-4657\n      no\n      yes\n      25\n      265.1\n      110\n      45.07\n      197.4\n      99\n      16.78\n      244.7\n      91\n      11.01\n      10.0\n      3\n      2.70\n      1\n      False\n    \n    \n      1\n      OH\n      107\n      415\n      371-7191\n      no\n      yes\n      26\n      161.6\n      123\n      27.47\n      195.5\n      103\n      16.62\n      254.4\n      103\n      11.45\n      13.7\n      3\n      3.70\n      1\n      False\n    \n    \n      2\n      NJ\n      137\n      415\n      358-1921\n      no\n      no\n      0\n      243.4\n      114\n      41.38\n      121.2\n      110\n      10.30\n      162.6\n      104\n      7.32\n      12.2\n      5\n      3.29\n      0\n      False\n    \n    \n      3\n      OH\n      84\n      408\n      375-9999\n      yes\n      no\n      0\n      299.4\n      71\n      50.90\n      61.9\n      88\n      5.26\n      196.9\n      89\n      8.86\n      6.6\n      7\n      1.78\n      2\n      False\n    \n    \n      4\n      OK\n      75\n      415\n      330-6626\n      yes\n      no\n      0\n      166.7\n      113\n      28.34\n      148.3\n      122\n      12.61\n      186.9\n      121\n      8.41\n      10.1\n      3\n      2.73\n      3\n      False\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      3328\n      AZ\n      192\n      415\n      414-4276\n      no\n      yes\n      36\n      156.2\n      77\n      26.55\n      215.5\n      126\n      18.32\n      279.1\n      83\n      12.56\n      9.9\n      6\n      2.67\n      2\n      False\n    \n    \n      3329\n      WV\n      68\n      415\n      370-3271\n      no\n      no\n      0\n      231.1\n      57\n      39.29\n      153.4\n      55\n      13.04\n      191.3\n      123\n      8.61\n      9.6\n      4\n      2.59\n      3\n      False\n    \n    \n      3330\n      RI\n      28\n      510\n      328-8230\n      no\n      no\n      0\n      180.8\n      109\n      30.74\n      288.8\n      58\n      24.55\n      191.9\n      91\n      8.64\n      14.1\n      6\n      3.81\n      2\n      False\n    \n    \n      3331\n      CT\n      184\n      510\n      364-6381\n      yes\n      no\n      0\n      213.8\n      105\n      36.35\n      159.6\n      84\n      13.57\n      139.2\n      137\n      6.26\n      5.0\n      10\n      1.35\n      2\n      False\n    \n    \n      3332\n      TN\n      74\n      415\n      400-4344\n      no\n      yes\n      25\n      234.4\n      113\n      39.85\n      265.9\n      82\n      22.60\n      241.4\n      77\n      10.86\n      13.7\n      4\n      3.70\n      0\n      False\n    \n  \n\n3333 rows × 21 columns\n\n\n\n\nprofile = ProfileReport(telecom_churn, title = \"Telecom Churn Report\")\n\n\nprofile.to_notebook_iframe()"
  },
  {
    "objectID": "scaled/01_test.html",
    "href": "scaled/01_test.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "currentBestTeam = \"\"\n\n\nscores = {currentBestTeam:0}\n\n\nscores\n\n{'': 0}\n\n\n\nscores[currentBestTeam]\n\n0\n\n\n\ncurrentBestTeam = 'python'\n\n\ncurrentBestTeam\n\n'python'\n\n\n\nimport pandas as pd\n\n\ndef isValidSubsequence(array, sequence):\n    # Write your code here.\n    j = 0\n    for i in range(len(array)):\n        if array[i] == sequence[j]:\n            j+=1\n        if j == len(sequence):\n            return True\n    return False\n\n\narray = [6,1,22,25,-1,8,10]\nsequence = [1,6,-1,10]\n\n\nisValidSubsequence(array, sequence)\n\nFalse"
  },
  {
    "objectID": "scaled/blog_post.html",
    "href": "scaled/blog_post.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Many of us are familiar with common applications of Data Science and Machine learning, such as churn prediction, recommendation engines, and customer segmentation.\nSometimes you will come across a use case that astonishes you and make you wonder, “Is it possible with DS and ML?” I came across such a use case recently and I was instantly hooked.\nPredicting Poverty using ML\nA lot of developing countries cannot afford regular surveys which estimate household income or consumption. When COVID hit and people were unable to find work, how can a government provide support to the needy when survey data is not available? How to decide to who support should be provided?\nHave you heard of using night lights as a proxy for regional development to provide social support?\nDo you know how mobile phone interactions are used to identify people living in extreme poverty?\nHow classification and object detection on satellite images are used to identify extreme poverty?\nDo you know how advanced techniques like Reinforcement learning are used in poverty detection?\nI also didn’t know about these things a few months back. I created a website to collect all the relevant information in one place. The website includes summaries of research papers, available datasets, information on geospatial data processing, and satellite imagery.\nI plan to expand the website as a comprehensive resource for using data science and machine learning to combat poverty.\nIf you are interested in this topic, feel free to check out my website and let me know if you have any information to contribute that could aid in poverty eradication using Data Science and Machine Learning."
  },
  {
    "objectID": "data_privacy/basic_privacy_approaches.html",
    "href": "data_privacy/basic_privacy_approaches.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Data which could be used separately or in combination with other information to identify a person or small group of persons\n\nPersonally Identifiable Information (PII)\nPerson-Related data\nProprietary and Confidential data\n\n\n\n\nIdentifying PII in images and text\n\n\n\n\nWe can have a classification system for the data.\nUse classification as an initial step for documentation\nToolkit for documenting Data - Data Cards\nFramework for documenting Models - Model Cards\nTool for Data Management\n\nDocumenting data Collection\nDocumenting Data Quality\nDocumenting Data Security\nDocumenting Data Privacy\nDocumenting Data Descriptions\nDocumenting Data Statistics\nDocumenting consent\n\nTrack Data Lineage\nData version control\n\n\n\n\n\n\n\nIt is a technique that allows us to use “Pseudonyms” instead of real names and data\n\n\n\n\npseudonymization approaches\n\n\n\n\n\nHow a linkage attack works\n\n\n\nLinking is a primary attack vector to determine the identity of an individual.\n\n\n\n\nAdvantages and Disadvantages of Pseudonymization\n\n\n\nIf the data will be only used intenally by a small group of individuals who may require privileged access, then pseudonymization might be a good fit for the needs\nTools for pseudonymization\n\nKIProtect’s Kodex\nFormat preserving library by Mysto\nMicrosoft’s Presidio\nPrivate Input Masked output based on GO"
  },
  {
    "objectID": "data_privacy/differential_privacy.html",
    "href": "data_privacy/differential_privacy.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Anonymize is to remove identifying information from data so that original source cannot be known\nPrivacy needs to be understood as a gradient and not a “on” or “off” thing \n\n\n\n\n\n\nHow Differential Privacy works\n\n\nA process A is epsilon-differentially private if for all databases D1 and D2 which differ in only one individual:\n\n\n\nDefinition\n\n\nThis must be true for all possible outputs O. If epsilon is very close to 0, then exponential of epsilon is very close to 1, so the probabilities are very similar. The bigger epsilon is, the more the probabilities can differ.\n\nIt is a rigorous and scientific definition of privacy-sensitive information release - that defines a limit or bounds on the amount of privacy loss you can have when you release information\nThis method focuses on the process rather than the result\nDifferential privacy shifts to thinking about what guarantees a particular algorithm can provide by measuring the information that is being continuously released via the algorithm itself.\nWhy is differential privacy special:-\n\nNo longer need attack modeling\nWe can quantify the privacy loss\nWe can compose multiple mechanisms - We can add the epsilon of multiple queries to arrive at the privacy loss for all the queries together.we can allocate budget for the user queries\n\nSensitivity measures the maximum change in the query result based on change in the underlying dataset.\n\n\n\n\n\n\nFree Udacity course\n\n\n\npython package pydp Tutorials using pydp python package opendp spark package PipelineDP Tensorflow Privacy\n\n\n\nBeautiful book with lot of plots and code\n\n\n\nDifferential Privacy blog by Damien Desfontaines\n\n\n\nMicrosoft session on privacy preserving ML"
  },
  {
    "objectID": "blogs.html",
    "href": "blogs.html",
    "title": "Blogs",
    "section": "",
    "text": "MusicLM: Generating music from text\nEthics and Bias in Data Science\nMitigating Bias\nWhat is a DMP and how to create it\nWhy Data Lakes become Data Swamps\nGo and the challenge to Arificial General Intelligence\nAnamoly Detection and Interpretability\nML Models: From Jupyter notebook to a database\nData Lakehouse\nDruid - Fast Analytics on Batch and Real-time Data\nApplications of Data science in Netflix\nFighting cancer with Data and Machine Learning\nBig Data - The backbone of Aadhar\nConverting Business problems to Data Science Problems\nHow to spot Data lies\nSimpson’s Paradox"
  },
  {
    "objectID": "recommender_systems/matrix_factorization.html",
    "href": "recommender_systems/matrix_factorization.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Recommendation System Presentations\n\nIntroduction to Recommendation Systems\nIndustrial Use Cases of Recommenders\nChallenges to Recommenders\nEvaluating Recommenders\nAnalytics for Recommenders\nNon-Personalized Recommenders\nBaseline Recommenders\nData for building Recommenders\nCollaborative Filtering\ncontent Filtering\nMatrix Factorization\nNeural Collaborative Filtering\nVariational Autoencoders for Collaborative Filtering\nDKN for News Recommendation\nCase Study - Designing a Recommendation System for a App"
  },
  {
    "objectID": "linear_algebra/resources.html",
    "href": "linear_algebra/resources.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Khan Academy Videos"
  },
  {
    "objectID": "data_quality/setup_datasource.html",
    "href": "data_quality/setup_datasource.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Use this notebook to configure a new pandas Datasource and add it to your project.\n\nimport os\nos.chdir('/home/thulasiram/personal/going_deep_and_wide/GiveDirectly/gx_tutorials/great_expectations')\n\n\nimport great_expectations as gx\nfrom great_expectations.cli.datasource import sanitize_yaml_and_save_datasource, check_if_datasource_name_exists\ncontext = gx.get_context()\n\n\n\nIf you are new to Great Expectations Datasources, you should check out our how-to documentation\nMy configuration is not so simple - are there more advanced options? Glad you asked! Datasources are versatile. Please see our How To Guides!\nGive your datasource a unique name:\n\ndatasource_name = \"data_quality_demo\"\n\n\n\nHere we are creating an example configuration. The configuration contains an InferredAssetFilesystemDataConnector which will add a Data Asset for each file in the base directory you provided. It also contains a RuntimeDataConnector which can accept filepaths. This is just an example, and you may customize this as you wish!\nAlso, if you would like to learn more about the DataConnectors used in this configuration, including other methods to organize assets, handle multi-file assets, name assets based on parts of a filename, please see our docs on InferredAssetDataConnectors and RuntimeDataConnectors.\n\nexample_yaml = f\"\"\"\nname: {datasource_name}\nclass_name: Datasource\nexecution_engine:\n  class_name: PandasExecutionEngine\ndata_connectors:\n  default_inferred_data_connector_name:\n    class_name: InferredAssetFilesystemDataConnector\n    base_directory: ../data\n    default_regex:\n      group_names:\n        - data_asset_name\n      pattern: (.*)\n  default_runtime_data_connector_name:\n    class_name: RuntimeDataConnector\n    assets:\n      my_runtime_asset_name:\n        batch_identifiers:\n          - runtime_batch_identifier_name\n\"\"\"\nprint(example_yaml)\n\n\nname: data_quality_demo\nclass_name: Datasource\nexecution_engine:\n  class_name: PandasExecutionEngine\ndata_connectors:\n  default_inferred_data_connector_name:\n    class_name: InferredAssetFilesystemDataConnector\n    base_directory: ../data\n    default_regex:\n      group_names:\n        - data_asset_name\n      pattern: (.*)\n  default_runtime_data_connector_name:\n    class_name: RuntimeDataConnector\n    assets:\n      my_runtime_asset_name:\n        batch_identifiers:\n          - runtime_batch_identifier_name"
  },
  {
    "objectID": "data_quality/setup_datasource.html#save-your-datasource-configuration",
    "href": "data_quality/setup_datasource.html#save-your-datasource-configuration",
    "title": "My Datascience Journey",
    "section": "Save Your Datasource Configuration",
    "text": "Save Your Datasource Configuration\nHere we will save your Datasource in your Data Context once you are satisfied with the configuration. Note that overwrite_existing defaults to False, but you may change it to True if you wish to overwrite. Please note that if you wish to include comments you must add them directly to your great_expectations.yml.\n\nsanitize_yaml_and_save_datasource(context, example_yaml, overwrite_existing=False)\ncontext.list_datasources()\n\n[{'execution_engine': {'class_name': 'PandasExecutionEngine',\n   'module_name': 'great_expectations.execution_engine'},\n  'module_name': 'great_expectations.datasource',\n  'class_name': 'Datasource',\n  'name': 'data_quality_demo',\n  'data_connectors': {'default_inferred_data_connector_name': {'default_regex': {'group_names': ['data_asset_name'],\n     'pattern': '(.*)'},\n    'base_directory': '../data',\n    'module_name': 'great_expectations.datasource.data_connector',\n    'class_name': 'InferredAssetFilesystemDataConnector'},\n   'default_runtime_data_connector_name': {'module_name': 'great_expectations.datasource.data_connector',\n    'class_name': 'RuntimeDataConnector',\n    'assets': {'my_runtime_asset_name': {'module_name': 'great_expectations.datasource.data_connector.asset',\n      'class_name': 'Asset',\n      'batch_identifiers': ['runtime_batch_identifier_name']}}}}}]"
  },
  {
    "objectID": "data_quality/four_steps_to_data_quality.html",
    "href": "data_quality/four_steps_to_data_quality.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Great Expectations is a python package which helps ensuring data quality in four steps.\n\nSetup the Data context\nConnect to data\nCreate Expectations\nValidate Data\n\n\n\n\nFollow these four steps\n\n\n\n\nIn this Demo we will be using NYC taxi data to show how we can ensure the quality of data in production. This is an open data set which is updated every month. Each record in the data corresponds to one taxi ride and contains information such as the pick-up and drop-off location, the payment amount, and the number of passengers, among others.\nWe will be using two CSV files, each with 10,000 row sample of taxi trip records. A sample for January 2019 and a sample for February 2019.\nFor purposes of this tutorial, we are treating the January 2019 taxi data as our “current” data, and the February 2019 taxi data as “future” data that we have not yet looked at. We will use Great Expectations to build a profile of the January data and then use that profile to check for any unexpected data quality issues in the February data. In a real-life scenario, this would ensure that any problems with the February data would be caught (so it could be dealt with) before the February data is used in a production application!\n\n\n\n\n\n\n\n\n\n\n\nExplore Expecatations\n\n\n\n\nExplore validations"
  },
  {
    "objectID": "data_quality/run_checkpoint.html",
    "href": "data_quality/run_checkpoint.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Create Checkpoint\nUse this notebook to configure a new Checkpoint and add it to your project:\nCheckpoint Name: data_quality_demo_checkpoint\n\nimport os\nos.chdir('/home/thulasiram/personal/going_deep_and_wide/GiveDirectly/gx_tutorials/great_expectations')\n\n\nfrom ruamel.yaml import YAML\nimport great_expectations as gx\nfrom pprint import pprint\n\nyaml = YAML()\ncontext = gx.get_context()\n\n\n\nCreate a Checkpoint Configuration\nIf you are new to Great Expectations or the Checkpoint feature, you should start with SimpleCheckpoint because it includes default configurations like a default list of post validation actions.\nIn the cell below we have created a sample Checkpoint configuration using your configuration and SimpleCheckpoint to run a single validation of a single Expectation Suite against a single Batch of data.\nTo keep it simple, we are just choosing the first available instance of each of the following items you have configured in your Data Context: * Datasource * DataConnector * DataAsset * Partition * Expectation Suite\nOf course this is purely an example, you may edit this to your heart’s content.\nMy configuration is not so simple - are there more advanced options?\nGlad you asked! Checkpoints are very versatile. For example, you can validate many Batches in a single Checkpoint, validate Batches against different Expectation Suites or against many Expectation Suites, control the specific post-validation actions based on Expectation Suite / Batch / results of validation among other features. Check out our documentation on Checkpoints for more details and for instructions on how to implement other more advanced features including using the Checkpoint class: - https://docs.greatexpectations.io/docs/reference/checkpoints_and_actions - https://docs.greatexpectations.io/docs/guides/validation/checkpoints/how_to_create_a_new_checkpoint - https://docs.greatexpectations.io/docs/guides/validation/checkpoints/how_to_configure_a_new_checkpoint_using_test_yaml_config\n\nmy_checkpoint_name = \"data_quality_demo_checkpoint\" # This was populated from your CLI command.\n\nyaml_config = f\"\"\"\nname: {my_checkpoint_name}\nconfig_version: 1.0\nclass_name: SimpleCheckpoint\nrun_name_template: \"%Y%m%d-%H%M%S-my-run-name-template\"\nvalidations:\n  - batch_request:\n      datasource_name: data_quality_demo\n      data_connector_name: default_inferred_data_connector_name\n      data_asset_name: yellow_tripdata_sample_2019-02.csv\n      data_connector_query:\n        index: -1\n    expectation_suite_name: data_quality_expectation_demo\n\"\"\"\nprint(yaml_config)\n\n\nname: data_quality_demo_checkpoint\nconfig_version: 1.0\nclass_name: SimpleCheckpoint\nrun_name_template: \"%Y%m%d-%H%M%S-my-run-name-template\"\nvalidations:\n  - batch_request:\n      datasource_name: data_quality_demo\n      data_connector_name: default_inferred_data_connector_name\n      data_asset_name: yellow_tripdata_sample_2019-02.csv\n      data_connector_query:\n        index: -1\n    expectation_suite_name: data_quality_expectation_demo\n\n\n\n\n\nCustomize Your Configuration\nThe following cells show examples for listing your current configuration. You can replace values in the sample configuration with these values to customize your Checkpoint.\n\n# Run this cell to print out the names of your Datasources, Data Connectors and Data Assets\npprint(context.get_available_data_asset_names())\n\n{'data_quality_demo': {'default_inferred_data_connector_name': ['yellow_tripdata_sample_2019-01.csv',\n                                                                'yellow_tripdata_sample_2019-02.csv'],\n                       'default_runtime_data_connector_name': ['my_runtime_asset_name']}}\n\n\n\ncontext.list_expectation_suite_names()\n\n['data_quality_expectation_demo']\n\n\n\n\nTest Your Checkpoint Configuration\nHere we will test your Checkpoint configuration to make sure it is valid.\nThis test_yaml_config() function is meant to enable fast dev loops. If your configuration is correct, this cell will show a message that you successfully instantiated a Checkpoint. You can continually edit your Checkpoint config yaml and re-run the cell to check until the new config is valid.\nIf you instead wish to use python instead of yaml to configure your Checkpoint, you can use context.add_checkpoint() and specify all the required parameters.\n\nmy_checkpoint = context.test_yaml_config(yaml_config=yaml_config)\n\nAttempting to instantiate class from config...\n    Instantiating as a SimpleCheckpoint, since class_name is SimpleCheckpoint\n    Successfully instantiated SimpleCheckpoint\n\n\nCheckpoint class name: SimpleCheckpoint\n\n\n\n\nReview Your Checkpoint\nYou can run the following cell to print out the full yaml configuration. For example, if you used SimpleCheckpoint this will show you the default action list.\n\nprint(my_checkpoint.get_config(mode=\"yaml\"))\n\nname: data_quality_demo_checkpoint\nconfig_version: 1.0\ntemplate_name:\nmodule_name: great_expectations.checkpoint\nclass_name: Checkpoint\nrun_name_template: '%Y%m%d-%H%M%S-my-run-name-template'\nexpectation_suite_name:\nbatch_request: {}\naction_list:\n  - name: store_validation_result\n    action:\n      class_name: StoreValidationResultAction\n  - name: store_evaluation_params\n    action:\n      class_name: StoreEvaluationParametersAction\n  - name: update_data_docs\n    action:\n      class_name: UpdateDataDocsAction\n      site_names: []\nevaluation_parameters: {}\nruntime_configuration: {}\nvalidations:\n  - batch_request:\n      datasource_name: data_quality_demo\n      data_connector_name: default_inferred_data_connector_name\n      data_asset_name: yellow_tripdata_sample_2019-02.csv\n      data_connector_query:\n        index: -1\n    expectation_suite_name: data_quality_expectation_demo\nprofilers: []\nge_cloud_id:\nexpectation_suite_ge_cloud_id:\n\n\n\n\n\nAdd Your Checkpoint\nRun the following cell to save this Checkpoint to your Checkpoint Store.\n\ncontext.add_checkpoint(**yaml.load(yaml_config))\n\n{\n  \"action_list\": [\n    {\n      \"name\": \"store_validation_result\",\n      \"action\": {\n        \"class_name\": \"StoreValidationResultAction\"\n      }\n    },\n    {\n      \"name\": \"store_evaluation_params\",\n      \"action\": {\n        \"class_name\": \"StoreEvaluationParametersAction\"\n      }\n    },\n    {\n      \"name\": \"update_data_docs\",\n      \"action\": {\n        \"class_name\": \"UpdateDataDocsAction\",\n        \"site_names\": []\n      }\n    }\n  ],\n  \"batch_request\": {},\n  \"class_name\": \"Checkpoint\",\n  \"config_version\": 1.0,\n  \"evaluation_parameters\": {},\n  \"module_name\": \"great_expectations.checkpoint\",\n  \"name\": \"data_quality_demo_checkpoint\",\n  \"profilers\": [],\n  \"run_name_template\": \"%Y%m%d-%H%M%S-my-run-name-template\",\n  \"runtime_configuration\": {},\n  \"validations\": [\n    {\n      \"batch_request\": {\n        \"datasource_name\": \"data_quality_demo\",\n        \"data_connector_name\": \"default_inferred_data_connector_name\",\n        \"data_asset_name\": \"yellow_tripdata_sample_2019-02.csv\",\n        \"data_connector_query\": {\n          \"index\": -1\n        }\n      },\n      \"expectation_suite_name\": \"data_quality_expectation_demo\"\n    }\n  ]\n}\n\n\n\n\nRun Your Checkpoint & Open Data Docs(Optional)\nYou may wish to run the Checkpoint now and review its output in Data Docs. If so uncomment and run the following cell.\n\ncontext.run_checkpoint(checkpoint_name=my_checkpoint_name)\ncontext.open_data_docs()"
  },
  {
    "objectID": "data_quality/generate_expectation.html",
    "href": "data_quality/generate_expectation.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "This process helps you avoid writing lots of boilerplate when authoring suites by allowing you to select columns and other factors that you care about and letting a profiler write some candidate expectations for you to adjust.\nExpectation Suite Name: data_quality_expectation_demo\n\nimport os\nos.chdir('/home/thulasiram/personal/going_deep_and_wide/GiveDirectly/gx_tutorials/great_expectations')\n\n\nimport datetime\n\nimport pandas as pd\n\nimport great_expectations as gx\nimport great_expectations.jupyter_ux\nfrom great_expectations.core.batch import BatchRequest\nfrom great_expectations.checkpoint import SimpleCheckpoint\nfrom great_expectations.exceptions import DataContextError\n\ncontext = gx.data_context.DataContext()\n\nbatch_request = {'datasource_name': 'data_quality_demo', 'data_connector_name': 'default_inferred_data_connector_name', 'data_asset_name': 'yellow_tripdata_sample_2019-01.csv', 'limit': 1000}\n\nexpectation_suite_name = \"data_quality_expectation_demo\"\n\nvalidator = context.get_validator(\n    batch_request=BatchRequest(**batch_request),\n    expectation_suite_name=expectation_suite_name\n)\ncolumn_names = [f'\"{column_name}\"' for column_name in validator.columns()]\nprint(f\"Columns: {', '.join(column_names)}.\")\nvalidator.head(n_rows=5, fetch_all=False)\n\n2022-12-23T15:35:54+0530 - INFO - Great Expectations logging enabled at 20 level by JupyterUX module.\n\n\n\n\n\nColumns: \"vendor_id\", \"pickup_datetime\", \"dropoff_datetime\", \"passenger_count\", \"trip_distance\", \"rate_code_id\", \"store_and_fwd_flag\", \"pickup_location_id\", \"dropoff_location_id\", \"payment_type\", \"fare_amount\", \"extra\", \"mta_tax\", \"tip_amount\", \"tolls_amount\", \"improvement_surcharge\", \"total_amount\", \"congestion_surcharge\".\n\n\n\n\n\n\n\n\n\n  \n    \n      \n      vendor_id\n      pickup_datetime\n      dropoff_datetime\n      passenger_count\n      trip_distance\n      rate_code_id\n      store_and_fwd_flag\n      pickup_location_id\n      dropoff_location_id\n      payment_type\n      fare_amount\n      extra\n      mta_tax\n      tip_amount\n      tolls_amount\n      improvement_surcharge\n      total_amount\n      congestion_surcharge\n    \n  \n  \n    \n      0\n      1\n      2019-01-15 03:36:12\n      2019-01-15 03:42:19\n      1\n      1.0\n      1\n      N\n      230\n      48\n      1\n      6.5\n      0.5\n      0.5\n      1.95\n      0.0\n      0.3\n      9.75\n      NaN\n    \n    \n      1\n      1\n      2019-01-25 18:20:32\n      2019-01-25 18:26:55\n      1\n      0.8\n      1\n      N\n      112\n      112\n      1\n      6.0\n      1.0\n      0.5\n      1.55\n      0.0\n      0.3\n      9.35\n      0.0\n    \n    \n      2\n      1\n      2019-01-05 06:47:31\n      2019-01-05 06:52:19\n      1\n      1.1\n      1\n      N\n      107\n      4\n      2\n      6.0\n      0.0\n      0.5\n      0.00\n      0.0\n      0.3\n      6.80\n      NaN\n    \n    \n      3\n      1\n      2019-01-09 15:08:02\n      2019-01-09 15:20:17\n      1\n      2.5\n      1\n      N\n      143\n      158\n      1\n      11.0\n      0.0\n      0.5\n      3.00\n      0.0\n      0.3\n      14.80\n      NaN\n    \n    \n      4\n      1\n      2019-01-25 18:49:51\n      2019-01-25 18:56:44\n      1\n      0.8\n      1\n      N\n      246\n      90\n      1\n      6.5\n      1.0\n      0.5\n      1.65\n      0.0\n      0.3\n      9.95\n      0.0"
  },
  {
    "objectID": "data_quality/generate_expectation.html#next-steps",
    "href": "data_quality/generate_expectation.html#next-steps",
    "title": "My Datascience Journey",
    "section": "Next steps",
    "text": "Next steps\nAfter you review this initial Expectation Suite in Data Docs you should edit this suite to make finer grained adjustments to the expectations. This can be done by running great_expectations suite edit data_quality_expectation_demo."
  },
  {
    "objectID": "dsa/selection_sort.html",
    "href": "dsa/selection_sort.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Selection Sort\n\n# O(n**2) Time | O(1) space\ndef selectionSort(array):\n    currentIdx = 0 #starting position of the unsorted array\n\n    while currentIdx < len(array) - 1:\n        smallestIdx = currentIdx\n\n        for i in range(currentIdx+1,len(array)):\n            if array[smallestIdx] > array[i]: # Find the smallest number in the array\n                smallestIdx = i\n        swap(currentIdx,smallestIdx,array) # Get the smallest number to the start of the array\n        currentIdx += 1 # Shift the position of the unsorted array\n    return array\n\ndef swap(i,j,array): # Function to swap the numbers in the array\n    array[i],array[j] = array[j],array[i]\n\n\nselectionSort([10,6,9,3,6,7,3,2,7,5])\n\n[2, 3, 3, 5, 6, 6, 7, 7, 9, 10]"
  },
  {
    "objectID": "dsa/quick_sort.html",
    "href": "dsa/quick_sort.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Quick Sort\n\n# Best and average case scenario is nlogn\n# worst case scenario is O(n**2)\n\ndef quick_sort(sequence):\n    \n    if len(sequence) <= 1: # return sequence if it 1 or less than 1\n        return sequence\n\n    lower = []\n    higher = []\n\n    pivot = sequence.pop() # select the last element as pivot. We can select any element we want.\n\n    # create two arrays which are less than and equal to or higher than pivot\n    for item in sequence:\n        if item < pivot:\n            lower.append(item) \n\n        else:\n            higher.append(item)\n    \n    # use recursion on each lower and higher array for sorting\n    # concatenante the results\n    return quick_sort(lower) + [pivot] + quick_sort(higher)   \n\n\n# Check if the function is working correctly\nquick_sort([5,8,3,9,5,0,9,387,9486,9,4,56])\n\n[0, 3, 4, 5, 5, 8, 9, 9, 9, 56, 387, 9486]"
  },
  {
    "objectID": "dsa/bst.html",
    "href": "dsa/bst.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Initiate the class\nclass BST:\n    def __init__(self,value):\n        self.value = value\n        self.left = None\n        self.right = None\n\n\n# Average O(log(n)) time | O(1) space\n# Worst O(n) time | O(1) space - In case of a tree with single branch\n    def insert(self,value):\n        currentNode = self\n        while True:      \n            if value < currentNode.value:\n                # If the value to be inserted is less then the current node's value\n                # and the left node is None, then insert the value there\n                # If the left node is not none, the current node is the left node  \n                if currentNode.left is None:\n                    currentNode.left = BST(value)\n                    break\n                else:\n                    currentNode = currentNode.left\n\n            else:\n                # This is when the value to be inserted is greater than the current node's value\n                if currentNode.right is None:\n                    currentNode.right = BST(value)\n                    break\n                else:\n                    currentNode = currentNode.right\n        return self\n\n\n\n# Average O(log(n)) time | O(1) space\n# Worst O(n) time | O(1) space\n    def contains(self,value):\n        currentNode = self\n        while currentNode is not None:\n            if value < currentNode.value:\n                currentNode = currentNode.left\n            elif value > currentNode.value:\n                currentNode = currentNode.right\n            else:\n                return True\n        return False\n\n\n\n# Average O(log(n)) time | O(1) space\n# Worst O(n) time | O(1) space\n    def remove(self,value):\n        currentNode = self\n        while currentNode is not None:\n            if value < currentNode.value:\n                parentNode = currentNode\n                currentNode = currentNode.left\n            elif value > currentNode.value:\n                parentNode = currentNode\n                currentNode = currentNode.right\n            else:\n                # When value is equal to currentNode.value\n                if currentNode.left is not None and currentNode.right is not None:\n                    # when both left and right nodes are not None\n                    # find the node with the minimum value in the right subtree\n                    # and replace the value of the node with the minimum value\n                    # remove the minimum value from the right subtree\n                    currentNode.value = currentNode.right.getMinValue()\n                    currentNode.right.remove(currentNode.value, currentNode)\n                elif parentNode is None:\n                    # When currentNode is the root node\n                    if currentNode.left is not None:                                                                        \n                        currentNode.value = currentNode.left.value\n                        currentNode.right = currentNode.left.right\n                        currentNode.left = currentNode.left.left\n                    elif currentNode.right is not None:\n                        currentNode.value = currentNode.right.value\n                        currentNode.left = currentNode.right.left\n                        currentNode.right = currentNode.right.right\n                    else:\n                        currentNode.value = None\n                elif parentNode.left = currentNode:\n                    parentNode.left = currentNode.left if currentNode.left is not None else currentNode.right\n                elif parentNode.right = currentNode:\n                    parentNode.right = currentNode.left if currentNode.left is not None else currentNode.right\n                break\n        return self\n\n    def getMinValue(self):\n        currentNode = self\n        while currentNode.left is not None:\n            currentNode = currentNode.left\n        return currentNode.value"
  },
  {
    "objectID": "dsa/bubble_sort.html",
    "href": "dsa/bubble_sort.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Bubble Sort\n\ndef bubbleSort(array):\n    isSorted = False \n    # After one iteration, the last element in the array will be in the correct position.\n    # `counter` is to Keep track of how many elements are there in the correct position\n    counter = 0 \n\n    while not isSorted:\n        isSorted = True\n        for i in range(len(array) - 1 - counter): # Don't go till last element\n            if array[i] > array[i+1]:                \n                swap(array,i,i+1)\n                isSorted = False # If a swap is done, then the array may not be sorted\n        counter += 1 # Increment the country\n    return array\n\n# Helper functiont to swap the array\ndef swap(array,j,k):\n    array[j],array[k] = array[k],array[j]\n\n\nbubbleSort([8,5,2,9,5,6,3])\n\n[2, 3, 5, 5, 6, 8, 9]"
  },
  {
    "objectID": "dsa/Binary_search.html",
    "href": "dsa/Binary_search.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Binary Search\n\nBinary search using Iterative Method\n\n# Time complexity O(log(n)) and space complexity is O(1)\n### Using three pointers - left, right and middle\n### We keep adjusting the three pointers and quit when \n### left pointer crosses the right pointer\n\ndef binarySearch(array, target):\n    left = 0\n    right = len(array) - 1    \n\n    while left <= right:\n        middle = (left+right) // 2\n        potentialMatch = array[middle]\n\n        if target == potentialMatch:\n            return middle\n\n        elif target < potentialMatch:\n            right = middle - 1\n\n        else:\n            left = middle + 1\n\n    return -1\n\n\narray = [0,1,21,33,45,61,71,72,73]\ntarget = 33\n\n\nbinarySearch(array=array,target=target)\n\n3\n\n\n\n\nBinary search using Recursion\n\n# O(log(n)) time complexity ! O(log(n)) space complexity\n\ndef binarySearch(array,target):\n    return binarySearchHelper(array,target,0,len(array)-1)\n\ndef binarySearchHelper(array,target,left,right):\n    if left > right:\n        return -1\n\n    middle = (left+right) // 2\n    potentialMatch = array[middle]\n    if target == potentialMatch:\n        return middle\n    elif target < potentialMatch:\n        return binarySearchHelper(array,target,left,middle-1)\n    else:\n        return binarySearchHelper(array,target,middle+1,right)  \n\n\nbinarySearch(array,target)\n\n3"
  },
  {
    "objectID": "dsa/find_closest_value_bst.html",
    "href": "dsa/find_closest_value_bst.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Find Closest Value in BST\nWrite a function that takes in a BST and a target integer value and returns the closest value to that target in the BST.\ndef findClosestValueInBst(tree, target):\n    # Initiate the closest value to infinity\n    closestValue = float(\"inf\")\n    currentNode = tree\n\n    while currentNode is not None:\n        # If the current node's value is closer to the target value then the closesetValue\n        # then update the closestValue\n        if abs(currentNode.value - target) < abs(target - closestValue):\n            closestValue = currentNode.value\n        \n        if currentNode.value < target:\n            currentNode = currentNode.right\n\n        elif currentNode.value > target:\n            currentNode = currentNode.left\n\n        else:\n            # Break from the loop if currenNode is equal to the target value\n            break\n            \n    return closestValue"
  },
  {
    "objectID": "dsa/merge_sort.html",
    "href": "dsa/merge_sort.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "# Time complexity is O(nlogn)\n\ndef merge_sort(sequence):\n    \n    if len(sequence) > 1: # run if the sequence is greater than 1\n\n        left_array = sequence[:len(sequence)//2]\n        right_array = sequence[len(sequence)//2:]\n\n        # Recursively split the array\n        merge_sort(left_array)  \n        merge_sort(right_array)\n\n        # Merge the array\n        i = 0\n        j = 0\n        k = 0\n\n        while i < len(left_array) and j < len(right_array):\n            if left_array[i] < right_array[j]:\n                sequence[k] = left_array[i]\n                i += 1\n\n            else :            \n                sequence[k] = right_array[j]\n                j += 1\n\n            k += 1\n\n        # If the lenght of the left array is greater than right array,\n        # append the elements in left array to the sequence\n        while i < len(left_array):\n            sequence[k] = left_array[i]\n            i += 1\n            k += 1\n\n        \n        # If the lenght of the right array is greater than left array,\n        # append the elements in right array to the sequence\n\n        while j < len(right_array):\n            sequence[k] = right_array[j]\n            j += 1\n            k += 1   \n    \n    return sequence\n\n\n# Check if the function is working correct\narray_test = [5,8,3,9,5,0,9,387,9486,9,4,56]\n\n\nmerge_sort(array_test)\n\n[0, 3, 4, 5, 5, 8, 9, 9, 9, 56, 387, 9486]"
  },
  {
    "objectID": "dsa/insertion_sort.html",
    "href": "dsa/insertion_sort.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Insertion Sort\n\n# O(n**2) time | O(1) space\ndef insertionSort(array):\n    for i in range(1,len(array)):\n        j = i\n\n        while j > 0 and array[j] < array[j-1]: \n            swap(array,j,j-1)\n            j -= 1\n    return array\n\ndef swap(array,m,n):\n    array[m],array[n] = array[n],array[m]\n\n\ninsertionSort([9,5,3,7,6,8,3,9,1,0,4,3])\n\n[0, 1, 3, 3, 3, 4, 5, 6, 7, 8, 9, 9]"
  },
  {
    "objectID": "ml_algorithms/gradient_boosting.html",
    "href": "ml_algorithms/gradient_boosting.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "In contrast to Adaboost which uses weights as a surrogate for residuals, gradeint boosting uses these residuals directly!\nAdaboost can be applied loss functions which considers weights for the training data. In contrast gradient boosting is a generalized method widely applicable to many loss functions.\nThe framework of gradient boosting can be applied to any loss function, which means that any classification, regression or ranking problem can be “boosted” using weak learners. This flexibility has been a key reason for the emergence and ubiquity of gradient boosting as a state-of-the-art ensemble approach.\n\n\n\n• Like AdaBoost, gradient boosting trains a weak learner to fix the mistakes made by the previous weak learner. Adaboost uses example weights to focus learning on misclassified examples, while gradient boosting uses example residuals to do the same. • Like gradient descent, gradient boosting updates the current model with gradient information. Gradient descent uses the negative gradient directly, while gradient boosting trains a weak regressor over the negative residuals to approximate the gradient.\n\n\n\n\nAdaboost Vs Gradient Boosting\n\n\n\n\n\nGradient Descent for Boosting\n\n\nTo adapt gradient boosting to a wide variety of loss functions, it adopts two general procedures:-\n\nApproximate gradients using weak regressors\nCompute the model weights using line search\n\n\n\n\n\nPseudo code Gradient Boosting\n\n\ndef fit_gradient_boosting (X,y, n_estimators=10):\n    n_samples, n_features = X.shape\n    n_estimators = 10\n    # Initialize an empty ensemble\n    estimators = []\n    # Predictions of the ensemble on the training set\n    F = np.full((n_samples,),0.0)\n\n    for t in range(n_estimators):\n        # Compute residuals as negative gradients of the squared loss\n        residuals = y - F\n        # Fit regression tree to the examples and residuals\n        h = DecisionTreeRegressor(max_depth=1)\n        h.fit(X,residuals)\n\n        hreg = h.predict(X)\n        # Set up the loss function as a line search problem\n        loss = lambda a: np.linalg.norm(y - (F + a*hreg))**2\n        # Find the best step length using the golden section search\n        step = minimize_scalar(loss,method = 'golden')\n        a = step.x\n        # Update the ensemble predictions\n        F += a*hreg\n        estimators.append((a,h))\nFor large datasets, GBM is very slow. For large datasets, the number of splits a tree-learner has to consider becomes prohibitively large\n\n\n\nTrading exactness for speed\nWe bin the values of a features to reduce the number of splits considered for evaluating the best split for a node\n\n\n\n\n\n\nGBDT (Gradient Boosting Decision Trees) perform gradient descent in functional space. Gradient descent is performed in parameter space.\nIn gradient descent we compute the gradient of the loss with respect to the parameters. In gradient boosting, we compute the gradient of the loss with respect to the predictions.\n\n\n\n\n\nGradient descent is often used to minimize a loss function to train a machine learning model.\nResiduals, or errors between the true labels and model predictions, can be used to characterize correctly classified and poorly classified training examples. This is analogous to how Unlike AdaBoost uses weights.\nGradient boosting combines gradient descent and boosting to learn a sequential ensemble of weak learners.\nWeak learners in gradient boosting are regression trees that are trained over the residuals of the training examples and approximate the gradient.\nGradient boosting can be applied to a wide variety of loss functions arising from classification, regression or ranking tasks.\nHistogram-based tree learning trades off exactness for efficiency, allowing us to train gradient boosting models very rapidly and scaling up to larger data sets.\nLearning can be sped up even further by smartly sampling training examples (Gradient OneSide Sampling, GOSS) or smartly bundling features (Exclusive Feature Bundling, EFB).\nLightGBM is a powerful, publicly available framework for gradient boosting that incorporates both GOSS and EFB.\nAs with AdaBoost, we can avoid overfitting in gradient boosting by choosing an effective learning rate or via early stopping. LightGBM provides support for both.\nIn addition to a wide variety of loss functions for classification, regression and ranking,LightGBM also provides support for incorporation of our own custom, problem-specific loss functions for training.\n\n\n\n\n\n\nAdditive Modeling - Adding up a bunch of subfunctions to create a composite function that models some data points is called Additive Modeling. GBM use additive modeling to gradually nudge an approximate model towards a reallly good model, by adding simple submodels to a composite model\nThe weak model learns direction vectors with direction information, not just magnitudes\nThe initial model is trying to learn target y given x, but the weak learners are trying to learn direction vectors given x\ntraining the weak models on a direction vector that includes the residual magnitude makes the composite model chase outliers. This occurs because mean computations are easily skewed by outliers and our regression tree stumps yield predictions using the mean of the target values in a leaf. For noisy target variables, it makes more sense to focus on the direction of y from the prediction rather than the magnitude and direction.\nWhile we train the weak models on the sign vector, the weak models are modified to predict a residual not a sign vector!\nOptimizing sign vector leads to a solution that optimized the model according to mean absolute value (MAE) or l1 loss function. (Better than MSE which is sensitive to outliers)\nOptimizing MAE means, we should use median as the prediction for the initial model.\nMSE optimizing GBMs train trees on residual vectors and MAE GBMs train trees on sign vectors\nThe second difference between GBMs is that tree leaves of MSE GBMs predict the average of the residuals. MAE GBM tree leaves predict the median of the residual. weak models for both GBMs predict residuals given a feature vector for an observtion.\nChanging the direction vector in a GBM is chasing the negative gradient of a loss function via gradient descent\nChasing the residual vector in a GBM is chasing the gradient vector of the MSE L2 loss function while performing gradient descent\nChasing the sign vector in a GBM is chasing the gradient vector of the MAE l1 loss function while performing gradient descent\n`GBM training occurs on two levels, one to train the weak models and one on the overall composite model. It is the overall training of the composite model that performs gradient descent by adding the residual vector (assuming MSE) to get the improved model prediction. Training a NN using gradient descent tweaks model parameters whereas training a GBM tweaks (boost) the model output.\nTo summarize, in case of MSE loss, residual is equivalent to the gradient of MSE loss and weight for each leaf is equal to the mean of the residuals in that particular leaf\n\n\n\n\n\nManning book - Ensemble Methods for ML\nUnderstanding Gradient Boosting as a gradient descent\nGradient Boosting code implementation\nGradient boosting explained by Jeremy Howard\nGradient Boosting from scratch\nGradient Boosting Explained"
  },
  {
    "objectID": "ml_algorithms/adaboost.html",
    "href": "ml_algorithms/adaboost.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Sequential Ensembles exploit the dependence of base estimators\nsequential ensembles train a new base estimator in such a manner that it minimizes mistakes made by the base estimator trained in the previous step\nBoosting aims to combine weak learners, or “simple” base estimators. In contrast, parallel ensembles like Bagging use strong learners as base estimators\nDecision trees typically use decision stumps, or decision trees of depth 1 as base estimators (weak learners)\nSequential ensemble methods such as boosting aim to combine several weak learners into a single strong learner. These methods literally “boost” weak learners into a strong learner\n\n\n\n\nTo ensure that the base learning algorithm prioritizes misclassified trainng examples, Adaboost maintains weight over individual training examples. Misclassified examples will have more weight\nWhen we train the next base estimator sequentially, the weights will allow the learning algorithm to prioritize (and hopefully fix) mistakes from the previous iteration. This is the “adaptive” component of AdaBoost, which ultimately leads to a powerful ensemble. The whole training set is used for each iteration\nEach weak estimator is different from each other and classify the problem in diversely different ways. Reweighting allows AdaBoost to train a different base estimator at each iteration, one that is often different from an estimator trained at the previous iterations. Adaptive reweighting or updating adaptively, promotes ensemble diversity\n\n\n\nBoosting Iteration 1\n\n\n\n\n\nBoosting Iteration 2\n\n\n\n\n\nBoosting Iteration 3\n\n\n\n\n\nBoosting - Ensemble of weak learners\n\n\n\n\n\n\nUse decision stumps as base estimators which are weak learners\nKeep track of weights on individual training examples\nKeep track of weights of individual base estimator\n\n\n\nAdaboost Algorithm\n\n\n\n\n\nWeighing weak learners\n\n\n\n\n\nWeighing weak learners\n\n\n\n\n\nTraining example Weights\n\n\n\n\n\nTraining example weights\n\n\n\n\n\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\n\ndef fit_boosting(X,y,n_estimators=10):\n    n_samples, n_features = X.shape\n    # Initialize the weights for the training set\n    D = np.ones((n_samples,))\n    estimators = []\n\n    for t in range(n_estimators):\n        # Noramlize the weights (add to 1)\n        D = D / np.sum(D)\n\n        h = DecisionTreeClassifier(max_depth=1)\n        h.fit(X, y, sample_weight=D)\n        \n        ypred = h.predict(X)\n        \n        # Calculate the error for the weak learner\n        e = 1 - accuracy_score(y, ypred, sample_weight = D)\n\n        # Update the weights for the weak learner\n        a = 0.5*np.log((1 -e) / e)\n\n        # calculate the sign for each training example\n        # If the training example is correct then its weight will be reduced\n        # If the training example is incorrect then its weight will be increased\n        m = (y == ypred) * 1 +(y != ypred) * -1\n        # Calculate the new weights for the training set\n        D *= np.exp(-a * m)\n        # Save the weak learner and its weight\n        estimators.append(a,h)\n        \n    return estimators\n\n\n\n\n\n\n\n\nThe adaptive property (adapt to mistakes made by previous week learners) is a disadvantage when outliers are present. The weak learners continue to misclassify the outlier, in which case Adaboost will increase its weight further, which in turn, causes succeding weak learners to misclassify, fail and keep growing its weight\nIt correctly classifies the outlier, in which case Adaboost overfit the data\n\n\n\nLearning Rate It adjusts the contribution of each estimator to the ensemble. (A learning rate of 0.75 decrease the overall contribution of each estimator by a factor of 0.75). In case of outliers, learning rate should be lowered.\n\n\n\n\nAdaboost Learning Rate\n\n\n\n\n\nAdaboost Learning Rate\n\n\n\n\n\n\n\nIdentifying the least number of base estimators in order to build an effective ensemble is known as early stopping.\n\n\n\n\nAdaboost pre-pruning and post-pruning\n\n\n\n\n\n\n\n\nAdaBoost, or Adaptive Boosting is a sequential ensemble algorithm that uses weak learners as base estimators.\nIn classification, a weak learner is a simple model that performs only slightly better than random guessing, that is 50% accuracy. Decision stumps and shallow decision trees are examples of weak learners.\nAdaBoost maintains and updates weights over training examples. It uses reweighting both to prioritize misclassified examples and to promote ensemble diversity.\nAdaBoost is also an additive ensemble in that is makes final predictions through weighted additive (linear) combinations of the predictions of its base estimators.\nAdaBoost is generally robust to overfitting as it ensembles several weak learners. However, it is sensitive to outliers owing to its adaptive reweighting strategy, which repeatedly increases the weight of outliers over iterations.\nThe performance of AdaBoost can be improved by finding a good tradeoff between the learning rate and number of base estimators\nCross validation with grid search is commonly deployed to identify the best parameter tradeoff between learning rate and number of estimators.\nUnder the hood, AdaBoost ultimately optimizes the exponential loss function.\nLogitBoost is another boosting algorithm that optimizes the logistic loss function. It differs from AdaBoost in two other ways\n\n\n\n\n\nManning book Ensemble methods for machine learning"
  },
  {
    "objectID": "ml_algorithms/logitboost.html",
    "href": "ml_algorithms/logitboost.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "LogitBoost\n\nAdaboost uses exponential loss. (This exponential loss is used to calculate the weight of each base estimator)\nAdaboost works with predictions and LogitBoost works with prediction probabilties.\nThe third key difference is, since AdaBoost works directly with discrete predictions (−1 or 1, for negative and positive examples), it uses any classification algorithm as the base learning algorithm. LogitBoost, instead, works with continuous prediction probabilities. Consequently, it uses any regression algorithm as the base learning algorithm."
  },
  {
    "objectID": "ml_algorithms/xgboost.html",
    "href": "ml_algorithms/xgboost.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Newton Boosting combines advantages of Adaboost and Gradient Boosting. It uses weighted gradients (weighted residuals) to identify the most misclassified examples.\nGradient Descent is first order derivative. Newton's method or Newton Descent is a second order optimization method. It uses both first and second order derivative information during optimization.\n\n\n\n\n\n\nNewton’s Method vs gradient descent\n\n\n\n\n\nTaking second derivative\n\n\n\n\n\nNewton’s Method vs gradient descent\n\n\n\n\n\n\nIt computes the descent step exactly and does not require a step length\nIt is sensitive to our choice of initial point. Different initializations will lead Newton descent to different local minimizers.\nIt is a powerful optimization method. It converges to solutions much faster as it takes local second-order derivative information (essentially curvature) into account in constructing descent directions. This comes at a computational cost. With more variables, the second derivative or Hessian becomes difficult to manage, especially as it has to be inverted.\n\n\n\n\n\n\nAdaBoost identifies and characterizes misclassified examples that need attention by assigning weights to them: badly misclassified examples are assigned higher weights. A weak classifier trained on such weighted examples will focus on them more during learning. Gradient boosting characterizes misclassified examples that need attention through residuals. A residual is simply another means to measure the extent of misclassification and is computed as the gradient of the loss function.\nNewton boosting does both and uses weighted residuals. The residuals in Newton boosting are caclulated using the gradient of the loss function (the first derivative). The weights are computed using the Hessian of the loss function (the second derivative)\nNewton boosting avoids the expense of computing the gradient or the Hessian by learning a weak estimator over the individual gradients and Hessians\nNewton boosting uses Hessian-weighted residuals, while gradient boosting uses unweighted residuals\n\n\n\n\n\n\n\nNewton Boosting for a single example\n\n\nHessians incorporate local curvature information to ensure that training examples that are badly misclassified get higher weights\n\n\n\n\n\nDistributed and parallel processing capabilities\nNewton boosting on regularized loss functions to directly control the complexity of weak learners\nAlogrithmic speedups such as weighted quantile sketch for faster training\nBlock based system design that stores data in memory in smaller units called blocks. This allows for parallel learning, better caching, efficient multi-threading\nSupport for a lot of loss functions\n\n\n\n\nXGBoost uses tree based methods to control the complexity of the model.\nInstead of using Gini or Entropy XGBoost uses regularized learning objective\n\n\n\n\nXGBoost Regularization\n\n\n\n\n\n\nFind ideal split points using qunatiles in the features\nInstead of using evenly sized bins, XGBoost bins data into feature-dependent buckets.\n\n\n\n\n\n\nIn boosting algorithms we select the learning rate to help us avoid overfitting (instead of convergence)"
  },
  {
    "objectID": "ml_algorithms/catboost.html",
    "href": "ml_algorithms/catboost.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Catboost\n\nCatboost is a Newton boosting algorithm\nNewton Boosting + Ordering = CatBoost\n\n\n\nTarget Leakage\n\n\n\n\n\nPrediction Shift\n\n\n\n\n\nHold out encoding\n\n\n\n\n\nLeave one out encoding does not fully eliminate target leakage and prediction shift issues\n\n\nCatBoost introduces three major modifications to classical Newton Boosting approach\n\nIt is specialized to categorical features\nIt uses ordered boosting as its underlying approach which allows it to address data leakage and prediction shift\nIt uses oblivious decision trees as base estimators which often leads to faster training times\n\nCatboost addresses prediction shift by using permutation for ordering training examples\n\n\nOrdered Target Statistics\n\n\n\n\nOrdered Target Statistics\n\n\nOne downside to this approach is that training examples that occur early in a randomized sequence are encoded with far fewer examples.\nTo compensate for this in practice and increase robustness, CatBoost maintains several sequences, or “histories” which are, in turn, randomly chosen. This means that CatBoost recomputes target statistics for categorical variables at each iteration\n\n\n\nOrdered Boosting\n\nIn addition to issues with encoding features, gradient and Newton boosting both reuse data between iterations, leading to a gradient distribution shift, which ultimately causes a further prediction shift. That is to say, even if we didn’t have categorical features, we would still have a prediction shift problem, which would bias our estimates of model generalization!\n\n\n\n\nOrdered Boosting\n\n\n\n\nOblivious Decision Trees\n\nOblivious decision trees use the same splitting criterion in all the nodes across an entire level/depth of the tree\n\n\n\n\nOblivious Decision Trees\n\n\n\nOblivious decision trees lead to less split criteria, as opposed to the standard decision tree. This makes them easier and more efficient to learn, which has the effect of speeding up overall training.\nOblivious trees are balanced and symmetric, making them less complex and less prone to overfitting\n\n\n\nTips to handle Categorical Features\n\nuse string similarity to handle spelling mistakes in categorical features. (String similarity approaches - character based or token based)\ndirty-cat package provides functionality off-the-shelf. Different methods available for handling dirty categories are\n\n\n\n\nMethods in Dirty cat\n\n\n\n\nReference\n\nManning book - Ensemble methods for Machine learning"
  },
  {
    "objectID": "ml_algorithms/lightGBM.html",
    "href": "ml_algorithms/lightGBM.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Developed and released by Microsoft. It Supports parallel and GPU learning.Below are some methods utilized by LighGBM to handle large data sets.\n\n\n\nIt is a histogram based gradient boosting approach. This will work reasonably well for medium-sized data sets. Histogram bin construction itself can be slow for very large number of data points or a large number of features.\n\n\n\n\n\nData is downsampled smartly using a procedure called Gradient-Based One Side Sampling. Sampling should consists of a healthy balance between examples which are misclassified and examples which are classified correctly\n\n\n\nLightGBM GOSS Sampling Method\n\n\n\n\n\n\n\nDownsampling the features. This will provide improvements in training speed if the feature space is sparse and features are mutually exclusive\nEFB exploits sparsity and aims to merge mutually exclusive columns into one column to reduce the number of effective features.\n\n\n\n\n\n\nSimilar to Dropout from Deep learning. DART randomly and temporarily drops base estimators from the overall ensemble during gradient fitting iterations to mitigate overfitting.\n\n\n\n\n\n\n\nTraining mode in LightGBM\n\n\n\n\n\n\nlearning_rate, shrinkage_rate and eta are different names for learning rate. Only one should be set.\nUse early_stopping to stop the training if the validation score metric does not improve over the last early_stopping_rounds. We can specify the score metric to be used.\n\n\n\n\n\nDropout meets Multiple Additive Regression Trees (DART)\nDART is slow - why this is the case\nEvaluating Machine Learning Models by Alice Zheng\nHow GPUs are used to train GBT models - If they are sequential"
  },
  {
    "objectID": "data_manipulation/01_numpy.html",
    "href": "data_manipulation/01_numpy.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "import numpy as np\n\n\n# Generate random numbers\nrng = np.random.default_rng()\n\n\n# Uniform inclusive of 0 and exclusive of 10\nrng.integers(0,10,3)\n\narray([5, 0, 8])\n\n\n\n# Uniform inclusive of 0 and  10\nrng.integers(0,10,3, endpoint=True)\n\narray([ 9,  3, 10])\n\n\n\n# Uniform, x belongs [0,1)\nrng.random(3)\n\narray([0.72175294, 0.0886603 , 0.80159783])\n\n\n\n# Uniform inclusive of 0 and exclusive of 10\nrng.uniform(1,10,3)\n\narray([5.44827948, 4.7933101 , 2.88206522])\n\n\n\n# Mean 0 and std deviation 1\nrng.standard_normal(3)\n\narray([ 0.5878078 ,  0.67988484, -0.83850319])\n\n\n\n# Mean 5 and std. dev 3\nrng.normal(5,2,3)\n\narray([5.54222614, 7.24807366, 5.68900357])\n\n\n\na = np.array([1,2,3,4,5,6,7,6,5,4,3,2,1])\n\n\nnp.where(a > 5)\n\n(array([5, 6, 7]),)\n\n\n\nnp.nonzero(a>5)\n\n(array([5, 6, 7]),)\n\n\n\nnp.where(a >= 5,1,0)\n\narray([0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0])\n\n\n\nnp.clip(a,2,5)\n\narray([2, 2, 3, 4, 5, 5, 5, 5, 5, 4, 3, 2, 2])\n\n\n\nb = np.array([1,2,3,4,5])\n\n\nb[::-1]\n\narray([5, 4, 3, 2, 1])"
  },
  {
    "objectID": "Interpretable_ml/theory/resources.html",
    "href": "Interpretable_ml/theory/resources.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Resources\n\nMegasource\n\nCynthia Rudin page\n\n\n\nPackage\n\n\n\nwhat If tool\n\n\n\n\nVideo\n\nThis looks like that\n\n\n\nPPT\n\nEBM explained\n\n\n\nResearch papers\n\nStop using blackbox models for high stakes\nNeural additive models\nunderrepresentation\nExplainable Neural Networks\nICE\nFooling Lime and shap\nFooling PDP\nFairwashing\nGAMchanger\nFIGS\n\n\n\nBlogs\n\ncorels\nDiCE\nGAMchanger\nInterpretability for NN- Distill blog\nFeature visualization - Distill\n\n\n\n\nTutorials\n\ncausal Inference\nInterpretableML\nLot of notebooks\nTutorial\n\n\n\nSoftware packages\n\nGAMchanger\nslim package\npyGAM\noptimal sparse decision tree\nPyMC\ndowhy\nAletheia\nThis looks like that\nRuleFit\nskope rules\nGLRM\nDeepLift\nEli5\nlofo\nAnchor\nPDP\nconditional Expectation plot\nALE\nDICE"
  },
  {
    "objectID": "Interpretable_ml/theory/07_rulefit.html",
    "href": "Interpretable_ml/theory/07_rulefit.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Sparse linear models that include automatically detected interaction effects in the form of decision rules\nNew features captures interactions between the features\nRuleFit automatically generates the interaction features from decision trees\nThe decision rules are binary\nThe feature importance can be calculated at the local and global level\nInterpretation of feature importance for interactions \nBagged ensembles, random forest, adaboost can be used for generating rules\n\n\n\n\nAutomatically adds feature interactions\nRules are easy to interpret\nFor an individual only a few rules will apply\n\n\n\n\n\nMany rules may get non-zero weight in the Lasso model\nInterpretation tricky when we have overlapping rules\n\n\n\n\n\nskope-rules (Seems the development stopped 2 years ago)\nimodels\n\n\n\n\n\nFast Interpretable greedy-tree sums (FIGS)\nHierarchical shrinkage:post-hoc regularization of tree based methods"
  },
  {
    "objectID": "Interpretable_ml/theory/11_neural_network.html",
    "href": "Interpretable_ml/theory/11_neural_network.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Techniques for interpreting NN\n\n\n\n\n\nThe approach of making the learned features explicit is called Feature Visualization\nFeature visualization for a unit of NN is done by finding the input that maximizes the activation of that unit\nFeature visualization can be done at Neuron, feature map, entire convolution layer etc.\n\n\n\n\n\nIt links highly activated areas of CNN channels with human concepts \n\n\n\n\n\nThis method highlights the pixels that were relevant for a certain image classification\nPixel attribution can be of two methods\n\nPerturbation based - Manipulate parts of the image to generate explanations\nGradient based - Many methods compute the gradient of the prediction with respect to the input features\n\n\n\n\n\nVanilla Gradient\nDeconvNet\nGrad-CAM\nGuided Grad-CAM\nSmoothGrad\n\n\n\n\n\n\nTCAV - Testing with concept activation vectors - For any given concept, TCAV measures the extent of that concept’s influence on the model’s prediction for a certain class. For example, TCAV can answer questions such as how the concept of “striped” influences a model classifying an image as a “zebra”. Since TCAV describes the relationship between a concept and a class, instead of explaining a single prediction, it provides useful global interpretation for a model’s overall behavior.\nDifferent types of CAV:-\n\nAutomated concept based explanation (ACE)\nConcept bottleneck models\nConcept whitening\n\n\n\n\n\n\nAn instance with small, intentional feature perturbations that cause a ml model to make a false prediction\n\n\n\n\n\nWe call a training instance “influential” when its deletion from the training data affect the resulting model.\nTwo approaches:-\n\nDeletion diagnostics\nInfluence functions\n\n\n\n\n\nDFBETA - Measures the effect of deleting an instance on model parameters\nCook’s distance - Measures the effect of deleting an instance on model predictions\nDFBETA works only for parameterized models\nwe can build a interpretable model between the influence of the instances and their features. This will provide more insights into the model.\nThe disadvantage of this method is that retraining is required for each instance in the dataset.\n\n\n\n\nlucid package with Notebook tutorials\ntf_cnnvis package\nKeras filter visualization\nDeepExplain\ninnvestigate\n\n\n\n\n\nNetwork Dissection"
  },
  {
    "objectID": "Interpretable_ml/theory/08_naive_bayes_and_knn.html",
    "href": "Interpretable_ml/theory/08_naive_bayes_and_knn.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Based on Bayes conditional probabilities\nStrong assumption of conditional independence of features \nIt is an interpretable model because of the independence of assumption"
  },
  {
    "objectID": "Interpretable_ml/theory/08_naive_bayes_and_knn.html#k-nearest-neighbors",
    "href": "Interpretable_ml/theory/08_naive_bayes_and_knn.html#k-nearest-neighbors",
    "title": "My Datascience Journey",
    "section": "K-Nearest Neighbors",
    "text": "K-Nearest Neighbors\n\nThe tricky part is finding the right K and how to measure the distance between instances\nThere are no parameters to learn, so no interpretability on a modular level\nNo global interpretability\nLocal interpretation depends on the number of features in a data instance. If the features are less then it can give good explanations."
  },
  {
    "objectID": "Interpretable_ml/theory/01_introduction.html",
    "href": "Interpretable_ml/theory/01_introduction.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Local explanations can be more accurate than global explanations.\nRashomon effect - An event can be explained by various causes\n\n\n\n\nExplanations are contrastive - The best explanation is the one that highlights the greatest difference between the object of interest and the reference object. Humans are interested in conterfactual explanations. Instead of knowing the reasons for why their loan was rejected, they would be interested in knowing what changes can be done to secure the loan.\nExplanations are selected - Give only 1 to 3 good reasons even if the world is very complex. Ensemble methods use different combinations of features with different models. It also means that there is more than one selective explanation of why a certain prediction was made.\nExplanations are social - Explanations should be contextualized to the social environment of the ML model and target audience.\nExplanations focus on the abnormal - (Teacher asking question and behaving abnormally as the reason for student failing the course). Any feature which was abnormal and which influenced the prediction should be included in explanation.\nGood explanations are consistent with prior beliefs of the explainee - How can we enforce monotonicity constraints on a feature? (feature can only affect the prediction in one direction)\nGood explanations are general and probable - In the absence of abnormal causes, a general explanation (explain many instances) is considered as a good explanation.\n\nTransparency is an enabler of trust\n\n\n\n* constraints - Sparsity, monotonic, interaction constriants\n* Additivity of Inputs - Example GAM\n* Prototypes - Using well understood data points for explaining previously unseen data point\n* Summarization - variable important measures, surrogate models and other post-hoc approaches\n\n\n\n\nGAM, GA2Ms, penalized regression\n\n\n\n\nIteratively Reweighted least squares (IRLS) - Minimize the effect of outliers. After first iteration IRLS checks which rows of input data are leadng to large errors. It then reduces the weight of those rows in subsequent iterations\nL1 Norm peanlty - LASSO - Avoid multiple comparison problemsthat arise in older stepwise feature selection.\nL2 Norm penalty - Ridge - Stabilize model parameters in the presence of correlation\nLink function\n\nlogarthim link function - count data\ninverse link function - Ganna distributed output\n\nElatic net\n\nIRLS, L1, L2 and link functions for various target or error distributions ##### when to use penalized regression:-\nwhen more features than rows\ncorrelated features\nNeed transparency\n\n\n\n\n\nState of the art in penalized regression models\nTransparent (Highest interpretability)\n\n\n\n\n\n\nGold standard of interpretability\nThey don’t fit to noise as traditional black-box ML models\n\n\n\n\n\nUse the same principles as GAM, EBM with some twists\nSimple additive combinations of shape functions\nBack propagation is used to learn optimal combinations of variables to act as inputs to shape functions learned via subnetworks\nShape functions are combined in additive fashion with weights to get output of the network\n\n\n\n\n\nprototype method\n\n\n\n\n\nRuleFit and scope rules are two techniques to find interpretable rules from the training data\nCertifiable optimal rule lists (CORELS)\nScalable Bayesian rule lists\n\n\n\n\n\nL1 penalty introduced to matrix factorization\nWe can extract new features from a large matrix of data, where only a few of the original columns have large weights on any new features\nsparse principal components analysis (SPCA) ???\nNonnegative Matrix Factorization (NMF) - Training data only takes positive values\nMany unsupervised learning techniques are instances of generalized low-rank models\n\n\n\n\n\n\n\n\nWhat input features value would have to be changed to change the outcome of a model prediction\n\n\n\n\n\nCommon in deep learning\nFor image and text data gradients are overlaid on input images to create highly visual explanations depicting where a function exhibits large gradients when creating a prediction for that input.\nIntegrated gradients - ????\nLayerwise relevance propagation - ???\nDeeplift - ???\nGrad-CAM\n\n\n\n\n\nRemove features from a model prediction and tracking the resulting change in the prediction\nThis method is basis for occlusion and leave-one-feature-out(LOFO) method\n\n\n\n\n\ncriticisms are data points that are not represented well by prototypes\nPrototypes and criticisms are used to generate local explanations\n\n\n\n\n\nAverage of all possible sets of inputs that do not include the feature of interest. Different groups of inputs are called coalitions.\nIt takes into account much more information than most other local feature methods\nTree SHAP - Exact - Applicable for tree based methods\nKernel and Deep SHAP - Approximate\nKernel SHAP - model agnostic\nSHAP is an offset from the average model prediction\nIt does not provide causal or counterfactual explanations\n\n\n\n\n\n\n\n\nAggreagte of local importance.\n\n\n\n\n\nSimple models of complex models\nModel agnostic\n\n\n\n\nDecision tree is trained on the inputs and predictions of a complex model\n\n\n\n\n\nLocal interpretable model-agnostic explanations (LIME)\nFitting linear model to the predictions of some small region of a more complex model’s predictions\nSparse local explanations\nA locally weighted interpretable surrogate model with a penalty to induce sparsity\nChallenges\n\nSampling is a problem for real-time explanation\nGenerated data may contain out-of-range data leading to unrealistic local feature importance values\nExtreme non-linearity and interactions in the selected local region can cause lime to fail completely\nLocal feature importance values are offsets from the local GLM intercept, and this intercept can sometimes account for the most important local phenomena.\n\n\n\n\n\n\nRule based methods to describe a ml model\nSpecial instance of using rule-based models as surrogate models\nRule based models have good capacity to learn non-linearities and interactions\n\n\n\n\n\n\n\n\nHow predictions change based on the values of one or two input features of interest, while averging out the effects of all oher input features.\nICE ??? depict how a model behaves for a single row of data\nPDP should be paired with ICE plots\n\n\n\n\n\nValuable when strong correlations exist in the training data"
  },
  {
    "objectID": "Interpretable_ml/theory/09_global_model_agnostic.html",
    "href": "Interpretable_ml/theory/09_global_model_agnostic.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "They describe the average behavior of ml models. They are often expressed as expected values based on the distribution of the data.\n\n\n\n\n\nGlobal Model Agnostic Methods\n\n\n\n\n\n\nPDP at a particular feature value represents the average prediction if we force all data points to assume that feature value\nIt shows the marginal effect one or two features have on the predicted outcome of an ML model. Other features are not touched or considered as random\nAssumption is that the feature for which we are calculating average behavior is not correlated with other features in the dataset\nIf this assumption is violated, then PDP will include data points that are very unlikely or even impossible\nFor classification where we have output probabilties, PDP display probabilities for a certain class given different values for a feature\nFor categorical features - For each of the category, we get a PDP estimate by forcing all data instances to have the same category and average the predictions\nFeature importance - A flat PDP indicates that the feature is not important, and the more the PDP varies, the more important the feature. This method ignores the effect of possible feature interactions.\n\n\n\n\nEasy to implement\nPDP has causal interpretation - We intervene on a feature and measure the changes in predictions\nMaximum number of features for PDP is two\nNeed to look at PDP along with data distribution, omititng the distribution can be misleading because we might overinterpret regions with almost no data\nAssumption of Independence - Unlikely values or impossible values\n\n\n\n\n\n\nFaster and unbiased alternative to PDP\nALE calculate how the model predictions change in a small window of the feature around v for data instances in that window.  \nWe average the changes of predictions, not predictions itself. We accumulate the average effects across all intervals. The effect is centered so that the mean effect is zero\nThis method isolates the effect of the feature of interest and blocks the effect of correlated features\nThe value of ALE can be interpreted as the main effect of the feature at a certain value compared to the average prediction of the data\nQuantiles of the feature are used as the grid that defines the intervals.\n\n\n\n\nALE plots are unbiased, work for correlated features\nFaster to compute\nInterpretation of ALE plots is clear: Conditional on a given value, the relative effect of changing the feature on the prediction can be read from the ALE plot. ALE plots are centered at zero. This makes their interpretation nice, because the value at each point of the ALE curve is the difference to the mean prediction. The 2D ALE plot only shows the interaction: If two features do not interact, the plot shows nothing.\nEntire prediction function can be decomposed into a sum of lower-dimensional ALE functions\nALE plots can become a bit shaky (many small ups and downs) with a high number of intervals\nUnlike PDPs, ALE plots are not accompanied by ICE curves\nImplementation of ALE plots are much more complex and interpretation remains difficult when features are strongly correlated.\nAs a rule of thumb use ALE instead of PDP\n\n\n\n\n\n\nThe interaction between two features is the change in the prediction that occurs by varying the features after considering the individual feature effects.\nH-Statistic - One way to estimate the interaction strength is to measure how much of the variation of the prediction depends on the interaction of the features\nWe measure the difference between the observed partial depedence function and the decomposed one without interactions. We calculate teh variance of the output. The amount of variance explained by the interaction is used as interaction strength statistic. The statistic is 0 if there is no interaction at all and 1 if all of the variance is explained.\n\n\n\n\nBacked by underlying theory, meaningful interpretation (share of variance explained by interaction), comparable across features and models\nComputationally expensive, results can be unstable in case of sampling, H-statistic can be greater than 1\nIt does not tell how the interaction look like. Measure the interaction strength and then create a 2D PDP plot for the interactions.\nWe can’t use this for images\nAssumption of independence of features\nVariable Interaction Networks (VIN) - is an approach that decomposes the prediction function into main effects and feature interactions. The interactions between features are then visualized as a network. Unfortunately no software is available yet.\n\n\n\n\n\n\nMeasures the increase in prediction error after we permute the feature’s values, which breaks the relationship between the feature and the true outcome.\nThis should be used on the test data\n\n\n\n\nNice interpretation - Feature importance is the increase in model error when the feature’s information is destroyed\nProvides highly compressed, global insight\nTakes into account all interactions with other features - It takes into account both the main feature effect and interaction effects on model performance (this is also a disadvantage - This is also a disadvantage because the importance of the interaction between two features is included in the importance measurements of both features. This means that the feature importances do not add up to the total drop in performance, but the sum is larger)\nIt needs access to true outcome\nIf features are correlated, feature importance can be biased by unrealistic data instances\n\n\n\n\n\n\nAn interpretable model that is trained to approximate the predictions of a black box model \nR-Squared can be used to measure how well surrogate replicates the black box model\nAny interpretable model can be used as a surrogate model\nEven if the underlying black-box model changes, you do not have to change your method of interpretation\nWith this method we should draw conclusions about the model and not the data\nIn case of close enough R-Squared, the surrogate model may be close enough for one subset of the dataset but widely divergent for another subset.\n\n\n\n\n\nA prototype is a data instance that is representative of all the data\nA criticism is a data instance that is not well represented by the set of prototypes\nPrototypes and criticisms are always actual instances from the data\nK-mediods can be used to find the prototypes - Any clustering algorithm that returns actual data points as cluster centers would qualify\nMMD-Critic approach is used to find prototypes and criticisms in a single framework. This method compares the distribution of the data and the distribution of the selected prototypes. Prototypes are selected that minimize the discrepancy between the two distributions. Data points from regions that are not well explained are selected as criticisms.\nMaximum Mean Discrepancy (MMD) is used to measure the discrepancy between two distributions. The closer the MMD squared is to zero, the better the distribution of the prototypes fits the data.\nMMD, Kernel and greedy search are used to find the prototypes\nwitness function is used to find the criticism. This tells us how much two density estimates differ at a particular point. Criticisms are points with high absolute value in the witness function \nThis method works with any type of data and any type of ML model\nWe are free to choose the number of prototypes and criticisms\nCriticisms depend on the number of existing prototypes\n\n\n\n\nIf we have a loss function that is twice differentiable with respect to its parameters, we can estimate the influence of the instance on the model parameters and on the prediction with influence functions.\nInstead of deleting the instance, the method upweights the instance in the loss by a very small step. Loss upweighting is similar to deleting the instance.\nAccess to the loss gradient with respect to the model parameters are required\n\n\n\n\n\nUnderstanding the weakness of a model by identifying influential instances helps to form a “mental model” of the machine learning model behavior in the mind\nDebugging model errors\nFixing the training data\n\n\n\n\n\nOne of the best debugging tools for ML models\nHelp to identify instances which should be checked for errors\nInfluence functions via derivaties can also be used to create adversarial training data\n\n\n\n\n\n\nprototype Implementation on text data using AIX360\nprototype on tabular data using AIX360\nprototype using Alibi\nALE using Alibi\nPDP and ICE using sklearn\npdp using PDPbox\nInfluence functions"
  },
  {
    "objectID": "Interpretable_ml/theory/12_nodegam.html",
    "href": "Interpretable_ml/theory/12_nodegam.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "EBMs will not work for large datasets and if the data consists of more features\n\n\n\n\nTrain a MLP for each feature and sum the outputs across features\n\n\n\n\nNAM working process\n\n\n\n\n\nNew activation function ExU is required\nAs ExU can be overfitting, they need to train multiple neural nets with different random seeds and take average\nComputationally very expensive\n\n\n\n\n\n\nNeural Oblivious Decision Trees (NODEs)\nInstead of neurons it uses differentiable oblivious decision trees (ODT) to learn\nIn the input layer, instead of summing all the input features and going through a Relu function, NodeGAM uses differentiable attention with temperature annealing to take only 1 feature. This makes sure there is no feature interaction in the model.\nFor the connections between layers, NodeGAM uses differentiable gates that only connect trees that belong to the same (set of) features. This also prevents feature interactions.\nFinally, it uses the DenseNet-like skip connections that take all the previous layers’ outputs as inputs. Also, in the output layer, it takes all the intermediate layers embedding as the inputs to the final linear layer as outputs. This helps the gradient to flow through the model since the tree response function is similar to the sigmoid that has a gradient vanishing problem.\n\n\n\n\nNodeGAM Architecture\n\n\n\n\n\n\nNeural Oblivious Decision Trees (NODEs)\nThis method learn pairwise interactions but removes any 3rd or higher-order interactions\nThis is achieved by limiting each tree to take at most 2 features. And the connections are only allowed among trees with the same sets of features.\nAn attention mechanism is added between layers to improve accuracy\n\n\n\n\nNode2GAM Architecture\n\n\nReference:- Medium Blogpost on NodeGAM code implementation of NodeGAM\n\n\n\nLink to research paper"
  },
  {
    "objectID": "Interpretable_ml/theory/10_local_model_agnostic.html",
    "href": "Interpretable_ml/theory/10_local_model_agnostic.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Local Methods\n\n\n\n\n\nHow the instance prediction changes when a feature changes\nThe equivalent of PDP for a individual row is called ICE.\nICE will provide more insight in case of interactions\nCentered ICE plot - Centre the curves at a certain point (may be at minimum) in the feature and display only the difference in prediction to this point.\nDerivative ICE plot - Individual derivaties of the prediction function with respect to the feature. It takes long time to compute and is impractical\n\n\n\n\nICE can uncover heterogeneous relationships\nCan display only one feature meaningfully\nSuffer from same points as pdp\nplot can become overcrowded - Add transparency or draw only a sample of the lines\nICE does not show average. Combine ICE with PDP\n\n\n\n\n\n\nInterpretable models that are used to explain individual predictions of a black box machine learning models\nLIME generates a new dataset consisting of perturbed samples and the corresponsing predictions of the black box model\nOn this new dataset LIME then trains an interpretable model, which is weighted by the proximity of the sampled instances to the instance of interest. \nDefining a meaningful neighborhood around a point is difficult. LIME uses exponential smoothing kernel to define the neighborhood.\nLIME can be used for Text and Images also. In case of text new data is generated by randomly removing words from the original text.\nLIME for images - variations of the images are created by segmenting the image into “superpixels” and turning superpixels off or on. Superpixels are interconnected pixels with similar colors and can be turned off by replacing each pixel with a user-defined color such as gray. The user can also specify a probability for turning off a superpixel in each permutation.\nThe fidelity measure (how well the interpretable model approximates the black box predictions) gives us a good idea of how reliable the interpretable model is in explaining the black box predictions in the neighborhood of the data instance of interest.\nThe surrogate model can use other features than the original model was trained on.\nFor defining the neighborhood we need to try different kernel setting and see if explanations make sense (Tabular data)\nInstability of explanations. Explanations of two very close points can be varied greatly. Repeating the sampling process can give different explanations.\n\n\n\n\n\nA counterfactual explanation of a prediction describes the smallest change to the feature values that changes the prediction to a predefined output.\nCounterfactuals suffer from ‘Rashomon effect’\nCriteria for counterfactuals\n\nCounterfactual instance produces the predefined prediction as closely as possible\nCounterfactual should be as similar as possible to the instance regarding feature values. It should change as few features as possible\nGenerate multiple diverse counterfactual explanations\nCounterfactual instance should have feature values that are likely\n\nInstead of trial and error, a loss function can be designed using the above four criteria and loss can be optimized to find counterfactuals. A genetic algorithm is used for optimization. (Nondominated Sorting Genetic Algorithm)\n\n\n\n\nClear explanations\nDoes not require access to the data or the model. (Attractive for companies which are audited by third parties)\nDisadvantage - ‘Rashomon effect’.\n\n\n\n\n\n\nExplains individual predictions of any black box classification model by finding a decision rule that “Anchors” the prediction sufficiently.\nA rule anchors a prediction if changes in other feature values do not affect the prediction\nUse perturbation based strategy to generate local explanations for predictions of black box ml models. The resulting explanations are expressed as easy to understand IF-THEN rules called “anchors”\nCoverage - To which other possibly unseen instances anchors apply\nReinforcement learning is used to find the anchors\n\n\n\nAnchors working\n\n\nThe algorithm’s efficiency decreases with many features\n\n\n\n\nEasy to interpret\nworks when model predictions are non-linear\nModel agnostic and parallelized\nNeed high configuration with hyperparameter tuning\nNeed discretization of numeric features\nNeed many calls to the ML model\nCoverage is undefined in some domains\nPresently it is implemented only for tabular data.\n\n\n\n\n\n\nA prediction can be explained by assuming that each feature value of the instance is a “player” in a game where the prediction is the payout. Shapley values – a method from coalitional game theory – tells us how to fairly distribute the “payout” among the features.\nShapley value is the average marginal contribution of a feature value across all possible coalitions.\nThe interpretation is how much a feature contributed to the prediction of this particular instance compared to the average prediction for the dataset. The sum of shapley values yields the difference of actual and average prediction\nThe Shapley value is the average contribution of a feature value to the prediction in different coalitions.\n\n\n\n\nThe difference between the prediction and the average prediction is fairly distributed among the feature values of the instance – the Efficiency property of Shapley values. This property distinguishes the Shapley value from other methods such as LIME. LIME does not guarantee that the prediction is fairly distributed among the features. The Shapley value might be the only method to deliver a full explanation.\nThe Shapley value allows contrastive explanations. Instead of comparing a prediction to the average prediction of the entire dataset, you could compare it to a subset or even to a single data point. This contrastiveness is also something that local models like LIME do not have.\nThe Shapley value is the only explanation method with a solid theory.\nIt requires a lot of computing time, only approximate solution is feasible.\nThe exponential number of the coalitions is dealt with by sampling coalitions and limiting the number of iterations M. Decreasing M reduces computation time, but increases the variance of the Shapley value. There is no good rule of thumb for the number of iterations M. M should be large enough to accurately estimate the Shapley values, but small enough to complete the computation in a reasonable time.\nShapley value method always use all the features.Humans prefer selective explanations as provided by LIME. SHAP can provide explanations with few features.\nNeed access to the data\nIt suffers from inclusion of unreaistic data instances when features are correlated\n\n\n\n\n\n\nKernelSHAP - A kernel based estimation approach for shapley values inspired by local surrogate models \nAs KernelSHAP uses linear regression for estimating shap values, we can go for sparse explanations.(I am not so sure whether the resulting coefficients would still be valid Shapley values though.)\n\n\n\n\n\nA variant of SHAP for tree based models\nIt uses conditional expectation to estimate effects which has its drawbacks. The shapley value of a feature not included in the coalition will not be zero in case of conditional expectation.\n\n\n\n\n\nShapley values can be combined into global explanations\nFeatures with large absolute shapely values are important \nDifference between Permuatation feature importance and shapley value. Permutation feature importance is based on the decrease in model performance. SHAP is based on magnitude of feature attributions.\nSummary plot combines feature importance with feature effects. #### SHAP Summary plot  #### SHAP feature dependence plot  #### SHAP Interaction values\nThe interaction effect is the additional combined feature effect after accounting for the individual feature effects. \n\n\n\n\n\nWe can cluster with shapley values. Features are often on different scales.The difficulty is to compute distances between instances with such different, non-comparable features.SHAP clustering works by clustering the Shapley values of each instance.This means that you cluster instances by explanation similarity. All SHAP values have the same unit – the unit of the prediction space. Any clustering technique can be selected"
  },
  {
    "objectID": "Interpretable_ml/theory/05_GLM_GAM.html",
    "href": "Interpretable_ml/theory/05_GLM_GAM.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Linear regression models have their drawbacks. Their assumptions also may not hold in real datasets. To overcome the assumptions and limitations of linear regression we have GLM\nGLM are used when target is not following gaussian distribution\nGAM are used when there is non-linear relationship between features and target\n\n\n\n\nThe core concept of GLM is to keep the weighted sum of features, but allow non-gaussian outcome distribution and connect the expected mean of this distribution (from exponential family of distributions) and the weighted sum through a possibly nonlinear function (link function). Example, logistic regression model assumes bernoulli distribution for the outcome and links the expected mean and weighted sum using the logistic function\n\n\n\n\n\nIt assumes that the outcome can be modeled by a sum of arbitary functions of each feature. \nUsing a flexible function allow non-linear relationships between some features and the output.\nSpline functions help learn nonlinearity\nSplines are functions that are constructed from similar basis functions. Splines can be used to approximate other, more complex functions.\nNonlinear modeling with slines is fancy feature engineering\nLet us assume we need to model a feature. We remove the feature from the data and replcae it with spline basis functions. For example four spine basis functions for the feature which we are interested. The value for each of the four spline feature depends on the original feature instance. Then weights are assigned to each of the spline basis function.\n\n\n\n\nData violates IID, Example - Repeated measurements from same patient - Mixed models\nHeteroscedastic - Variance of errors is not constant - For expensive houses error of price prediction will be higher - Robust regression\nOutliers - Robust regression\nTime until an event occurs - Parametric survival models, cox regression, survival analysis\nPredict ordered categories - Proportional odds model\nOutcome is a count - Poisson regression\nIf the count value of zero is very frequent - Zero inflated poisson regression, hurdle model\ncause and effect - Causal inference, mediation analysis\nMissing data - Multiple imputation\nIntegrate prior knowledge into my model - Bayesian inference\n\n\n\n\n\nIt keeps input features independent and also allow for complex modeling of each feature’s behavior\nThe output is a linear combination of parameters and some functions applied to data input values\nNeural Additive Model - A neural network is used to fit the shape function\nModel Editing - Change out parts of a model to better match reality or human intuition is model editing.\nAmenable to model editing\n\n\n\n\n\n2 in GA2M means consideration of a small group of pairwise interactions as inputs to the model\nEBM\n\nEach feature fitted with boosted tree\nTrees have advantages over spline functions - scalable, accept categorical or missing data\n\n\n\n\n\n\nParent-child relationships, especially near the top of the tree, tend to point toward feature interactions\nDecision tree does not end up with the best model for the dataset, but instead is one good candidate for the best model out of many, many possible options. (Rashomon effect)\nRashomon effect will lead to underspecification - Good performance on validation data but failing in real world\nTo overcome this problem:-\n\noptimal decision trees\nManual constraints using human domain knowledge\n\n\n\n\n\n\nXGBoost now supports monotonicity and interaction constraints\nconstrained xgboost can be used with a post-hoc explainable methods\n\n\n\n\n\nGAM - pyGAM"
  },
  {
    "objectID": "Interpretable_ml/theory/03_logistic_regression.html",
    "href": "Interpretable_ml/theory/03_logistic_regression.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Logistic Regression\n\nInterpretation\n\nNumerical Feature: If we increase the value of feature by one unit, the estimated odds change by a factor of exp(weight). Final weight = weight * exp(weight)\nCategorical Feature: One of the value of a feature is reference category. Changing the value of feature from reference category to other category will change estimated odds by a factor of exp(weight)\nIntercept: When all numeric features are zero and categorical features are at reference category, the extimated odds are exp(bias). Intercept weight is ususally not relevant\n\n\n\nPros and Cons\n\nSame as linear regression\nInterpretation is more difficult as the interpretation of weights is multiplicative and not additive"
  },
  {
    "objectID": "Interpretable_ml/theory/04_explainable_boosting_machine.html",
    "href": "Interpretable_ml/theory/04_explainable_boosting_machine.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Explainable Boosting Machines\n\nIt is a tree based, cyclic gradient boosting generalized additive model with automatic interaction detection\nOften accurate as blackbox models\nSlower to train than other modern algorithms\nExtremely compact and fast at prediction time\nFor both classification and regression\n\n\nReferences:-\n\nCornell Paper\nResearch paper\nInterpret Ml\n\n\n\nBlogs:-\n\nMedium Blog post\nNotebook\n\n\n\nGithub:-\n\nlink"
  },
  {
    "objectID": "Interpretable_ml/theory/06_Decision_tree.html",
    "href": "Interpretable_ml/theory/06_Decision_tree.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Decision Trees\n\nFeature Importance\n\nThe number of splits for which a feature was used and measure how much it has reduced the variance or gini index compared to the parent node. The sum of all importances is scaled to 100.\n\n\n\nDisadvantages\n\nTrees fail to deal with linear relationships\nSlight changes in input can have a big impact on outcome - Lack of smoothness\nTrees are unstable. A few changes in the training data can create a completely different tree\nThe number of terminal nodes increases quickly with depth"
  },
  {
    "objectID": "Interpretable_ml/theory/02_linear_regression.html",
    "href": "Interpretable_ml/theory/02_linear_regression.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Linear regression\n\nWeights come with confidence intervals.\n\n\nAssumptions of linear regression\n\nLinearity between independent and dependent features\nTarget follows a normal distribution. If this assumption is violated, the estimated confidence interval of the feature weights are invalid\nHomoscedasticity (constant variance of errors for prediction) - Suppose the average error (difference between predicted and actual price) in your linear regression model is 50,000 Euros. If you assume homoscedasticity, you assume that the average error of 50,000 is the same for houses that cost 1 million and for houses that cost only 40,000. This is unreasonable because it would mean that we can expect negative house prices.\nIndependence between data instances\n\nFixed features - Features are free of measurement errors\nAbsence of multicollinearity\n\n\n\nInterpretation\n\nIncreasing the numerical feature by one unit changes the estimated outcome by its weight\nChanging the categorical feature from reference category to the other category changes the estimated outcome by the feature’s weight\nFor an instance with all numeric feature values at zero and categorical features at the reference categories, the model prediction is the intercept weight\nR-Squared increases with increase in number of features, even if they do not contain any information about the target value. We use adjusted R-Square, which accounts for the number of features used in a model.\nThe importance of feature is measured by the value of t-statistic. The t-statistic is the estimated weight scaled with the standard error.\nWeight plot is used to visualize the weight and variance of the features \nEffect plot can help understand how much the combination of weight and feature contributes to the predictions in the data  For individual prediction we can overlay the value for each feature on top of the effect plot \nIn Lasso regression, we can use lambda as a parameter to control the interpretability of the model\n\n\n\nFeature selection\n\nForward selection: Fit a model with one feature. Do this for all the features. Select the model with high R-squared value. Keep adding features and selecting the best model. Continue till some criterion such as the max number of features are reached\nBackward selection: Similar to forward, here we keep removing features till some criterion is reached.\nLasso can be used, as it considers all features simultaneously and can be automated.\n\n\n\nPros and Cons\n\nAdvantages\n\nTransparent model\nComes with confidence intervals, tests etc\n\n\n\nDisadvantages\n\nNon-linearities need to be hand-crafted\nNot good predictive performance\nThe interpretation of weights can be unintuitive as it depends on all other features. House size and number of rooms are highly correlated. The weight of house size can be positive and number of rooms negative due to correlation."
  },
  {
    "objectID": "Interpretable_ml/interpret_ai/notebooks/01_ebm.html",
    "href": "Interpretable_ml/interpret_ai/notebooks/01_ebm.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Editable Interpretable Models\n\nLoad the libraries\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom interpret.glassbox import ExplainableBoostingClassifier\nfrom json import load\nfrom interpret import set_visualize_provider\nfrom interpret.provider import InlineProvider\nfrom interpret import show\nimport gamchanger as gc\nset_visualize_provider(InlineProvider())\n\n\n\nLoad the data\n\ndf = pd.read_csv(\n    \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\",\n    header=None)\n\n\ndf.columns = [\n    \"Age\", \"WorkClass\", \"fnlwgt\", \"Education\", \"EducationNum\",\n    \"MaritalStatus\", \"Occupation\", \"Relationship\", \"Race\", \"Gender\",\n    \"CapitalGain\", \"CapitalLoss\", \"HoursPerWeek\", \"NativeCountry\", \"Income\"\n]\n\n\n\nExplore the data\n\ndf.head()\n\n\n\n\n\n  \n    \n      \n      Age\n      WorkClass\n      fnlwgt\n      Education\n      EducationNum\n      MaritalStatus\n      Occupation\n      Relationship\n      Race\n      Gender\n      CapitalGain\n      CapitalLoss\n      HoursPerWeek\n      NativeCountry\n      Income\n    \n  \n  \n    \n      0\n      39\n      State-gov\n      77516\n      Bachelors\n      13\n      Never-married\n      Adm-clerical\n      Not-in-family\n      White\n      Male\n      2174\n      0\n      40\n      United-States\n      <=50K\n    \n    \n      1\n      50\n      Self-emp-not-inc\n      83311\n      Bachelors\n      13\n      Married-civ-spouse\n      Exec-managerial\n      Husband\n      White\n      Male\n      0\n      0\n      13\n      United-States\n      <=50K\n    \n    \n      2\n      38\n      Private\n      215646\n      HS-grad\n      9\n      Divorced\n      Handlers-cleaners\n      Not-in-family\n      White\n      Male\n      0\n      0\n      40\n      United-States\n      <=50K\n    \n    \n      3\n      53\n      Private\n      234721\n      11th\n      7\n      Married-civ-spouse\n      Handlers-cleaners\n      Husband\n      Black\n      Male\n      0\n      0\n      40\n      United-States\n      <=50K\n    \n    \n      4\n      28\n      Private\n      338409\n      Bachelors\n      13\n      Married-civ-spouse\n      Prof-specialty\n      Wife\n      Black\n      Female\n      0\n      0\n      40\n      Cuba\n      <=50K\n    \n  \n\n\n\n\n\ndf.shape\n\n(32561, 15)\n\n\n\ndf.isna().sum()\n\nAge              0\nWorkClass        0\nfnlwgt           0\nEducation        0\nEducationNum     0\nMaritalStatus    0\nOccupation       0\nRelationship     0\nRace             0\nGender           0\nCapitalGain      0\nCapitalLoss      0\nHoursPerWeek     0\nNativeCountry    0\nIncome           0\ndtype: int64\n\n\n\ntrain_cols = df.columns[0:-1]\n\n\nlabel = df.columns[-1]\n\n\nX = df[train_cols]\n\n\ny = df[label]\n\n\nseed = 163\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.20,random_state=seed)\n\n\n\nTrain the model\n\nebm = ExplainableBoostingClassifier(random_state=seed)\nebm.fit(X_train,y_train)\n\nExplainableBoostingClassifier(feature_names=['Age', 'WorkClass', 'fnlwgt',\n                                             'Education', 'EducationNum',\n                                             'MaritalStatus', 'Occupation',\n                                             'Relationship', 'Race', 'Gender',\n                                             'CapitalGain', 'CapitalLoss',\n                                             'HoursPerWeek', 'NativeCountry',\n                                             'Relationship x HoursPerWeek',\n                                             'Age x Relationship',\n                                             'EducationNum x Occupation',\n                                             'EducationNum x MaritalStatus',\n                                             'Age x HoursPerWeek',\n                                             'MaritalSta...\n                              feature_types=['continuous', 'categorical',\n                                             'continuous', 'categorical',\n                                             'continuous', 'categorical',\n                                             'categorical', 'categorical',\n                                             'categorical', 'categorical',\n                                             'continuous', 'continuous',\n                                             'continuous', 'categorical',\n                                             'interaction', 'interaction',\n                                             'interaction', 'interaction',\n                                             'interaction', 'interaction',\n                                             'interaction', 'interaction',\n                                             'interaction', 'interaction'],\n                              random_state=163)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.ExplainableBoostingClassifierExplainableBoostingClassifier(feature_names=['Age', 'WorkClass', 'fnlwgt',\n                                             'Education', 'EducationNum',\n                                             'MaritalStatus', 'Occupation',\n                                             'Relationship', 'Race', 'Gender',\n                                             'CapitalGain', 'CapitalLoss',\n                                             'HoursPerWeek', 'NativeCountry',\n                                             'Relationship x HoursPerWeek',\n                                             'Age x Relationship',\n                                             'EducationNum x Occupation',\n                                             'EducationNum x MaritalStatus',\n                                             'Age x HoursPerWeek',\n                                             'MaritalSta...\n                              feature_types=['continuous', 'categorical',\n                                             'continuous', 'categorical',\n                                             'continuous', 'categorical',\n                                             'categorical', 'categorical',\n                                             'categorical', 'categorical',\n                                             'continuous', 'continuous',\n                                             'continuous', 'categorical',\n                                             'interaction', 'interaction',\n                                             'interaction', 'interaction',\n                                             'interaction', 'interaction',\n                                             'interaction', 'interaction',\n                                             'interaction', 'interaction'],\n                              random_state=163)\n\n\n\nebm.feature_names\n\n['Age',\n 'WorkClass',\n 'fnlwgt',\n 'Education',\n 'EducationNum',\n 'MaritalStatus',\n 'Occupation',\n 'Relationship',\n 'Race',\n 'Gender',\n 'CapitalGain',\n 'CapitalLoss',\n 'HoursPerWeek',\n 'NativeCountry',\n 'Relationship x HoursPerWeek',\n 'Age x Relationship',\n 'EducationNum x Occupation',\n 'EducationNum x MaritalStatus',\n 'Age x HoursPerWeek',\n 'MaritalStatus x HoursPerWeek',\n 'WorkClass x CapitalLoss',\n 'Age x CapitalLoss',\n 'Occupation x Relationship',\n 'fnlwgt x HoursPerWeek']\n\n\n\n\nGlobal explanations\n\n\nebm_global = ebm.explain_global()\nshow(ebm_global)\n\n\n\n    \n    \n    \n\n\n\n\nLocal Explanations\n\nebm_local = ebm.explain_local(X_test[:5],y_test[:5])\n\n/tmp/ipykernel_312818/1167488945.py:1: FutureWarning:\n\nThe behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`.\n\n\n\n\nshow(ebm_local)\n\n\n\n    \n    \n    \n\n\n\n\nEdit the model to match expert expectations\n\ngc.visualize(ebm=ebm,x_test=X_test,y_test=y_test)\n\n\n        \n        \n    \n\n\n\n\nLoad the model and check if the changes took place\n\ngc_dict = load(open('/home/thulasiram/personal/going_deep_and_wide/Interpretable_ml/interpret_ai/interpret_ai/data/modified_model.gamchanger','r'))\n\n\nnew_ebm = gc.get_edited_model(ebm,gc_dict)\n\n\ngc.visualize(ebm=new_ebm,x_test=X_test,y_test=y_test)"
  },
  {
    "objectID": "Interpretable_ml/interpret_ai/notebooks/02_gamchanger.html",
    "href": "Interpretable_ml/interpret_ai/notebooks/02_gamchanger.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "from json import load\nimport gamchanger as gc\nimport pandas as pd\nfrom interpret.glassbox import ExplainableBoostingClassifier\n\n\nsamples = load(open('/home/thulasiram/personal/going_deep_and_wide/Interpretable_ml/interpret_ai/interpret_ai/data/sample.json','r'))\nebm_model = load(open('/home/thulasiram/personal/going_deep_and_wide/Interpretable_ml/interpret_ai/interpret_ai/data/model.json','r'))\n\n\nsamples.keys()\n\ndict_keys(['featureNames', 'featureTypes', 'samples', 'labels'])\n\n\n\nX_test = pd.DataFrame(samples['samples'],columns=samples['featureNames'])\n\n\ny_test = pd.Series(samples['labels'])\n\n\nebm = ExplainableBoostingClassifier(random_state=123)\n\n\ngc.visualize(ebm=ebm,model_data=ebm_model,sample_data=samples)\n\n\n        \n        \n    \n\n\n\ngc_dict = load(open('/home/thulasiram/personal/going_deep_and_wide/Interpretable_ml/interpret_ai/interpret_ai/data/modified_model.gamchanger','r'))\n\n\nnew_ebm = gc.get_edited_model(ebm=ebm,gamchanger_export=gc_dict)\n\nTypeError: 'NoneType' object is not iterable\n\n\n\ngc."
  },
  {
    "objectID": "Interpretable_ml/interpret_ai/notebooks/03_rulefit.html",
    "href": "Interpretable_ml/interpret_ai/notebooks/03_rulefit.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom imodels import FIGSClassifier\nfrom sklearn.datasets import load_breast_cancer\nimport os\n\n\nraw_data = load_breast_cancer()\n\n\nX = raw_data.data\ny = raw_data.target\nfeatures = raw_data.feature_names\ntarget_names = raw_data.target_names\n\n\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.20,random_state=123)\n\n\nfigs = FIGSClassifier(max_rules=4)\n\n\nfigs.fit(X_train,y_train,feature_names=features)\n\n\n> ------------------------------\n> FIGS-Fast Interpretable Greedy-Tree Sums:\n>    Predictions are made by summing the \"Val\" reached by traversing each tree\n> ------------------------------\nworst radius <= 16.795 (Tree #0 root)\n    worst concave points <= 0.136 (split)\n        Val: 0.985 (leaf)\n        worst texture <= 25.670 (split)\n            Val: 0.812 (leaf)\n            Val: 0.091 (leaf)\n    mean concavity <= 0.072 (split)\n        Val: 0.500 (leaf)\n        Val: 0.007 (leaf)\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.FIGSClassifier> ------------------------------\n> FIGS-Fast Interpretable Greedy-Tree Sums:\n>    Predictions are made by summing the \"Val\" reached by traversing each tree\n> ------------------------------\nworst radius <= 16.795 (Tree #0 root)\n    worst concave points <= 0.136 (split)\n        Val: 0.985 (leaf)\n        worst texture <= 25.670 (split)\n            Val: 0.812 (leaf)\n            Val: 0.091 (leaf)\n    mean concavity <= 0.072 (split)\n        Val: 0.500 (leaf)\n        Val: 0.007 (leaf)\n\n\n\n\nprint(figs)\n\n> ------------------------------\n> FIGS-Fast Interpretable Greedy-Tree Sums:\n>   Predictions are made by summing the \"Val\" reached by traversing each tree\n> ------------------------------\nworst radius <= 16.795 (Tree #0 root)\n    worst concave points <= 0.136 (split)\n        Val: 0.985 (leaf)\n        worst texture <= 25.670 (split)\n            Val: 0.812 (leaf)\n            Val: 0.091 (leaf)\n    mean concavity <= 0.072 (split)\n        Val: 0.500 (leaf)\n        Val: 0.007 (leaf)\n\n\n\n\nprint(figs.print_tree(X_train, y_train))\n\n------------\nworst radius <= 16.795 284/455 (62.42%)\n    worst concave points <= 0.136 275/302 (91.06%)\n        ΔRisk = 0.98 260/264 (98.48%)\n        worst texture <= 25.670 15/38 (39.47%)\n            ΔRisk = 0.81 13/16 (81.25%)\n            ΔRisk = 0.09 2/22 (9.09%)\n    mean concavity <= 0.072 9/153 (5.88%)\n        ΔRisk = 0.50 8/16 (50.0%)\n        ΔRisk = 0.01 1/137 (0.73%)\n\n\n\n\nfigs.plot(fig_size=8)\n\n\n\n\n\nfigs.complexity_\n\n4\n\n\n\nfigs.get_params()\n\n{'max_features': None,\n 'max_rules': 4,\n 'min_impurity_decrease': 0.0,\n 'random_state': None}\n\n\n\nfigs.feature_names_\n\narray(['mean radius', 'mean texture', 'mean perimeter', 'mean area',\n       'mean smoothness', 'mean compactness', 'mean concavity',\n       'mean concave points', 'mean symmetry', 'mean fractal dimension',\n       'radius error', 'texture error', 'perimeter error', 'area error',\n       'smoothness error', 'compactness error', 'concavity error',\n       'concave points error', 'symmetry error',\n       'fractal dimension error', 'worst radius', 'worst texture',\n       'worst perimeter', 'worst area', 'worst smoothness',\n       'worst compactness', 'worst concavity', 'worst concave points',\n       'worst symmetry', 'worst fractal dimension'], dtype='<U23')"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Thulasiram Gunipati",
    "section": "",
    "text": "As a seasoned professional with 7 years of experience as a Data Science Manager and Data Scientist, I have a wealth of knowledge and expertise to offer. Throughout my career, I have tackled a wide range of challenging data science projects, including building a recommendation engine for a social media app, developing look-a-like models at scale, implementing computer vision for content moderation and tagging, creating churn prediction models, constructing a data lake, and automating dashboards.\nI possess a deep understanding of various data science topics, including computer vision, natural language processing, interpretable machine learning, causal inference, and probabilistic programming. My passion lies in utilizing my data science and machine learning skills to create tangible value for organizations."
  },
  {
    "objectID": "graph_ml/02_resources.html",
    "href": "graph_ml/02_resources.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Standford course\nGeometric Deep learning youtube course\n\n\n\n\n\npytorch geometric\nNetworkx\npygod\nUGFraud\nigraph\nGephi\n\n\n\n\n\nTop 10 resources for GNN\nGetting started in Graph ML\nGraph Representational learning\nUnderstanding Deepwalk\nGraph CNN\nGraphsage\nCHEBNET\nGraph Attention Networks\nNeptune AI blog on GNN\n\n\n\n\n\nRepo of datasets\n\n\n\n\n\nSNAP node2vec tutorial\nnetworkx tutorial\n\n\n\n\n\nGraph Representation learning\nNetwork Science\nGraph Neural Networks in Action\nGraph Neural Networks\n\n\n\n\n\npytorch Graph Attention Networks\nGraph Representational Learning\nList of resources\n\n\n\n\n\n\n\n\n\n\n\n\n\nGithub repo\nMicrocluster-based Detector of Anomalies"
  },
  {
    "objectID": "graph_ml/graph_theory.html",
    "href": "graph_ml/graph_theory.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Number of Nodes N, we often call it the size of the network.\nNumber of Links, denoted L, represents the total number of interactions between the nodes.\nThe links of the network can be directed or undirected.\nA network is called directed (or digraph) if all of its links are directed. It is called undirected if all of its links are undirected. some networks simultaneously have directed and undirected links.\n\n\n\n\n\n\n\nDegree represents the number of links it has to other nodes.\nIn an undirected network the total number of links, L, can be expressed as the sum of node degrees.\n\n\nL = 1/2 ∑i=1N ki\n\nThe 1/2 factor corrects for the fact that each link is counted twice.\nFor directed networks, total degree is given by the sum of incoming and outgoing degree\n\n\n\n\nDegress in a directed network\n\n\n\n\n\nLinks in a directed network\n\n\n\n\n\n\n\nAn important property of the network\n\n\n\nAverage Degree for undirected network\n\n\n\n\n\nAverage Degree of a directed network\n\n\n\n\n\n\n\n\n\n\n\n\nThe adjacency matrix of a directed network of N nodes has N rows and N columns. Its elements being:"
  },
  {
    "objectID": "graph_ml/01_intro.html",
    "href": "graph_ml/01_intro.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Node Centrality measures the node importance in a graph. Node degree is the quantity of direct neighbours it has. clustering coefficient measures how connected are the node neighbours. Graphlets degree Vectors count how many different graphlets are rooted at a given node.\n\n\n\n2-to-5 node graphlets\n\n\n\n\n\n\nShortest distance between two nodes\nCommon neighbours of two nodes\nKatz index - Number of possible walks up to a certain length between two nodes\n\n\n\n\n\nGraphlet counts\nKernel methods measure similarity between graphs through different “bag of nodes” methods (similar to bag of words)\n\n\n\n\n\n\n\nAdvances in GraphML\n\n\n\n\n\n\n\n\nSteps in Node2Vec\n\n\n\nNode2Vec simulates random walks between nodes of a graph, then processes these walks with skip-gram, to compute embeddings\nThe drawback of Node2Vec is as the graph changes the embeddings should change. Suitable for a static graph\n\n\n\n\n\n\nGraph Convolutional Networks averages the normalised representation of the neighbours for a node (most GNNs are actually GCNs)\nGraph Attention Networks learn to weigh the different neighbours based on their importance (like transformers)\nGraphSAGE samples neighbours at different hops before aggregating their information in several steps with max pooling.\nGraph Isomorphism Networks aggregates representation by applying an MLP to the sum of the neighbours’ node representations.\n\n\n\n\nHuggingface blog"
  },
  {
    "objectID": "anomaly_detection/ad_online_event_streams.html",
    "href": "anomaly_detection/ad_online_event_streams.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "This research paper propose to tackle the online event anomaly detection problem using next activity prediction methods.\nMl models are used to predict the probabilities of next-activities and consider the events predicted unlikely as anomalies.\n\n\nThe models in unsupervised anomaly detection extract information from the data and map input matrix to a feature space. The events from a running case is provided to the classifier which calculates a distance between the running case and training data mapped int the feature space. If the target event sufficiently deviates from the training data, above the anomaly threshold, the event is denoted as anomalous.\n\n\n\nAnomaly detection using unsupervised method\n\n\n\n\n\n\n\n\nAnomaly Detection on event logs\n\n\nRandom Forest, extreme gradient Boosting, and LSTM were used for training. ML models performed better than the deep neural network.\n\n\n\nResearch Paper"
  },
  {
    "objectID": "anomaly_detection/Anomaly_based_IDS.html",
    "href": "anomaly_detection/Anomaly_based_IDS.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Research done on probing attacks in an industrial setting.\n\n\n\n\n\nIntrusion Detection Systems\n\n\n\n\n\n\n\n\nMachine Learning Methods for Anomaly Detection\n\n\n\n\n\n\n\n\nData Capture and Processing\n\n\n\n\n\nSnort and Suricata tools were used for data labelling \n\n\n\n\n\n\nFeature Selection\n\n\n\n\n\nTwo ML models were used for training. Ensemble method (Bagging) with base classifier as naive Bayes, KNN, Logistic Regression and SVM. The flow features were converted to a 32X32 matrix and a CNN model was developed.  \n\n\n\nCNN model was performing better than ensemble method.\n\n\n\nThe tradeoff between them is the complexity and the training time involved. CNN requires more computation resources and is an offline model.\n\n\n\nAnomaly Based Intrusion Detection by Machine Learning: A case study on probing attacks to an institutional network"
  },
  {
    "objectID": "anomaly_detection/ad_dynamic_graphs_using_midas.html",
    "href": "anomaly_detection/ad_dynamic_graphs_using_midas.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Real world networks are dynamic in nature and are constantly changing.\nContributions of Midas:-\n* Streaming approach\n* Theoretical Gurantee\n* Effectiveness\nMicrocluster based detector of anomalies in Edge streams (MIDAS) performs detections by considering the temporal nature of the networks and by considering micro-clusters instead of individual edges.\n\n\n\n\n\ncapturing the temporal characteristics\n\n\nStatic graphs do not capture the temporal relations.\n\n\n\n\n\n\nDetecting DDOS attack\n\n\n\n\n\nBurst in activity\n\n\nMIDAS monitors suddenly appearing bursts of activity sharing several nodes or edges that are close by in spatial locality\n\n\n\nMIDAS can give binary decisions upto a user defined threshold.\n\n\n\n\n\n\nTowards data science blog post\nGithub Repo\nResearch Paper"
  },
  {
    "objectID": "anomaly_detection/anomaly_detection_using_graphs.html",
    "href": "anomaly_detection/anomaly_detection_using_graphs.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Structural anomalies - They consider graph structural information. Abnormal nodes have different connection patterns.\nGlobal anomalies - They only consider the node attributes. Nodes with attributes significantly different from other nodes.\nCommunity anomalies - Considers both node attributes and graph structural information.\n\n\n\n\n\n\n\n\n\n\nUsing statistical features associated with each node, such as in/out degree, to detect anomalous nodes.\nProhibitive cost for assessing the most significant features and do not effectively capture the structural information.\nNetwork representation methods such as Deepwalk, Node2Vec and LINE is used to generate node representations. By pairing the conventional anomaly detection techniques such as density or distance based techniques, anomalous nodes can be identified with regard to their distinguishable locations in the embedding space.\n\n\n\n\n\nThese techniques encode the graph structure into an embedded vector space and identify anomalous nodes through further analysis.\n\n\n\n\n\n\n\n\nAutoencoder and DNN provide solid basis for learning data representations.\n\n\n\n\nAutoencoder based anomaly detection\n\n\n\n\n\n\n\n\nGCN anomaly detection\n\n\n\n\n\n\n\nIn addition to structural information and node attributes,, dynamic graphs also contain rich temporal signals. Some anomalies might appear to be normal in the graph snapshot at each time stamp, and, only when the changes in a graph’s structure are considered, do they become noticeable.\n\n\n\n\n\nResearch paper"
  },
  {
    "objectID": "anomaly_detection/ad_using_unsupervised_methods.html",
    "href": "anomaly_detection/ad_using_unsupervised_methods.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Some anomalies can be detected by Using both unsupervised and supervised methods\n\n\n\n\n\n\n\n\n\nBoxplot to detect an anomaly\n\n\n\n\n\n\n\n\nusing a histogram to detect an anomaly\n\n\n\n\n\n\n\n\nusing a distribution based approach to detect an anomaly\n\n\n\n\n\n\n\n\nGuassian Mixture model\n\n\n\n\n\n\n\n\n\nReason we need to use a multivariate detection methods\n\n\n\n\n \n\n\n\n\n\n\n\n\n\nA is detected as an anomaly but not B\n\n\n\n\n\n\n\n\nUsing LOF to detect an anomaly\n\n\n\n\n\n\n\n\nDetecting isolated outliers\n\n\n\n\n\nDifferent Neighborhood approaches to use in different situations\n\n\n\n\n\nAdvantages vs disadvantages of Neighborhood approaches\n\n\n\n\n\n\n\n\n\n\n\nOne-class SVM\n\n\n\n\n\n\n\n\n\nCluster Approach\n\n\n\n\n \n\n\n\n\nIn higher dimensions the similarity between two similar people is decreased and increased for irrelevant people - Curse of dimensionality\nIn high dimensions, distance metrics such as Eculidean distance and neighborhood concept   does not make sense\n\n\n\nDimensions Reduction Techniques\n\nPCA\nMatrix / Tensor Factorization\nAutoencoder\n\nAngle-based outlier detection\nEnsemble Approaches\n\nIsolation Forest\nFeature Bagging\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\nUsing Tensor Factorization to capture temporal features\n\n\n\n\n\nUsing Tensor Factorization with other methods to find anomalies\n\n\n\n\n\n\n\n\n\nUsing reconstruction errors to detect anomalies\n\n\n\n\n\n\n\n\n\nExamine the variance over the angles\n\n\n\n\n\n\n\n\n\n\n\n\nUsing Average deptth to Isolate Outliers"
  },
  {
    "objectID": "nlp/transformers_production.html",
    "href": "nlp/transformers_production.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Techniques to speed up predictions and reduce the memory footprint of transformer models:-\n\nKnowledge distillation\nQuantization\nPruning\nGraph Optimization\n\n\n\nIt is a general purpose method for training a smaller student model to mimic the behavior of a slower, larger but better-performing teacher.\nFor supervised tasks like fine-tuning, the main idea is to augment the ground truth labels with a distribution of “soft probabilities” from the teacher which provide complementary information for the student to learn from. For example, if our BERT-base classifier assigns high probabilities to multiple intents, then this could be a sign that these intents lie close to each other in the feature space. By training the student to mimic these probabilities, the goal is to distill some of this “dark knowledge” that the teacher has learned—that is, knowledge that is not available from the labels alone.\nWe “soften” the probabilities by scaling the logits with a temperature hyperparameter T before applying the softmax.\n\n\n\nsoftened probabilities\n\n\nThe student also produces softened probabilties of its own. We then measure the Kullback-Leibler (KL) divergence to measure the difference between two probability distributions. Using this we can define knowledge distillation loss.\nFor classification tasks the student loss is a weighted average of the distillation loss with the usual cross-entropy loss of the ground truth labels.\n\n\n\n\nSelect model with fewer parameters and size\nSelect model from the same family type as the teacher\n\n\n\n\n\nThis makes computation more efficient by representing the weights and activations with low-precision data types.\nThree different types of quantization for DNN models:\nDynamic quantization - Quantization happens only during inference. Weights and model’s activation are quantized. Conversion between FP32 and INT8 is a performance bottleneck.\nStatic quantization - Quantization scheme is decided ahead of inference by taking a sample, calculating the quantization and saving. Challenge of discrepancy between precision during training and inference.\nQuantization aware training - The effect of the quantization can be effectively simulated during trainig by “fake” qunatization of the FP32 values. Instead of using INT8 values during training, the FP32 values are rounded to mimic the effect of quantization. This is done during both the forward and the backward pass and improves performance in terms of model metrics over static and dynamic quantization.\nThe main bottleneck for running inference with transformers is the compute and memory bandwidth associated with the enormous numbers of weights in these models. For this reason, dynamic quantization is currently the best approach for transformer-based models in NLP.\n\n\n\nWhen a model is exported to the ONNX format, a common set of operators is used to construct a computational graph that represents the flow of data through the neural network. This graph makes it easy to switch between frameworks.\nONNX can be coupled with accelerator like ONNX Runtime (ORT). ORT provides tools to optimize the ONNX graph through techniques like operator fusion and constant folding, and defines an interface to execution providers that allow you to run the model on different types of hardware.\n\n\n\nArchitecuture of ONNX Runtime\n\n\n\n\n\nIn case if the model is required to be deployed on mobile, we may need to shrink the number of parameters in our model by identifying and removing the least important weigths in the network.\nThe main idea behind pruning is to gradually remove weight connections during training such that the model becomes progressively sparser. The non-zero parameters are stored in a compact sparse matrix format. Pruning can be combined with quantization to obtain further compression.\n\n\n\nPruning neurons and synapses\n\n\n\nWeight Pruning methods\nMagnitude Pruning\n\n\n\nIt calculates the scores according to the magnitude of the weights. In this method sparsity is increased from an initial value to the final value after a number of steps.\nThe masks (where weigts are zero) are reactivated during training and recover from any potential loss in accuracy that is induced by the pruning process. The pruning is highest in the early process and gradually tapers off.\nThis method is designed for supervised learning and may not work for transfer learning.\n\n\n\nThe basic idea behind movement pruning is to gradually remove weights during fine-tuning such that the model becomes progressively sparser. The key novelty is that both the weights and the scores are learned during fine-tuning. So, instead of being derived directly from the weights (like with magnitude pruning), the scores in movement pruning are arbitrary and are learned through gradient descent like any other neural network parameter. This implies that in the backward pass, we also track the gradient of the loss with respect to the scores.\n\n\n\nDistribution of remaining weights for Magnitude pruning and movement pruning\n\n\nNeural Networks Block Movement Pruning package can be used for pruning.\n\n\n\n\n\nNotes from the book Natural language processing with Transformers, chapter 8"
  },
  {
    "objectID": "nlp/transformer_models.html",
    "href": "nlp/transformer_models.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Uses knowledge distillation during pretraining. Achieves 97% of BERT’s performance while using 40% less memory and being 60% faster.\n\n\n\nIt is trained longer, on larger batches with more training data data, and it drops the Next Sentence Prediction (NSP) task. It improve performance compared to the BERT model\n\n\n\nSeveral pretraining objectives were considered. Autoregressive language modeling from GPT like models, Masked Language Model (MLM) from BERT. In addition they introduced translation language modeling (TLM) which is an extension of MLM to mutiple language inputs.\n\n\n\nTrained on 2.5 TB data, TLM was dropped.\n\n\n\nThe ALBERT model introduced three changes to make the encoder architecture more efficient. First, it decouples the token embedding dimension from the hidden dimension, thus allowing the embedding dimension to be small and thereby saving parameters, especially when the vocabulary gets large. Second, all layers share the same parameters, which decreases the number of effective parameters even further. Finally, the NSP objective is replaced with a sentence-ordering prediction: the model needs to predict whether or not the order of two consecutive sentences was swapped rather than predicting if they belong together at all. These changes make it possible to train even larger models with fewer parameters and reach superior performance on NLU tasks.\n\n\n\nOne limitation of the standard MLM pretraining objective is that at each training step only the representations of the masked tokens are updated, while the other input tokens are not. To address this issue, ELECTRA uses a two-model approach: the first model (which is typically small) works like a standard masked language model and predicts masked tokens. The second model, called the discriminator, is then tasked to predict which of the tokens in the first model’s output were originally masked. Therefore, the discriminator needs to make a binary classification for every token, which makes training 30 times more efficient. For downstream tasks the discriminator is fine-tuned like a standard BERT model.\n\n\n\nThe DeBERTa model introduces two architectural changes. First, each token is represented as two vectors: one for the content, the other for relative position. By disentangling the tokens’ content from their relative positions, the self-attention layers can better model the dependency of nearby token pairs. On the other hand, the absolute position of a word is also important, especially for decoding. For this reason, an absolute position embedding is added just before the softmax layer of the token decoding head.\n\n\n\n\n\nGPT\nGPT-2\nCTRL (Conditional Transformer Language) The model addresses the issue by adding “control tokens” at the beginning of the sequence. These allow the style of the generated text to be controlled, which allows for diverse generation\nGPT-3\nGPT-Neo / GPT-J-6B\n\n\n\n\n\n\nThe T5 model unifies all NLU and NLG tasks by converting them into text-to-text tasks. All tasks are framed as sequence-to-sequence tasks, where adopting an encoder-decoder architecture is natural. For text classification problems, for example, this means that the text is used as the encoder input and the decoder has to generate the label as normal text instead of a class.\n\n\n\nIt combines the pretraining procedures of BERT and GPT.The input sequences undergo one of several possible transformations, from simple masking to sentence permutation, token deletion, and document rotation. These modified inputs are passed through the encoder, and the decoder has to reconstruct the original texts.\n\n\n\nFirst translation model that can translate between any of 100 languages. The model uses prefix tokens (similar to the special [CLS] token) to indicate the source and target language.\n\n\n\nThis model tackles quadratic memory requirements of the attention mechanism. It uses a sparse form of attention that scales linearly. [4096 tokens considered in BigBird]\n\n\n\n\n\nBook - Natural Language Processing with Transformers, second edition"
  },
  {
    "objectID": "nlp/chatgpt.html",
    "href": "nlp/chatgpt.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "ChapGPT is trained in three main stages\n\nGenerative pretraining\nSupervised fine-tuning (SFT)\nReinforcement learning from human feedback (RLHF)"
  },
  {
    "objectID": "nlp/multilingual_transformers.html",
    "href": "nlp/multilingual_transformers.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "The corpus used for pretraining consists of documents in many languages. A remarkable feature of this approach is that despite receiving no explicit information to differentiate among the languages, the resulting linguistic representations are able to generalize well across languages for a variety of downstream tasks.\n\n\n\nmBERT\nXLM - RoBERTa (XLM- R)\n\n\n\n\n\nUses MLM as a pretraining objective, for 100 languages\nHuge size of the training corpus (2.5 TB data)\nUses SentencePiece to tokenize the raw text directly\nVocabulary size is 250,000 tokens"
  },
  {
    "objectID": "nlp/text_preprocessing.html",
    "href": "nlp/text_preprocessing.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Steps involved in tokenization\n\n\n\n\n\nStripping whitespace\nLowercasing\nUnicode Normalization - There exist various ways to write the same character. Unicode Normalization schemes like NFC, NFD, NFKC and NFKD replace various ways to write the same character with standard forms.\nConverting numbers to characters\nExpanding abbrevations\nRemoving characters like @\nCorrecting spellings\n\n\n\n\n\nSplitting text into words\n\n\n\n\n\nSplit into subwords with Byte-Pair Encoding (BPE)\nTokenizer needs to be trained on the corpus or that has been trained if we are using a pretrained tokenizer.\nThe words are divided into subwords to reduce the size of the vocabulary and try to reduce the number of out-of-vocabulary tokens\nAlgos for subword tokenization are\n\nBPE\nUnigram\nWordPiece\n\n\n\n\n\n\nAdding special tokens at the beginning or end\n\n\n\n\nIt encodes each input text as a sequence of Unicode characters. (useful for handling multilingual corpora)"
  },
  {
    "objectID": "nlp/instruction_fine_tuned_txt_embedding.html",
    "href": "nlp/instruction_fine_tuned_txt_embedding.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Instruction Finetuned Text Embeddings\nAn instruction-finetuned text embedding model that can generate text embeddings tailored to any task (e.g., classification, retrieval, clustering, text evaluation etc.) and domains(e.g,, science, finance etc) by simply providing the task instruction, without any finetuning.\n\nInstructor can calculate domain-specific and task-aware embeddings without any further training. It can be applied to any task for computing fixed-length embeddings of text.\n\nArchitecture\n\nBuilt on single encoder architecture\nGTR models are used as backbone\nGiven an input text and a task instruction, this model encodes their concatenation, then a fixed sized task specific embedding is generated\nThe model is trained by maximizing the similarity between positive pairs and minimize negative pairs\n\n\n\nDataset\n\n330 datasets with instructions across diverse task categories and domains was constructed. The dataset is known as Multitask Embeddings Data with Instructions (MEDI)\nInputs for Instruction\n\nText type\nTask objective\nDomain\n\n\n\n\nTraining\n\nInstructor is initialized with GTR-Large model and finetune it on MEDI using AdamW optimizer and finetuned for 20k steps\n\n\n\nEvaluation\n\nIntructor is evaluated on 70 downstream tasks. Out of 70 evaluation tasks, 66 are unseen during training\n\n\n\n\nInstruction examples for evaluation dataset\n\n\n\n\nResults\nInstructor significantly outperforms prior state-of-the-art embedding models by an average of 3.4% over the 70 diverse datasets. (This is despite the fact that Instructor has one order of magnitude fewer parameters i.e 335M)\n!(As the Instructions become detailed, the performance improves)[/Images/instruction_gtre.png]\n\n\n\nThe performance of Instructor increases with model size\n\n\nInstructions mitigate domain shifts\n\nInstructor largely inproves the GTR-Large’s performance on three unseen domains\n\n\n\n\nInstructor performance on unseen domain data\n\n\n\n\n\nT-SNE with and without instruction\n\n\n\n\nReferences:-\n\nInstructor Github page\nResearch paper\nCode\nInstructor model hosted on Huggingface"
  },
  {
    "objectID": "nlp/text_summarization.html",
    "href": "nlp/text_summarization.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Text summarization is of two types abstract and extractive. Abstract summarization will consist of words/sentence which is not in the original text. Extractive summarization will consist of sentences which are part of the original text.\n\n\nTo find a pretraining objective that is closer to summarization than general language modeling, the authors identified, in a very large corpus, sentences containing most of the content of their surronding paragraphs and pretrained the PEGA model to reconstruct these sentences, thereby obtaining a state of the art model for text summarization.\n\n\n\nPEGASUS predicting masked words and sentences\n\n\n\n\n\n\n\nWe look at n-grams, when we compare the generated text with reference text, we count the number of words in the generation that occur in the reference text and divide it by the length of the generated text. (A word is only counted as many times as it occurs in the reference).\nBELU is based on precision and recall is not considered because for generated text there can be multiple reference texts.\nThe above method benefits short sentences over longer ones. To conpensate for that a brevity penalty is used.\n\n\n\nFor summarization high recall is important than just precision. In ROUGE we also check how many n-grams in the reference text also occur in the generated text.\nROUGE is F-1 score with harmonic mean of both precision and recall ROUGE scores"
  },
  {
    "objectID": "nlp/text_generation.html",
    "href": "nlp/text_generation.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Instead of decoding the token with the highest probability at each step, beam search keeps track of the top-b most probable next tokens, where b is referred to as the number of beams or partial hypotheses. The next set of beams are chosen by considering all possible next-token extensions of the existing set and selecting the b most likely extensions. The process is repeated until we reach the maximum length or an EOS token, and the most likely sequence is selected by ranking the b beams according to their log probabilities.\n\n\n\nBeam Search\n\n\nBeam Search with n-gram penalty - A penalty is added to the beam search to avoid repetetion of the text generated. Another alternative is to avoid repetition is to use sampling."
  },
  {
    "objectID": "nlp/rnn.html",
    "href": "nlp/rnn.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "RNN deals with the problem of having different amounts of input values in a network. RNN consists of feedback loops.\n\n\n\nFeedback loop in RNN\n\n\nIn an RNN the weights an biases are shared i.e they are the same. The more we unroll a RNN, the more difficult it is to train. This is called the vanishing / Exploding gradient problem.\n\n\nLSTM uses two separate paths to make predictions. One path is for long term memories and the other is for short term memory. The long term memory does not consist of any weights and biases thus it will not suffer from vanishing/exploding gradients.\n\n\n\nLSTM Cell Architecture"
  },
  {
    "objectID": "nlp/gpt.html",
    "href": "nlp/gpt.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Trained on Webtext - 40 GB dataset\nGPT-2 is built using only transformer decoder blocks\nGPT-2 uses byte pair encoding to create tokens for its vocabulary\nFew differences between GPT2 and BERT\n\nGPT-2 uses decoder blocks and BERT uses encoder blocks\nGPT-2 is a autoregressive model and BERT is not autoregressive. By losing autoregressive, BERT gained the ability to incorporate the context on both sides of a word to gain better results.\nGPT-2 uses masked self attention to prevent looking at tokens from the future. BERT does not do that.\nGPT-2 is trained as a language generation model (predicting next word) while BERT is trained to use as a backbone in transfer learning for different downstream tasks.\n\nThe GPT-2 decoder blocks are similar to the original transformer block except that they do not have the second self-attention layer which gets inputs from the encoder blocks.\nGPT-2 can process 1024 tokens\nGPT-2 had 12 attention heads\nWe can allow GPT-2 to ramble on its own or we can give it a prompt to have it speak about a certain topic.\n\n\n\n\n\nThe architecture is a transformer decoder model.\nA total of 96 transformer decoder layers.\n300 Billion word tokens are used for training.\nThe model consists of 175 Billion parameters.\nGPT is 2048 tokens wide. This is called its “context window”\nThe difference between GPT2 and GPT3 is the alternating dense and sparse self-attention layers\n\n\n\n\n\nDecoder Model"
  },
  {
    "objectID": "nlp/ner_transformer.html",
    "href": "nlp/ner_transformer.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "NER with Transformers\n\n\n\nNER as token classification task\n\n\n\n\n\nHugging Face Transformer class"
  },
  {
    "objectID": "nlp/qa.html",
    "href": "nlp/qa.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Getting specific answers to the user question by searching through oceans of documents.\nThere are many flavours of QA systems\n* Extractive QA \n* Abstractive or Generative QA \n* Community QA\n* long-form QA\n* QA over structured data\n\n\nThis is a type of QA where the answer is identified as a span of text in a document.\n\n\nThe supervised model should predict the starting and ending position of the answer tokens.\n\n\n\nSpan classification\n\n\n\n\n\n\nMiniLM\nRoBERTa-base\nALBERT-XXL\nXLM-RoBERTa-large\n\n\n\n\nUsing sliding windows to deal with long passages\n\n\n\n\nModern QA systems are based on retriever-reader architecture. Retriever gets the relevant documents for a given query. Retrievers are categorized as sparse or dense. Spare retrievers use word frequencies and dense retrievers use encoders to get contextualized embeddings.\nReader extracts the answer from the documents provided by the retriever. Reader is usually a reading comprehension model\n\n\n\nQA architecure\n\n\nElasticsearch can be used as a document store. FAISS can also be used as a document store.\nDense passage retrieval(DPR) uses bi-encoder architecture for computing the relevance of a document and query\n\n\n\nBi-encoder architecture\n\n\n\n\n\n\nRecall - The fraction of all relevant documents that are retrieved.\nMean Average Precision (mAP) - Rewards retrievers that place correct answers higher up in the document ranking.\n\n\n\n\n\nExact Match (EM)\nF1-Score\n\nExact match is a very strict metric and F1 score is optimistic. Better to track both of them to get a good understanding of the reader performance\n\n\n\nIf the domain dataset is very small compared to the data used by pre-trained model, then fine-tuning using only the domain dataset may give less performance boost. Fine-tuning using both the domain and the data used by the pre-trained model is recommended.\n\n\n\n\nAt times, the answer to a question may be distributed across documents and paragraphs. Extractive QA cannot be used in such cases. In such cases the answer can be generated with a pretrained language model. One such model is Retrieval Augmented Generation (RAG)\nRAG-Token is a category of RAG model which can use a different document to generate each token in the answer. This allows the generator to synthesize evidence from multiple documents.\n\n\n\nRAG\n\n\n\n\n\nTo implement QA sytems, we can start with providing user with good search capabilities. Then we can go for extraction based methods followed by answer generation techniques.\n\n\n\n\n\nMultimodal QA using text, images, tables etc.\nQA over a knowledge graph\n\n\n\n\n\nHaystack\n\n\n\n\n\nNotes from Natural language processing with Transformers book\n\n\n\n\n\nCloudera Fast Forward lab blog articles\nGrid Dynamics Blog\nHaystack Tutorials"
  },
  {
    "objectID": "nlp/MusicLM.html",
    "href": "nlp/MusicLM.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Generating high-fidelity music from text descriptions\nConditional music generation as a hierarchical sequence-to-sequence modeling task and it generates music at 24kHZ that remains consistent over several minutes.\nIt can be conditioned on both text and a melody - It can transform whistled and hummed melodies according to the style described in a text caption.\n\n\n\n\n\nScarcity of paired audio-text data. Text descriptions of general audio is harder. Not possible to capture with a few words the characteristics of acoustic scenes.\nAudio is structured along a temporal dimension which makes sequence-wide captions a much weaker level of annotation than an image caption\n\n\n\n\n\nQuantization\nGenerative models for audio\nConditioned audio generation\nText conditioned Image generation\nJoint embedding models for music and text (MuLan)\n\nQuantization - The goal is to provide a compact, discrete representation, which at the same time allows for high-fidelity reconstruction. (VQ-VAE)\nSoundStream is a universal neural audio codec - compress audio and reconstruct - tokenizer. SoundStream uses residual vector quantization (RVQ)\n\n\n\n\nMusicCaps - High quality music caption dataset with 5.5K examples prepared by expert musicians.\n\n\n\n\n\n\n\n\nJoint music-text model that is trained to project music and its corresponding text description to representations close to each other in an embedding space. This shared embedding space eliminates the need for captions at training time altogether, and allows training on massive audio-only corpora.\nIt is a music-text joint embedding model consisting of two embedding towers, one for each modality. The towers map the two modalities to a shared embedding space of 128 dimensions using contrastive learning.\n\n\n\n\naudio –> embeddings –> quantization (RVQ) –> acoustic tokens. Each second of audio is represented by 600 tokens, referred as audio tokens.\n\n\n\nAn intermediate layer of MLM module of a w2v-BERT model with 600M parameters are used. Embeddings are extracted from the 7th layer and quantize them using the centroids of a learned k-means over the embeddings. We use 1024 clusters and a sampling rate of 25 Hz, resulting in 25 semantic tokens for every second of audio.\n\n\n\n\nSoundStream, w2v-BERT, MuLan are pretrained independently and frozen, such that they provide the discrete audio and text representations for the sequence-to-sequence modeling.\n\n\n\nTo achieve text-conditioned music generation, hierarchical modeling is proposed. Each stage is modeled autogressively by a separate decoder-only transformer.\n\n\n\n\nPretrained MuLan\nSoundStream and w2v-BERT trained on Free Music Archive (FMA) dataset. They are trained on a dataset amounting to 280K hours of music\n\n\n\n\nMusicCaps dataset was prepared. It consists of 5.5K music clips each paired with corresponding text descriptions in English, written by ten professional musicians. For each 10-second music clip, MusicCaps provides: (1) a free-text caption consisting of four sentences on average, describing the music and (2) a list of music aspects, describing genre, mood, tempo, singer voices, instrumentation, dissonances, rhythm, etc. On average, the dataset includes eleven aspects per clip\n\n\n\nAudio quality\nAdherence to text description\n\nFrechet Audio Distance (FAD) is a reference-free audio quality metric, low scores are preferred\n\nKL Divergence (KLD)\n\nThere is many to many relationship between text descriptions and music clips compatible with them. To overcome this a proxy was adopted. A classifier trained on multi-label classification on AudioSet is used to compute class predictions for both the generated and the reference music and measure the KL divergence between probability distributions of class predictions. KLD is expected to be low.\n\nMuLan Cycle Consistency (MCC) - High preferred.\n\nAs a joint musictext embedding model, MuLan can be used to quantify the similarity between music-text pairs. We compute the MuLan embeddings from the text descriptions in MusicCaps as well as the generated music based on them, and define the MCC metric as the average cosine similarity between these embeddings.\n\nQualitative evaluation\nTraining data memorization\n\nTo study the extent to which MusicLM might memorize music segments.\n\n\n\n\n\n\n\n\nMusicLM compared with Mubert and Riffusion. MusicLM perform better than Mubert and Riffusion.\n\nwhen the transformer models are directly trained on acoustic tokens from MuLAN tokens, there is a drop n KLD and MCC. Semantic modeling facilitate the adherence to the text description.\n\n\n\n\nPotential misappropriation of creative content - conducted a thorough study of memorization - when feeding MuLan embeddings to MusicLM, the sequences of generated tokens significantly differ from the corresponding sequences in the training set.\n\n\n\n\n\nMusicLM website with Music samples generated\nMusicLM Research Paper"
  },
  {
    "objectID": "nlp/bert.html",
    "href": "nlp/bert.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "It comes in two model sizes BERT Base and BERT Large.\n\n\n\nBERT model Sizes\n\n\nBERT is a trained transformer Encoder stack.\nThey have large feedforward networks (768 and 1024 hidden units) and 12 and 16 attention heads respectively.\nBERT has a [CLS] token which is used for classification. The output from [CLS] will go to a feedforward network along with softmax for classification.\n\n\n\nClassification with BERT\n\n\nBERT builds on clever ideas like ELMo, ULMFiT and Transformers.\n\n\nThe embedding for a word will remain the same irrespective of the context. ELMo provides contextualized word-embeddings. ELMo looks at the entire sentence before assigning each word in it an embedding. It uses Bi-directional LSTM (previous word and next words) trained on a specific task to be able to create embeddings.\nELMo gained its language understanding from being trained to predict the next word in a sequence of words - Language Modeling.\n\n\n\nELMo Bi_directional LSTM Training\n\n\nELMo comes up with the contextualized embedding through grouping together the hidden states (and initial embedding) in a certain way (concatenation followed by weighted summation).\n\n\n\nConcatenation and weighted summation of embedding\n\n\n\n\n\nULM-FiT introduced a language model and process to effectively fine-tune that language model for various tasks.\n\n\nIt turned out that we don’t need an entire transformer to adopt transfer learning, we can do with just the decoder of the Transformer.\nThe model stacked twelve decoder layers. Since there is no encoder in this set up, these decoder layers would not have the encoder-decoder attention sublayer that vanilla transformer decoder layers have. It would still have the self-attention layer, however (masked so it doesn’t peak at future tokens).\n\nThe Transformer was trained on 7000 books to learn long-term dependencies.\nThe OpenAI transformer once trained can be used for using it for downstream tasks.\n\n\n\nUsing different input transformations to carry out different tasks using OpenAI transformer\n\n\n\n\n\n\nELMo’s language model was bi-directional and OpenAI transformer only trains a forward language model.\nBERT randomly masks 15% of the input and predict the missing word. (A language modeling task)\n\nBERT also uses two-sentence tasks to handle different tasks.\n\n\n\n\n\n\n\n\n\n\n\nWhich layer weights should be used for feature extraction\n\n\n\n\n\n\n\nJ Alammar Blog on BERT\nNotebook on Fine tuning BERT for classification\nAn Alternative method to using BERT for classification"
  },
  {
    "objectID": "nlp/IE.html",
    "href": "nlp/IE.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Graph algorithms are used in unsupervised fashion. The nodes are weighted according to the frequency of the words and their connection to other words in the text. The top N nodes which are important are returned as key phrases. textacy is a package which can be used for key phrase extraction.\n\n\n\nIt is a sequence labeling challenge. The context of the surronding words and their POS tags are considered for creating features. conditional random fields (CRFs) algorithm is one of the popular choices for training NER. BIO scheme is used to annotate the text. B - Beggining, I - Intermediate and O - other. For example, First Name and last Name are B and I respectively. MITIE is a library to train NER systems. Stanford NER, spaCy and AllenNLP have pre-trained NER models.\nTo customize NER model to a domain or use case, use customized heuristics for the problem domain (using tools such as RegexNER and EntityRuler) or use active learning tools like Prodigy\n\n\n\nNER and NED together are known as named entity linking (NEL). NEL needs to go beyond POS tagging and require parsing to identify items like subject, verb and object. It also requires coreference resolution to resolve and link multiple references to the same entity. This is modeled as supervised ML problem. It is common to use off-the-shelf services for NEL.\nDBpedia Spotlight is a popular tool for entity linking.\n\n\n\nIt is the task of extracting entities and relationshops between them from text documents.\nIt can be treated as supervised classification. It can be modeled as a two step classification problem\n\nIf two entities are related\nIf they are related what is the relation between them\n\n\n\n\nDuckling library can be used to extract temporal events. when we run the sentence “Let us meet at 3 p.m. today and decide on what to present at the meeting on Friday” through Duckling. It’s able to map “3 p.m. today” to the correct time on a given day."
  },
  {
    "objectID": "nlp/dealing_few_labels.html",
    "href": "nlp/dealing_few_labels.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Adapt a pretrained model for another task without training it.\ntext entailment\nunsupervised data augmentation (UDA) uncerainty-aware self training (UST)\nMultilabel classification can be casted as a one-versus-rest classification tasks.\n\n\n\nscikit-multilearn for working with multi label challenges"
  },
  {
    "objectID": "nlp/algos_for_chatbots.html",
    "href": "nlp/algos_for_chatbots.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Rasa uses Dual Intent and Entity Transformer (DIET) architecture to detect both Intent and Entity using Transformer.\n\n\n\nDIET Architecture\n\n\nThe DIET architecture can be customized to a specific domain. The architecture can be thought of as lego blocks. We can customize and use the blocks we want for our use case. Depending on the accuracy, latency and memory usage of the use case we can decide which components to use.\n\n\n\nTED is used to decide what next action to be taken by the bot given the conversation with the user.\n\n\n\nCreating Features for TED using Intent, Entity, Slots and Previous actions\n\n\n\n\n\nTraining Transformers for next action prediction using features created and action to be taken at that particular time step to calculate loss\n\n\n\n\n\nPredicting next action\n\n\nTED is not trained on sequence of events which happen in the dialog. In dialog lot of context switching will happen. Using Transformers here will be useful because transformers can pay attention to dialog events which occured in the past.\n\n\n\nAn example of TED paying attention to past events to decide next action\n\n\n\n\n\n\n\nUsing DIET backend for response selection\n\n\n\n\n\n\n\nResearch paper on DIET architecture\nResearch paper on TED architecture"
  },
  {
    "objectID": "nlp/starspace.html",
    "href": "nlp/starspace.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Starspace\nUsing Starspace we can train domain specific embeddings.\n\n\n\nTraining embedding for a specific domain using documents and their tags"
  },
  {
    "objectID": "nlp/chatbot.html",
    "href": "nlp/chatbot.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Chatbots are classified into two broad categories\n\nGoal oriented dialogs\nchitchats\n\nTypes of chatbots\n\nExact answer or FAQ bot\nFlow based bot\nOpen-ended bot\n\n\n\n\n\n\n\nRASA is a Task oriented dialogue system. It is a framework that makes it easier to build custom chatbots\nThere are two mechanisms inside Rasa to drive conversational AI:\n\nNLU\nDialog Policy\n\n\n\n\nIt accepts text and turn it into intents and entities. It can be rule based or based on neural networks.\n\n\nUsing Regular Expressions. It Can’t handle things they haven’t seen before.It is more lightweight and require domain knowledge.\n\n\n\nTransformer based model (DIET) that sorts text into intents and entities based on examples it’s been provided. It require more compute and training examples. It is good at handling thins they haven’t seen before.\n\n\n\n\nIt predicts the next action to take. The next action is determinec by current intent and entire conversation so far.\nIt is again based on rules or neural networks.\n\n\nRule based - Traditional method. Can’t handle digressions and are difficult to extend\n\n\n\nNeural - Transformers (TED) picks the next best turn based on conversation so far and all the conversations it’s been trained on.\n\n\n\n\nIt works better with more high quality data. we have to manually Review and annotate the conversations. correct any errors made by the bot and add them to the training data, retrain and redeploy. This approach is called Conversation Driven Development.\n\n\n\nWrapper frameworks are available for Rasa, which eases the data annotation process to generate large-scale dialog datasets. Chatette is such a framework\n\n\n\n\n\n\nRASA Architecture"
  },
  {
    "objectID": "create_python_package.html",
    "href": "create_python_package.html",
    "title": "Python Package",
    "section": "",
    "text": "Python Package I am currently developing\nThe aim of this package is to reduce the workload of a Data Scientist by providing a simple and easy-to-use APIs for Data Cleaning.\n\nGithub repo for the Python Package I am developing\nLink to Python Package\nDocumentation for the Python Package"
  },
  {
    "objectID": "distributed_processing/fugue_quickstart.html",
    "href": "distributed_processing/fugue_quickstart.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Fugue Quickstart\n\nImport the required libraries\n\nimport warnings\nwarnings.filterwarnings(action='ignore')\n\n\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.getOrCreate()\n\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nfrom typing import List, Dict, Iterable, Any\n\n\n\nCreate a model in Sklearn and do predictions using Spark\n\n\nX = pd.DataFrame({\"x_1\": [1, 1, 2, 2], \"x_2\":[1, 2, 2, 3]})\ny = np.dot(X, np.array([1, 2])) + 3\nreg = LinearRegression().fit(X, y)\n\n\n# define our predict function\ndef predict(df: pd.DataFrame, model: LinearRegression) -> pd.DataFrame:\n    \"\"\"\n    Function to predict results using a pre-built model\n    \"\"\"\n    return df.assign(predicted=model.predict(df))\n\n# create test data\ninput_df = pd.DataFrame({\"x_1\": [3, 4, 6, 6], \"x_2\":[3, 3, 6, 6]})\n\n# test the predict function\npredict(input_df, reg)\n\n\n\n\n\n  \n    \n      \n      x_1\n      x_2\n      predicted\n    \n  \n  \n    \n      0\n      3\n      3\n      12.0\n    \n    \n      1\n      4\n      3\n      13.0\n    \n    \n      2\n      6\n      6\n      21.0\n    \n    \n      3\n      6\n      6\n      21.0\n    \n  \n\n\n\n\n\n# import Fugue\nfrom fugue import transform\n\n# create a spark dataframe\nsdf = spark.createDataFrame(input_df)\n\n# use Fugue transform to switch exection to spark\nresult = transform(\n    df=sdf,\n    using=predict,\n    schema=\"*,predicted:double\",\n    params=dict(model=reg),\n    engine=spark\n)\n\n# display results\nprint(type(result))\nresult.show()\n\n<class 'pyspark.sql.dataframe.DataFrame'>\n\n\n[Stage 2:==========================================>               (8 + 3) / 11]\n\n\n+---+---+------------------+\n|x_1|x_2|         predicted|\n+---+---+------------------+\n|  3|  3|              12.0|\n|  4|  3|              13.0|\n|  6|  6|20.999999999999996|\n|  6|  6|20.999999999999996|\n+---+---+------------------+\n\n\n\n                                                                                \n\n\n\n\nDo the predictions in Dask\n\n# using transform to bring predict to dask execution\nresult = transform(\n    df=input_df.copy(),\n    using=predict,\n    schema=\"*,predicted:double\",\n    params=dict(model=reg),\n    engine=\"dask\"\n)\n\n# display results\nprint(type(result))\nresult.compute().head()\n\n<class 'dask.dataframe.core.DataFrame'>\n\n\n\n\n\n\n  \n    \n      \n      x_1\n      x_2\n      predicted\n    \n  \n  \n    \n      0\n      3\n      3\n      12.0\n    \n    \n      0\n      4\n      3\n      13.0\n    \n    \n      0\n      6\n      6\n      21.0\n    \n    \n      0\n      6\n      6\n      21.0\n    \n  \n\n\n\n\n\n\nReturn the output as a Pandas Dataframe\n\n# use as_local=True to return a Pandas DataFrame\nlocal_result = transform(\n    df=input_df,\n    using=predict,\n    schema=\"*,predicted:double\",\n    params=dict(model=reg),\n    engine=\"dask\",\n    as_local=True\n)\n\nprint(type(local_result))\nlocal_result.head()\n\n<class 'pandas.core.frame.DataFrame'>\n\n\n\n\n\n\n  \n    \n      \n      x_1\n      x_2\n      predicted\n    \n  \n  \n    \n      0\n      3\n      3\n      12.0\n    \n    \n      1\n      4\n      3\n      13.0\n    \n    \n      2\n      6\n      6\n      21.0\n    \n    \n      3\n      6\n      6\n      21.0\n    \n  \n\n\n\n\n\n\nType Hints\nThe input type annotation tells Fugue what to convert the input data to before the function is applied whereas the output type annotation informs Fugue how to convert it back to a Pandas, Spark, Dask, or Ray DataFrame.\n\n\nSchema\nWhen using transform() function, the best practice is to provide schema definition.When using the transform(), the * in a schema expression means all existing columns. From there we can add new columns by adding “,column_name:type”\n\ndf = pd.DataFrame({\"a\": [1,2,3], \"b\": [1,2,3], \"c\": [1,2,3]})\n\n\ndef add_col(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Function that creates a column with a value of column a + 1.\n    \"\"\"\n    return df.assign(new_col=df[\"a\"] + 1)\n\ntransform(\n    df=df, \n    using=add_col, \n    schema=\"*,new_col:int\"\n    )\n\n\n\n\n\n  \n    \n      \n      a\n      b\n      c\n      new_col\n    \n  \n  \n    \n      0\n      1\n      1\n      1\n      2\n    \n    \n      1\n      2\n      2\n      2\n      3\n    \n    \n      2\n      3\n      3\n      3\n      4\n    \n  \n\n\n\n\n\n\nPartitioning\nThe type hint conversion is applied on the partition level.\n\ndf = pd.DataFrame({\"a\": [1,2,3,4], \"b\": [1,2,3,4], \"c\": [1,2,3,4]})\n\ndef size(df: pd.DataFrame) -> Iterable[Dict[str,Any]]:\n    \"\"\"\n    Function that calculates the size of a DataFrame.\n    \"\"\"\n    yield {\"size\":df.shape[0]}\n\n\ntransform(\n    df=df, \n    using=size, \n    schema=\"size:int\", \n    engine=\"dask\",\n    as_local=True\n    )\n\n\n\n\n\n  \n    \n      \n      size\n    \n  \n  \n    \n      0\n      1\n    \n    \n      1\n      1\n    \n    \n      2\n      1\n    \n    \n      3\n      1\n    \n  \n\n\n\n\nThe type hint conversion happens on each partition. We can control the partition by specifying the column.\n\ndf = pd.DataFrame({\"col1\": [\"a\",\"a\",\"a\",\"b\",\"b\",\"b\"], \n                   \"col2\": [1,2,3,4,5,6]})\ndf\n\n\n\n\n\n  \n    \n      \n      col1\n      col2\n    \n  \n  \n    \n      0\n      a\n      1\n    \n    \n      1\n      a\n      2\n    \n    \n      2\n      a\n      3\n    \n    \n      3\n      b\n      4\n    \n    \n      4\n      b\n      5\n    \n    \n      5\n      b\n      6\n    \n  \n\n\n\n\n\ndef min_max(df:pd.DataFrame) -> List[Dict[str,Any]]:\n    \"\"\"\n    Calculates the min and max of a given column based\n    on the grouping of a separate column.\n    \"\"\"\n    return [{\"group\": df.iloc[0][\"col1\"], \n             \"max\": df['col2'].max(), \n             \"min\": df['col2'].min()}]\n\n\ntransform(\n    df=df, \n    using=min_max, \n    schema=\"group:str, max:int, min:int\",\n    partition={\"by\": \"col1\"}\n    )\n\n\n\n\n\n  \n    \n      \n      group\n      max\n      min\n    \n  \n  \n    \n      0\n      a\n      3\n      1\n    \n    \n      1\n      b\n      6\n      4\n    \n  \n\n\n\n\nWe can use transform() operation to save the output as a parquet file\n\ndf = pd.DataFrame({\"a\": [1,2,3], \"b\": [1,2,3], \"c\": [1,2,3]})\ndf.to_parquet(\"../data/df.parquet\")\n\n\ndef drop_col(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    A function that drops a column labelled 'b'.\n    \"\"\"\n    return df.drop(\"b\", axis=1)\n\ntransform(\n    df=\"../data/df.parquet\",\n    using=drop_col,\n    schema=\"*-b\",\n    engine=spark,\n    save_path=\"../data/processed.parquet\"\n    )\n\npd.read_parquet(\"../data/processed.parquet/\").head()\n\n                                                                                \n\n\n\n\n\n\n  \n    \n      \n      a\n      c\n    \n  \n  \n    \n      0\n      1\n      1\n    \n    \n      1\n      2\n      2\n    \n    \n      2\n      3\n      3\n    \n  \n\n\n\n\nThis expression makes it easy for users to toggle between running Pandas with sampled data and using Spark, Dask or Ray on the full dataset.We can use transform() to distribute the processing of a single step in our process."
  },
  {
    "objectID": "distributed_processing/fugue_intro.html",
    "href": "distributed_processing/fugue_intro.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Fugue\nThis module is included to show how we can easily port pandas code to a compute engine of our choice (with minimal code changes).\nFugue provides an easier interface to using distributed compute effectively and accelerates big data projects.Fugue ports Python, Pandas and SQL code to Spark, Dask and Ray\n\n\n\nBenefits of Fugue"
  },
  {
    "objectID": "distributed_processing/fugue_sql.html",
    "href": "distributed_processing/fugue_sql.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "FugueSQL\nFugueSQL can be used on top of Pandas, Spark and Dask. FugueSQL is parsed and then executed on top of the underlying engine.\n\nimport warnings\nwarnings.filterwarnings(action='ignore')\n\n\nfrom fugue_notebook import setup\nsetup(is_lab=False)\n\n\n\n\n\n\nimport pandas as pd\n\ndf = pd.DataFrame({\"col1\": [\"A\",\"A\",\"A\",\"B\",\"B\",\"B\"], \"col2\": [1,2,3,4,5,6]})\ndf2 = pd.DataFrame({\"col1\": [\"A\", \"B\"], \"col3\": [1, 2]})\n\n\nRun FugueSQL\n\n%%fsql\n   SELECT df.col1, df.col2, df2.col3\n     FROM df\nLEFT JOIN df2\n       ON df.col1 = df2.col1\n    WHERE df.col1 = \"A\"\n    PRINT\n\n\n\n\n\n  \n    \n      \n      col1\n      col2\n      col3\n    \n  \n  \n    \n      0\n      A\n      1\n      1\n    \n    \n      1\n      A\n      2\n      1\n    \n    \n      2\n      A\n      3\n      1\n    \n  \n\n\n\n\nschema: col1:str,col2:long,col3:long\n\n\n\n\nUsing FugueSQL dataframe in Python\n\n%%fsql\nSELECT *\n  FROM df\n YIELD DATAFRAME AS result\n\n\nprint(type(result))\nprint(result.native.head())\n\n<class 'fugue.dataframe.pandas_dataframe.PandasDataFrame'>\n  col1  col2\n0    A     1\n1    A     2\n2    A     3\n3    B     4\n4    B     5\n\n\n\n\nLoading files\n\n%%fsql\ndf = LOAD \"../data/processed.parquet\"\n\nnew = SELECT *\n        FROM df\n       YIELD DATAFRAME AS result\n\n\nprint(result.native)\n\n   a  c\n0  1  1\n1  2  2\n2  3  3\n\n\nCommon table expressions (CTEs) are also supported by FugueSQL\n\n\nUsing python code on SQL\n\nf = pd.DataFrame({\"col1\": [\"A\",\"A\",\"A\",\"B\",\"B\",\"B\"], \"col2\": [1,2,3,4,5,6]})\n\n\n# schema: *+col2:float\ndef std_dev(df: pd.DataFrame) -> pd.DataFrame:\n    return df.assign(col2=df['col2']/df['col2'].max())\n\nThe function above is defined to handle one group of data at a time. In order to apply it per group, we partition the DataFrame first by group using the PREPARTITION and TRANSFORM keywords of FugueSQL.\n\n%%fsql\nTRANSFORM df PREPARTITION BY col1 USING std_dev\nPRINT\n\n\n\n\n\n  \n    \n      \n      col1\n      col2\n    \n  \n  \n    \n      0\n      A\n      0.333333\n    \n    \n      1\n      A\n      0.666667\n    \n    \n      2\n      A\n      1.000000\n    \n    \n      3\n      B\n      0.666667\n    \n    \n      4\n      B\n      0.833333\n    \n    \n      5\n      B\n      1.000000\n    \n  \n\n\n\n\nschema: col1:str,col2:float\n\n\n\n\nRun SQL code using either Duckdb, Spark or Dask engine\nFugue supports Pandas, Spark, Dask, and DuckDB. For operations on a laptop or single machine, DuckDB may give significant improvements over Pandas because it has a query optimizer.\nFor data that is too large to process on a single machine, Spark or Dask can be used. All we need to do is specify the engine in the cell. For example, to run on DuckDB we can do:\n\n%%fsql duckdb\nTRANSFORM df PREPARTITION BY col1 USING std_dev\nPRINT\n\n\n\n\n\n  \n    \n      \n      col1\n      col2\n    \n  \n  \n    \n      0\n      A\n      0.333333\n    \n    \n      1\n      A\n      0.666667\n    \n    \n      2\n      A\n      1.000000\n    \n    \n      3\n      B\n      0.666667\n    \n    \n      4\n      B\n      0.833333\n    \n    \n      5\n      B\n      1.000000\n    \n  \n\n\n\n\nschema: col1:str,col2:float\n\n\n\n%%fsql spark\nTRANSFORM df PREPARTITION BY col1 USING std_dev\nPRINT\n\n\n\n\n\n  \n    \n      \n      col1\n      col2\n    \n  \n  \n    \n      0\n      A\n      0.333333\n    \n    \n      1\n      A\n      0.666667\n    \n    \n      2\n      A\n      1.000000\n    \n    \n      3\n      B\n      0.666667\n    \n    \n      4\n      B\n      0.833333\n    \n    \n      5\n      B\n      1.000000\n    \n  \n\n\n\n\nschema: col1:str,col2:float"
  },
  {
    "objectID": "data_architecture/data_lake.html",
    "href": "data_architecture/data_lake.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Image Credit: Amazon AWS\n\n\nData lakes are powerful tools for storing, managing and analyzing vast amounts of data from a variety of sources. They promise to provide businesses with invaluable insights that can help drive growth and innovation. However, like any powerful tool, data lakes can also pose significant risks if not managed properly. Without careful planning and management, a data lake can quickly become a “data swamp,” filled with irrelevant, low-quality, and outdated data that can harm business operations and hinder decision-making. In this blog post, we will explore the common causes of data swamps in data lakes and provide practical tips and best practices to help you avoid the risks and ensure your data lake remains a valuable asset for your organization.\nBefore discussing data swamps, let’s take a look at what are the benefits of a data lake.\n\n\nThere are several benefits to using a data lake, including:\nCentralized data storage: Data lakes provide a centralized location for storing all data, making it easier to access and analyze.\nScalability: Data lakes can store and process petabytes of data, making them ideal for businesses that generate large amounts of data.\nFlexibility: Data lakes can store structured, semi-structured, and unstructured data, allowing businesses to store and analyze data from a variety of sources.\nCost-effectiveness: Data lakes are typically more cost-effective, as they typically store raw data in object stores like s3 or blob storage.\nImproved data analysis: Data lakes enable businesses to apply a wide range of analytics techniques to their data, including machine learning, artificial intelligence, and natural language processing. This enables businesses to gain insights and make more informed decisions.\nOverall, data lakes provide businesses with a powerful tool for storing, processing, and analyzing large amounts of data, enabling better decision-making and business outcomes.\n\n\n\nData lakes are like Gardens. What makes a Garden either attractive or unattractive to us?\nA garden is a thing of beauty, a place to escape the hustle and bustle of everyday life and connect with nature. However, just like any other beautiful thing, a garden requires constant maintenance to keep it looking its best. Without regular care, a garden can quickly become overgrown, unkempt, and unsightly. Weeds will sprout up where they’re not wanted, flowers will wither and die, and before long, the once-beautiful garden will be unrecognizable. But with a little bit of attention and care, a garden can thrive, with vibrant flowers, lush greenery, and a peaceful ambience that can soothe the soul. Data lakes need care and maintenance just like a beautiful garden.\nHere are some reasons why a data lake becomes a data swamp:-\n\nLack of data governance: Without proper data governance policies and procedures, a data lake can become cluttered with irrelevant or inaccurate data, making it difficult to find and use the data that is needed.\nPoor data quality: If data is ingested into the data lake without being properly validated or cleaned, it can lead to poor data quality.\nLack of metadata management: Metadata is data that describes the data, and it is essential for understanding and using the data in the data lake. Without proper metadata management, it can be difficult to understand what data is stored in the data lake and how it should be used.\nLack of data access controls: If the data lake is not properly secured, unauthorized users may be able to access and modify data, which can lead to inaccurate or irrelevant data being stored in the data lake.\nLack of privacy controls: If Personally Identifiable Information (PII) data is not handled properly, it can lead to data leakage and loss of user privacy. This can severely damage the reputation of the organization.\n\n\n\n\nImage Credit: Reddit\n\n\n\n\n\nSimply put, taking regular care and maintenance can be a great way to prevent a data lake from becoming a data swamp.\nHere are some ways to maintain a data lake:-\n\nEstablishing proper data governance policies to ensure that data is properly managed throughout its lifecycle.\nEnsure data quality during the processing of the raw data. Take necessary action to ensure raw data is clean.\nImplement Metadata management and data discovery, your users should be able to find the required data in the data lake and also know how it was generated and what various attributes mean.\nEnsure data access controls, such that only authorized users can access and modify the data.\nEnsure user privacy is protected, never store PII data in the data lake. Ensure PII data is either masked or any appropriate privacy controls are in place. If there is no business case for using PII data then better not to store it in the first place.\n\nThese are easier said than done. It needs a lot of time, effort and coordination with different teams to ensure data lakes are crystal clear and beneficial to the organisation.\nData lakes are not the only solution to handle large amounts of an organisation’s data. There are other alternatives to data lakes like Enterprise Data Warehouse, Data Mart, Data Virtualization, Data Fabric, Data Hub and Data Mesh. Every organization should evaluate the requirements, strengths and weaknesses of each framework and choose the best solution. Let us discuss these frameworks in some of our future blog posts.\nTo Summarize, data lakes are capable to handle a variety of data sources in huge volumes. They are flexible and scalable. When they are planned, built and maintained properly, they can be very beneficial for any organization. If not they will become data swamps. A few ways to prevent a data lake from becoming a data swamp are ensuring data quality, governance, metadata management, data discovery, data access control, and protecting user privacy.\nHope you learned something about data lakes today and let us meet in our next blog post."
  },
  {
    "objectID": "data_architecture/feature_stores.html",
    "href": "data_architecture/feature_stores.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Feature store is an interface between models and the data.\n\nProductionize new features\nAutomate feature computation, backfills, logging\nShare and reuse feature pipelines across teams\nConsistency between training and serving data\n\n\n\n\nFeature store\n\n\n\n\n\nFeature store 101\n\n\n\n\n\nFeature store supporting batch and realtime use cases\n\n\n\n\n\n\n\nFive main components of a feature store\n\n\n\nServing - Avoid Training-serving skew\n\n\n\n\nServing\n\n\n\nStorage - Offline and online storage. Data lake is extended for offline storage. Online storage use latest feature values for each entity. Redis, DynamoDB or Cassandra is used for online storage.\n\n\n\n\nStorage\n\n\n\nTransformation - Regular processing of new data into feature values. Feature stores both manage and orchestrate data transformations that produce these values, as well as ingest values produced by external systems.\n\n\n\n\nTransformation\n\n\n\nMonitoring - Data quality is tracked by monitoring for drift, training-serving skew etc.\nRegistry - A registry of standardized feature definitions and metadata. It is a common catalog to explore, develop, collaborate and publish new definitions within and across teams.\n\n\n\n\nWhat is a feature store"
  },
  {
    "objectID": "data_architecture/model_monitoring.html",
    "href": "data_architecture/model_monitoring.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Also called feature drift\nThe population on which the model inference is performed shifts\n\nExamples:-\n\nIncrease exposure to one particular marketing channel which you haven’t had enough exposure to before\nMoving to new markets etc.\n\n\n\n\n\nThe functional relationship between model inputs and outputs change.\n\nExample:-\n\nChange in housing prices with change in economic situation\n\n\n\n\n\n\n\nDrift in ML"
  },
  {
    "objectID": "data_architecture/mlops.html",
    "href": "data_architecture/mlops.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "MlOps Architecture"
  },
  {
    "objectID": "data_architecture/big_data_architectures.html",
    "href": "data_architecture/big_data_architectures.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Lambda Architecture\n\n\n\n\n\n\n\n\nKappa Architecture\n\n\n\n\n\nLambda VS Kappa\n\n\n\n\n\n\n\n\nIOT Architecture\n\n\n\n\n\n\nSwirlai Post"
  },
  {
    "objectID": "data_architecture/data_quality.html",
    "href": "data_architecture/data_quality.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Data Quality for ML"
  },
  {
    "objectID": "Time Series/resources.html",
    "href": "Time Series/resources.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "statsforecast\nmlforecast\nNeuralforecast\nNixtla\nGreykite\nprophet"
  },
  {
    "objectID": "Time Series/03_stats_models.html",
    "href": "Time Series/03_stats_models.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Autoregressive(AR) models\nMoving Average(MA) models\nHierarchical models\n\n\n\n\nAssumes data is iid\nIn time series data points are correlated to each other\n\n\n\n\n\n\n\n\n\n\n\n\n\nFuture values of a time series are a function of its past values\nThis is the first model people use when they have no information other than the time series itself\nRegression on past values to predict future values\nStationarity is required for AR models\nAssumption of stationarity - Expected value of the process must be the same at all times\nweak stationarity - Mean and variance of a process be time invariant\nThe lag which should be considered for modeling can be found manually by looking at PACF plots\nAuto Arima can be used to find the lag automatically\nAfter modelling we should check the ACF of residuals to see if there are any patterns\nBox Jenkins hypothesis test should be done to see if there is any pattern which is left out by the model\nUse for short term forecast. In the long term the forecast will be mean value of the data\n\n\n\n\n\nThese are not Moving Averages\nThis is like linear regression but we consider errors with respect to lag. Here I think error means difference from the mean.\nThe lag which should be considered for modeling can be found manually by looking at ACF plots\nAuto Arima can be used to find the lag automatically\nAfter modelling we should check the ACF of residuals to see if there are any patterns and model needs to be improved\nLjung-Box test hypothesis test should be done to see if there is any pattern which is left out by the model. Null hypothesis means data does not exhibit serial correlation. H1 is data exhibit serial correlation\nDon’t do the forecast for more than number of time lag considered for modelling.  \n\n\n\n\n\nAuto Regressive Integrated Moving Average.\nIntegrated means differencing. we take difference of a value from its previous time step (I think so), to make the time series stationary. In practice we should not do this more than two times.\nBoth Auto regression and Moving Average are considered for modelling\nARIMA(P, D, Q) - P is number of time lags to be considered for AR. D is number of times differencing considered and Q is time lags to be considered for the MA model\nSARIMA(Seasonal ARIMA) - Model assumes multiplicative seasonality\nARCH, GARCH Family\n\nARCH - Autoregressive Condition Heteroskedasticity - Variance is not constant. Variance of a process is modeled as an autoregressive process rather than the process itself.\n\n\n\n\n\n [Disadvantages] (/Images/stats_ts_disad.png)"
  },
  {
    "objectID": "Time Series/02_exploratory_analysis.html",
    "href": "Time Series/02_exploratory_analysis.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Often the relationship between data at different points or the change over time that is most informative about how our data behaves\n\n\n\n\nstationarity\nNormal distribution of input variables - Boxcox transformation\n\n\n\n\n\nDoes the time series reflect a system that is “stable” or one that is constantly changing\nOnce we have assessed stability (i.e stationarity), we try to determine whether there are internal dynamics in that series i.e self-correlations\nwhen we think we have found certain behavioral dynamics within the system, we need to make sure we are not identifying relationships based on dynamics that do not in any way imply the causal relationships we wish to discover; hence, we must look for spurious correlations.\n\n\n\n\n\nMany statistical time series models rely on time series being stationary\nStationary time series is one that has fairly stable statistics properties over time\nA stationary time series is one in which a time series measurement reflects a system in a steady state. Mean and variance should remain same and seasonality should not be there for stationarity\nA simple definition of a stationary process is the following: a process is stationary if for all possible lags, k, the distribution of yt, yt+1,…, yt+k, does not depend on t.\nAugmented Dickey-Fuller (ADF) test is the most commonly used metric to assess a time series for stationarity. This test posits a null hypothesis that a unit root is present in a time series. Depending on the results of the test, this null hypothesis can be rejected for a specified significance level, meaning the presence of a unit root test can be rejected at a given significance level. (If unit root is present, then the series is non-stationary)\nA model to help you estimate the mean of a time series with a nonstationary mean and variance, the bias and error in your model will vary over time, at which point the value of your model becomes questionable.\nTime series can be made stationary with a few simple transformations - Log and a square root transformations are popular. Removing a trend is commonly done by differencing. Sometimes a series must be differenced more than once. However, if you find yourself differencing too much (more than two or three times) it is unlikely that you can fix your stationarity problem with differencing.\n\n\n\n\n\nSelf-correlation of a time series is the idea that a value in a time series at one given point in time may have a correlation to the value at another point in time\nAutocorrelation Fuction (ACF) - This generalizes self-correlation by not anchoring to a specific point in time. It is the similarity between observations as a function of the time lag between them\nPartial autocorrelation function (PACF) - The PACF of a time series for a given lag is the partial correlation of the time series with itself at that lag given all the information between the two points in time. PACF shows which data points are informative and which are harmonics of shorter time periods\n\n\n\n\n\nData with an underlying trend is likely to produce spurious correlations"
  },
  {
    "objectID": "Time Series/01_overview.html",
    "href": "Time Series/01_overview.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Time series analysis often comes down to the question of causality: How did the past influence the future?\n\n\n\n\n\nwhat process generated the timestamp, how and when\nTime zone information\nRecord your own actions and see how they are captured\nLocal or universal time\n\n\n\n\n\nForward fill\nMoving average of past values - use only the data that occurred before the missing data point\nInterpolation - Determining the values of missing points based on geometric constraints regarding how we want the overall data to behave\n\n\n\n\n\nTo match the frequency of different time series data\nWe either increase or decrease the timestamp frequency\n\n\n\n\nThe original resolution of the data isn’t sensible\nMatch against data at a lower frequency - we can either downsample, take average, weighted mean etc.\nSelecting out every nth element\n\n\n\n\n\nconvert irregularly sampled time series to a regularly timed one\nInputs sampled at different frequencies\nKnowledge of time series dynamics - Treat as a missing data problem and missing data techniques can be applied.\n\n\n\n\n\nMoving average is used to eliminate measurement spikes, errors of measurement etc.\npurpose of smoothing\nData preparation - Raw data unsuitable\nFeature generation\nPrediction - For many processes prediction is mean, which we get from a smoothed feature\nVisualization - Add signal to a noisy plot\nExponential smoothing - All data points are not treated equally\nMore weight to recent data\n\n\n\nEquation for exponential smoothing\n\n\n\n\n\nDoing smoothing with pandas\n\n\nExponential smoothing does not perform well in case of data with a long-term trend\nHolt’s method and holt-winters smoothing are two exponential smoothing methods applied to data with a trend or with a trend and a seasonality\nKalman filters and LOESS(locally estimated scatter plot smoothing) are other computationally involved methods - These methods leak information from the future - Not appropriate for forecasting\nSmoothing is a commonly used form of forecasting, and you can use a smoothed time series (without lookahead) as one easy null model when you are testing whether a fancier method is actually producing a successful forecast\n\n\n\n\n\n\nSeasonal time series are time series in which behaviors recur over a fixed period. There can be multiple periodicities reflecting different tempos of seasonality, such as the seasonality of the 24-hour day versus the 12-month calendar season, both of which exhibit strong features in most time series relating to human behavior.\n\n\n\n\n\nPython libraries to deal with Timezone - datetime, pytz and dateutil\n\n\n\n\ncurrent time in python\n\n\n\nBe careful with timezone conversions\nDealing with daylight savings can be tricky\n\n\n\n\nProcessing and cleaning time-related data can be a tedious and detail-oriented process.There is tremendous danger in data cleaning and processing of introducing a lookahead! You should have lookaheads only if they are intentional, and this is rarely appropriate."
  },
  {
    "objectID": "Time Series/05_ml_for_ts.html",
    "href": "Time Series/05_ml_for_ts.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "ML for Time Series\n\nWe extract and select features from time series and use the results for building an ML model\nXGBoost and Random forest models can be used for classification. Random forest is not successful in forecasting. Gradient Boosted models are giving good results in case of forecasting.\nClustering of time series - we should use Temporally aware distance metrics for clustering. One such metric is Dynamic Time Warping (DTW). Other distance metrics which are used are ‘Frechet distance’, ‘Pearson correlation’ etc.\nClassification and Forecasting can be combined with clustering in case of any requirement.\n\n\n\nDeep learning for time series\n\nDeep learning did not deliver amazing results for forecasting\nIn case of deep learning model assumptions are not required - stationarity etc. In practice, deep learning is not doing a good job of fitting data with a trend, unless architectures are modified to fit the trend. Preprocessing is required"
  },
  {
    "objectID": "Time Series/04_feature_eng.html",
    "href": "Time Series/04_feature_eng.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Feature Engineering for Time Series\n\nBy describing a time series not with a series of numbers detailing the step-by-setp outputs of a process but rather by describing it with a set of features, we can access ML methods designed for cross sectional data\nThe features can be computed over the entire time series of as rolling or expanding window functions\n\n\nConsiderations for extracting features from Time series\n\nStationarity\nLength of the time series - features may become unstable as the length of time series increases\nDomain knowledge\nExternal considerations\n\n\n\nCatalog of common features\n\nMean and variance\nMaximum and minimum\nDifference between last and first values\nNumber of local maxima and minima\nSmoothness of the time series\nPeriodicity and autocorrelation of the time series\n\n\n\nPackages for feature generation\n\ntsfresh\nThe following categories of features are computed\n\nDescriptive statistics\nPhysics inspired category of indicators - nonlinearity (C3), complexity(cid_ce), friedrich_coefficients(returns coefficients of a model fitted to describe complex nonlinear motion) etc\nHistory-compressing counts\n\n\n*Cesium. This library also has a web-based GUI for feature generation\n\n\nFeature selection\n\nAutomatic feature selection based on automatic feature generation\nFRESH - feature extraction based on scalable hypothesis tests - Implemented in tsfresh\nRecursive Feature Elimination (RFE) can be used (forward, backward methods)"
  },
  {
    "objectID": "industry_use_cases/insurance.html",
    "href": "industry_use_cases/insurance.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Vehicle damage assessment\nFraud Detection\nClaims management to ease the process\n\n\n\n\nSeverity of damage\nWhich parts are damaged\nDoes it require repair or replacement\nEstimating the cost of repair or replacement\n\n \n\n\n\n\nFinding a proper data set\nPrivacy concerns\nOptimizing performance and cost\nIf possible building 3D models and using them to train classifiers to achieve higher precision\n\nAltoros Blog post\n\n\n  \n\n\n\nWired article\n\n\n\nJust Analytics Flowmagic\n\n\n\nGithub Repo"
  },
  {
    "objectID": "system_design/step_by_step.html",
    "href": "system_design/step_by_step.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Ask as many questions as possible to understand the problem\n\nClarify the requirements\nNote down the assumptions\nPrority of the features to build\nFocus on non-fuctional requirements like scale and performance\nDo rough back of the envelope calculations\n\nBy the end of this step we should have a clear understanding of the requirements and non fuctional features\n\nSample questions: * What specific features are we building? * How many users do we have? * How fast the company wants to scale up? * what is the company’s technology stack?\n\n\n\n\nUse Top-down approach\nstart with API Design and see if the APIs are satisfying the functional requirements. Do not consider APIs which are not aligned to the requirements.\n\nInput parameters\nOutput responses\n\nStart high-level design once the APIs are finalized.\nReview if all the features are covered in the high-level design\n\n\n\n\n\nIdentify areas with challenges and discuss at least two options with their tradeoffs\nPick a solution and discuss\n\n\n\n\n\nSummarize the design\nFocus on the parts which are unique to the situation\nDiscuss how you can make improvements to this design to handle monitoring, tracking metrics or scaling the system etc.\nOther refinements you need if you had more time\n\n\n\n\nByteByteGo youtube channel\nSystem Design Book"
  },
  {
    "objectID": "system_design/scaling_to_million_users.html",
    "href": "system_design/scaling_to_million_users.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Keep web tier stateless\nBuild redundancy at every tier\nCache data as much as you can\nSupport mutliple data centers\nHost static assets in CDN\nScale your data tier by sharding\nSplit tiers into individual services\nMonitor your system and use automation tools\n\n\n\nByteByteGo website"
  },
  {
    "objectID": "math_for_ai/theory/04_neural_network.html",
    "href": "math_for_ai/theory/04_neural_network.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "We don’t know the true function which generated the data. Neural networks training function are used to approximate the true function which generated the data. Training function is used to generate the predictions. Universal approximation theorem assert that neural networks can approximate the underlying functions to any precision.\n\n\n\n\n\nAn exmaple of a loss function (in the context of neural networks). It quantifies the amount of information lost when the learned distribution is used to approximate the true distribution or the relative entropy of the true distribution with respect to the learned distribution\n\n\n\n\n\nThe landscape of the loss function is non-convex, it has local minima\nDeep learning still lacks a solid theoretical foundation\n\n\n\n\nActivation functions\n\n\n\n\n\nGradient Descent\n\n\n\nDont initialize with all zeros or all equal numbers. This will diminish the network’s ability to learn different features, since different nodes will output exactly the same numbers.\nAs neural networks are non-convex, how we initiate the w’s matters a lot. weights are sampled either from the uniform distribution over small intervals (Xavier Glorot initialization) or from Gaussian distribution with a preselected mean and variance (Kaiming He initialization)\nThe scale of the features affects the performance of the gradient descent. Very different scales of the input features change the shape of the bowl of the loss function, making minimization process harder. This means that the shape of the bowl of loss function is a long narrow valley. Gradient descent zigzags as it tries to locate the minimum and slowing down the convergence considerably.\nNear the minima (local or global), flat regions or saddle points of the loss function, the gradient descent method crawls.\nStochastic gradient descent - the points hop a lot, as opposed to following a more consistent route toward the minimum.\nLoss functions like mean squared error, cross entropy and hinge loss are all convex but not nondecreasing (what is non descreasing???)\n\n\n\n\n\n\n\nUsually about twenty percent of the input layer’s nodes and about half of each of the hidden layers nodes are randomly dropped.\n\n\n\n\n\nThe error on the validation set start increasing after a decrease. This indicates the start of overfitting and we stop training.\n\n\n\n\n\nNormalize the inputs to each layer of the network. Inputs to each layer will have mean zero and variance one. \n\n\n\n\n\nSome regularization is always good\nRidge differentiation is more stable\nIf we need to select features than use lasso\nElasticnet is preferred over lasso as it might behave badly when features are greater than data instances or when several features are correlated."
  },
  {
    "objectID": "math_for_ai/theory/02_distributions.html",
    "href": "math_for_ai/theory/02_distributions.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Distributions\n\nContinous distribution\n\nUniform Distribution\nNormal Distribution\nProbability of observing an exact value is zero\nProbability will exist for an interval\nTo find the probability of a random variable in an interval, we integrate the probability density function over the interval\nTo find the joint probability of two variables between intervals, we double integrate the joint probability density function. For example height between 6.0 and 7.0, weight between 70 and 80 Kgs\n\n\nUniform Distribution\n\nThe probability density function for uniform distribution is constant\nIt is used for generating random numbers from any other probability distribution\n\n\n\nThe Exponential Distribution\n\nIf we happen to know that a certain event occurs at a constant rate , then exponential distribution predicts the waiting time until this event occurs.\nThe time until a machine part fails - This is very useful for reliability field  \n\n\n\nThe Weibull Distribution\n\nSimilar to exponential distribution but it can model rates which occur with increase or decrease with time\nUsed in the field of product life times\nA car will not work if the battery fails, or if a fuse in the gearbox burns out. A Weibull distribution provides a good approximation for the lifetime of a car before it stops working, after accounting for its many parts and their weakest link.\nIt is controlled by shape, scale and location\nExponential is a special case of this distribution - constant rate of event occurrence\n\n\n\nThe Log-normal distribution\n\nIf we take logarithms of each value provided in this distribution, we get normal distribution.\nThis is a good distribution to use when using skewed data with low mean, large variance and assuming only positive values\nLog normal distribution will appear when we take product of many positive sample values\nParameters - shape, scale and location\nReal world examples\n\nvolume of gas in a petroleum reserve\nThe ratio of the price of a security at the end of one day to its price at the end of the day before.\n\n\n\n\nChi-Squared distribution\n\nIt is a distribution for the sum of squares of normally distributed independent random variables\nStatistical test associated:-\n\nThe goodness of fit test - How far is our expectation from observations\nIndependence and homogeneity of data features test\n\n\n\n\nNormal Distribution\n\nThe distribution is symmetrical from the mean\nMean, Median and Mode are the same\n68% of the data within one standard deviation, 95% of the data falls within 2 SD and 99.7% within 3 SDs.\nCentral limit theorem - The average of independent random samples from any distribution is normally distributed.\nIf you happen to find yourself in a situation where you are uncertain and have no prior knowledge about which distribution to use for your application, the normal distribution is usually a reasonable choice. In fact, among all choices of distributions with the same variance, the normal distribution is the choice with maximum uncertainty, so it does in fact encode the least amount of prior knowledge into your model.\n\n\n\n\nEquation\n\n\n\n\nStudent’s t-distribution\n\nUsed when sample size is small and population variance is unknown\n\n\n\nGamma distribution\n\nTime until n independent events occur, instead of only one event  \n\n\n\nBeta distribution\n  \n\n\n\nDiscrete distribution\n\nBinomial distribution\n\nProbability of obtaining a certain number of successess when repeating one experiment,independently, multiple times\nParameters - n (number of experiments), p - predefined probablity of success\nReal world examples\n\nNumber of patients who will develop side-effects for vaccine\nNumber of ad-clicks that will result in a purchase\nNumber of customers who will default on monthly credit card payments\n\n\n  * The special case when N=1 corresponds to the Bernoulli distribution * \n\n\nPoisson Distribution\n\nIt will predict the number of rare events that will occur in a given period of time\nparameter - Event occur at a known average rate (lambda)\nReal world examples\n\nNumber of babies born in a given hour\nThe number of earthquakes happening within a particular time period\n\n\n\n\n\npoisson Distribution\n\n\n\n\n\npoisson Distribution with different lambda values\n\n\n\n\nGeometric Distribution\n\nIt predicts the number of trials needed before we obtain a success when performing independent trials, each with a known probability p for success.\nParameter - Probability of success\nNumber of weeks a company can function without experiencing failure\n\n\n\nNegative Binomial\n\nNumber of independent trails needed to obtain a certain number of successes\n\n\n\nHypergeometric Distribution\n\nSimilar to binomial but the trials are not independent\n\n\n\nNegative Hypergeometric Distribution\n\nNumber of dependent trails needed before we obtain a certain number of successes"
  },
  {
    "objectID": "math_for_ai/theory/03_fitting_functions_to_data.html",
    "href": "math_for_ai/theory/03_fitting_functions_to_data.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Fitting functions to data\n\nLinear functions can be either concave or convex. when we take the maximum of linear functions, we are compensated with convexity\nIf we have non-linear convex function, then the maximum of all the linear functions that stay below our function is exactly equal to it. This gives us a path to exploit the simplicity of linear functions when we have convex functions.\nFor convex functions the global and local minima are same\nA non-linear function can be either convex or non-convex\nFor non-convex functions with peaks, valleys and saddle points we run the risk of getting stuck at local minima\nNumerical method’s search for minimum happens on the ground level and does not search in the same space dimension as the landscape of the function is embedded in.\nIt is important to locate, on ground level, a direction that quickly decreseas the function height and how far we can move in that direction on ground level (learning rate) while still decreasing the function height above us\n\n\nLinear regression\n\nAnalytic and numeric method exist for linear regression. For others analytic solution is difficult, so we depend on numeric methods \n\n\n\nLogistic regression\n \n\n\nMulticlass logistic function\n  \n\n\nSVM\n\nSeek to separate a labeled data using widest possible margin\nAn optimal highway of separation instead of thin line of separation\nSVM uses hinge loss function. When the loss is greater than 1 then it is a high penalty, if the loss is between 0 and 1 then it is still penalized and if the loss is 0, there is no penalty.    \n\n\n\nDecision Tree\n\nIt is a non-parametric model, it does not fix the shape of the function ahead of time.\nThis flexibility makes it overfitting to the data\nEntropy and Gini Index are used to measure the importance of a feature. Gini Index is less expensive, so it a default in most packages.\nEntropy approach - Feature split that provides the maximum information gain\nGini Impurity - Split that provides lowest average Gini Impurity\n\n       \n\nPros & Cons\n\nunstable\nSensitive to rotations in the data, since their decision boundaries are ususally horizontal and vertical. Fix - Transform the data to match its principal axes, using SVD\noverfit the data\n\n\n\n\nK-Means clustering\n\nIt minimizes the variance (the squared euclidean distances to the mean) within each cluster\n\n\n\nFeature selection\n\nF-test and mutual information test for feature selection\n\n\nWhat is bagging, pasting, random patches and stacking?\n\nBagging is taking random subsets with replacement and pasting taking random subsets without replacement\nRandom patches means taking a subset of features for modelling\nStacking is using a prediction model on all the results of all models to get final prediction\n\n\n\n\nGradient Descent\n\nGradient - The effect on the loss by changing a single parameter, while keeping everything else constant. How much the loss changes if one parameter changes a little bit\nGradient is steepness. Learning rate can be equated to step size and the steepness dictates the number of steps (relative impact of the parameter). we then take a number of steps that’s proportional to the relative impact: more impact, more steps\nAs we will be using a single learning rate for all the parameters, the size of the learning rate is limited by the steepest curve. All other curves will be using suboptimal learning rate, given their shapes. (why can’t we use different learning rates for different curves? - Is it a implementation problem or other challenge?)\nTo make the gradients equally steep - we should do feature standardization or normalization Impact on loss with and without scaling for two parameters. We should use training set only to fit the standardscaler, we should use its transform method to apply the preprocessing step to all datasets: training, validation and test\n\n\n\n\nchange in loss\n\n\n\n\n\nEffect of learning rate on loss"
  },
  {
    "objectID": "math_for_ai/theory/01_intro.html",
    "href": "math_for_ai/theory/01_intro.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "contionous variables\n\nProbability density function\n\nDiscrete variables\n\nProbability mass function\n\nMarginal probability distribution\n\nGetting the probability distribution of a single or multiple variables from the full joint probability distribution of multiple random variables.\n\nJoint probability distribution\n\nProbability distribution of two or more variables occuring together\nIf we fix the value of one of the variable, we get a distribution proportional to conditional probability distribution\nJoint probability distribution = marginal probability distribution + conditional probability distribution\nJoint probability in case of dependent variables is not separable. we need to store each value for every co-occurance between the variables. (curse of dimensionality)\n\n\n\n\n\n\n\nEstimating conditional probabilities when full joint probability distribution is not known\n\n\n\n\n\nMix probability distributions\n\n\n\n\n\nIf the data we care for is not sampled or observed, we speculate on it using the langauge of expectation.\nLaw of large numbers - When sample size goes to infinity, expectation matches the sample mean\n\n\n\n\n\nCorrelation works on normalized random variables\nCovariance works on unnormalized random variables"
  },
  {
    "objectID": "transformers/image_worth_16_16.html",
    "href": "transformers/image_worth_16_16.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Split an image into patches and provide the sequence of linear embeddings of these patches as an input to Transformer.\nImage patches are treated the same way as tokens (words) in an NLP application. We train the model on image classification in supervised fashion.\nTransformers lack some of the inductive biases inherent to CNNs such as translation equivariance and locality and therefore do not generalize well when trained on insufficient amounts of data\nIt is often beneficial to fine-tune at higher resolution than pre-training\nViT performs better when trained on large scale data and transferred to tasks with fewer datapoints\n\n\n\n\nViT Architecture\n\n\n\nThe image patches are flattened and mapped to D dimensions with a trainable linear projection.These are called patch embeddings\nLearnable embedding is prepended to the sequence of embedded patches, whose state at the output of the transformer encoder serves as the image representation.\nLearnable 1D position embeddings are added to patch embeddings to retain positional information\nMLP contains two layers with GELU (Gaussina Error Linear Units) non-linearity\n\n\n\n\nVariants of ViT\n\n\n\nRegularization used - weight decay, dropout and label smoothing\n\nIn the lowest layers some heads attend to most of the image, showing that the ability to integrate information globally is indeed used by the model. The attention distance increases with network depth. Globally, the model attends to image regions that are semantically relevant for classification\n\n\nResearch Paper"
  },
  {
    "objectID": "transformers/resources.html",
    "href": "transformers/resources.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Resources\n\nCode implementation and Explanation of various Transformer Architectures\nTransformer Resources\nTransformer architectures from Papers with code\nThe Annotated Transformer\nPytorch tutorial on Language Modeling\nStanford course on Transformers\ncode review of Transformer\nGPT in 60 lines of Numpy"
  },
  {
    "objectID": "transformers/BERT.html",
    "href": "transformers/BERT.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Pre-trained language model\nCan be fine tuned with a simple additional output layer and reasonably sized dataset for a broad range of NLP tasks\n\n\n\nBERT Inputs\n\n\n\n\n\nBERT Pretrained on NSP and MLM\n\n\n\n\n\nBERT fine-tuning architecture for token tagging tasks like POS, NER, QA\n\n\n\n\n\n\nMasked Language Modelling\nNext Sentence Prediction\n\n\n\n\n\n\n\n\nSame architecture as BERT with difference in the amount of training data, training tasks, methods and hyperparameter tuning\n\nThe Important differences are:- * Pre-training the model for longer time * Using bigger batches * Using more training data * Removing the Next sentence Prediction (NSP) task * Training on longer sequences * Dynamically changing the masking pattern applied to the training data for each epoch. (For BERT the masked token were static for all epochs)\n\n\n\n\n\n\n\nThe first model to have been pre-trained on both natural language sentences and tabular data formats.\nTaBERT is built on top of the BERT model that accepts natural language queries and tables as input.\nIt acquires contextual representations for sentences as well as the consitituents of the DB table\nThese representations may be further fine-tuned using the training data for that job\n\n\n\n\n\nBERT for topic modeling\nBERT models are used to create the embeddings of the documents of interest\nPreprocessing takes care of the document size by dividing to small paragraphs that are smaller than the token size for the transformer model\nClustering is performed on the document embeddings to cluster all the documents with similar topics together\nDimensionality reduction is used to reduce the dimensionality of the embeddings\nclass-based TF-IDF in which all documents in a certain category is considered as a single document then tf-idf computed - This calculates the relative importance of a word in a class instead of documents\n\n\n\n\n\n\n\n\nIt aims to answer why BERT performs well on so many NLP tasks\n\n\n\n\n\n\nTeacher forcing in RNN models\nCovariance shift - Gradient dependencies between each layer and speeds up the convergence as fewer iterations are needed. How Layer Normalization and Covariance shift are related?\nDifferent Normalizations - Batch, layer etc\nPerplexity - Evaluation metric"
  },
  {
    "objectID": "transformers/Transformer_architectures.html",
    "href": "transformers/Transformer_architectures.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "The changes to the transformer architectures can be split into two broad categories:-\n\nChanges to the internal arrangement of transformer block\nChanges to the layers that a transformer block is made of\n\n\n\n\nDecreasing memory footprint and compute\nAdding connections between transformer blocks\nAdaptive computation time (Ex - Early stopping)\nRecurrence or hierarchial structure\nNeural Architecture Search\n\n\n\n\nModifications to the transformer block\n\n\n\n\n\n\nchanges to the four important parts of a transformer\n\nPositional Encodings\nMulti-Head Attention\nResidual connection with layer normalization\nPosition-wise Feed Forward network\n\n\nThe most important being changes to the multi-head attention.\n\n\n\n\nModifications to the Multi-Head Attention sublayer\n\n\n\n\n\nModifications to the positional encodings sublayer"
  },
  {
    "objectID": "transformers/rough_work.html",
    "href": "transformers/rough_work.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\nd_embed = 512       # embedding size for the attention modules\nnum_heads = 8       # Number of attention heads\nnum_batches = 1     # number of batches (1 makes it easier to see what is going on)\nvocab = 50000       # vocab size\nmax_len = 5000      # Max length of TODO what exactly?\nn_layers = 1        # number of attention layers (not used but would be an expected hyper-parameter)\nd_ff = 2048         # hidden state size in the feed forward layers\nepsilon = 1e-6      # epsilon to use when we need a small non-zero number\n\n\n\nx = torch.tensor([[1, 2, 3]]) # input will be 3 tokens\ny = torch.tensor([[1, 2, 3]]) # target will be same as the input for many applications\nx_mask = torch.tensor([[1, 0, 1]]) # Mask the 2nd input token\ny_mask = torch.tensor([[1, 0, 1]]) # Mask the 2nd target token\nprint(\"x\", x.size())\nprint(\"y\", y.size())\n\nx torch.Size([1, 3])\ny torch.Size([1, 3])\n\n\n\n\n# Make the embedding module. It understands that each token should result in a separate embedding.\nemb = nn.Embedding(vocab, d_embed)\nx = emb(x)\n# Scale the embedding\nx = x * math.sqrt(d_embed)\nprint(x.size())\n\ntorch.Size([1, 3, 512])\n\n\n\npe = torch.zeros(max_len,d_embed, requires_grad=False)\n\n\npe.size()\n\ntorch.Size([5000, 512])\n\n\n\nposition = torch.arange(0, max_len).unsqueeze(1)\nprint(position.size())\n\ntorch.Size([5000, 1])\n\n\n\n# Start with an empty tensor\npe = torch.zeros(max_len, d_embed, requires_grad=False)\n# array containing index values 0...max_len\nposition = torch.arange(0, max_len).unsqueeze(1)\ndivisor = torch.exp(torch.arange(0, d_embed, 2) * -(math.log(10000.0) / d_embed))\n# Make overlapping sine and cosine wave inside positional embedding tensor\npe[:, 0::2] = torch.sin(position * divisor)\npe[:, 1::2] = torch.cos(position * divisor)\npe = pe.unsqueeze(0)\n# Add the position embedding to the main embedding\nx = x + pe[:, :x.size(1)]\nprint(x.size())\n\ntorch.Size([1, 3, 512])\n\n\n\nmean = x.mean(-1, keepdim=True)\nstd = x.std(-1, keepdim=True)\nW1 = nn.Parameter(torch.ones(d_embed))\nb1 = nn.Parameter(torch.zeros(d_embed))\nx = W1 * (x - mean) / (std + epsilon) + b1\nprint(x.size())\n\ntorch.Size([1, 3, 512])\n\n\n\n# Make three versions of x, for the query, key, and value\n# We don't need to clone because these will immediately go through linear layers, making new tensors\nk = x # key\nq = x # query\nv = x # value\n# Make three linear layers\n# This is where the network learns to make scores\nlinear_k = nn.Linear(d_embed, d_embed)\nlinear_q = nn.Linear(d_embed, d_embed)\nlinear_v = nn.Linear(d_embed, d_embed)\n# We are going to fold the embedding dimensions and treat each fold as an attention head\nd_k = d_embed // num_heads\n# Pass q, k, v through their linear layers\nq = linear_q(q)\nk = linear_k(k)\nv = linear_v(v)\n# Do the fold, treating each h dimenssions as a head\n# Put the head in the second position\nq = q.view(num_batches, -1, num_heads, d_k).transpose(1, 2)\nk = k.view(num_batches, -1, num_heads, d_k).transpose(1, 2)\nv = v.view(num_batches, -1, num_heads, d_k).transpose(1, 2)\nprint(\"q\", q.size())\nprint(\"x\", k.size())\nprint(\"v\", v.size())\n\nq torch.Size([1, 8, 3, 64])\nx torch.Size([1, 8, 3, 64])\nv torch.Size([1, 8, 3, 64])\n\n\n\nq.transpose(-2, -1).size()\n\ntorch.Size([1, 8, 64, 3])\n\n\n\nd_k = q.size(-1)\n# Compute the raw scores by multiplying k and q (and normalize)\nscores = torch.matmul(k, q.transpose(-2, -1)) / math.sqrt(d_k)\nprint(\"scores\", scores.size())\n# Mask out the scores\nscores = scores.masked_fill(x_mask == 0, -epsilon)\nattn = F.softmax(scores, dim = -1)\nprint(\"attention\", attn.size())\n# Apply the scores to v\nx = torch.matmul(attn, v)\nprint(\"x\", x.size())    \n\nscores torch.Size([1, 8, 3, 3])\nattention torch.Size([1, 8, 3, 3])\nx torch.Size([1, 8, 3, 64])\n\n\n\nF.softmax(scores, dim=-1).size()\n\ntorch.Size([1, 8, 3, 3])\n\n\n\nF.softmax(scores, dim=-1)\n\ntensor([[[[0.2252, 0.3892, 0.3856],\n          [0.4831, 0.2371, 0.2798],\n          [0.3575, 0.3727, 0.2698]],\n\n         [[0.5478, 0.2035, 0.2487],\n          [0.3136, 0.3352, 0.3512],\n          [0.3864, 0.3723, 0.2413]],\n\n         [[0.3558, 0.3108, 0.3334],\n          [0.3114, 0.3117, 0.3768],\n          [0.4919, 0.3081, 0.2000]],\n\n         [[0.2825, 0.3268, 0.3907],\n          [0.2674, 0.4068, 0.3257],\n          [0.4279, 0.3212, 0.2510]],\n\n         [[0.2240, 0.4012, 0.3748],\n          [0.5012, 0.2689, 0.2299],\n          [0.3869, 0.3520, 0.2611]],\n\n         [[0.2569, 0.3529, 0.3902],\n          [0.2282, 0.2568, 0.5150],\n          [0.2551, 0.3774, 0.3676]],\n\n         [[0.4467, 0.2480, 0.3053],\n          [0.3810, 0.3086, 0.3104],\n          [0.3773, 0.3159, 0.3068]],\n\n         [[0.2699, 0.4120, 0.3181],\n          [0.3626, 0.3377, 0.2997],\n          [0.2675, 0.3964, 0.3361]]]], grad_fn=<SoftmaxBackward0>)"
  },
  {
    "objectID": "transformers/transformers_new.html",
    "href": "transformers/transformers_new.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Self-attention is permutation-invariant. It is an operation on sets.\n\n\n\n\nSinusoidal positional encoding\nLearned positional encoding\nRelative position encoding\nRotary position encoding\n\n\n\n\nLilian Weng Blog post"
  },
  {
    "objectID": "transformers/data_eff_img_transformer.html",
    "href": "transformers/data_eff_img_transformer.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "This uses a teacher-student strategy specific to transformers. It relies on a distillation token ensuring that the student learns from the teacher through attention.\n\n\nResearch paper"
  },
  {
    "objectID": "transformers/vision_transformer_architectures.html",
    "href": "transformers/vision_transformer_architectures.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "pyramid vision transformer\n\n\n\nPyramid vision transformer\n\n\nTo overcome the quadratic complexity of the attention mechanism, Pyramid Vision Transformers (PVTs) employed a variant of self-attention called Spatial-Reduction Attention (SRA), characterized by a spatial reduction of both keys and values. By applying SRA, the spatial dimensions of the features slowly decrease throughout the model.\n\n\nSWIN Transformer"
  },
  {
    "objectID": "transformers/transformers_from_scratch.html",
    "href": "transformers/transformers_from_scratch.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "The trick to use one-hot vectors to pull out a particular row of a matrix is at the core of how transformers work.\n\n\n\nMatrix multiplication with one-hot vectors\n\n\n\n\n\n\n\n\nMarkov model\n\n\nMarkov chains can be expressed conveniently in matrix form.\n\n\n\nMarkov chain represented as a matrix form\n\n\nUsing one-hot vector to pull out the relevant row and shows the probability distribution of what the next word will be.\n\n\n\nUsing one-hot vector ro pull out the transition probabilities associated with given word\n\n\n\n\n\nThe mask has the effect of hiding a lot of the transition matrix (which is not relevant for the word combinations)\n\n\n\nMasking\n\n\n\n\n\nMasking hiding transition matrix\n\n\n\n\n\n\n\n\n\nAttention\n\n\nThe highlighted piece in the above formula Q represents the feature of interest nad the matrix K represents the collection of masks (which words are important for the given query of interest)\n\n\n\nMatrix multiplication of query with masks\n\n\n\n\n\nThe following happens in Feed Forward Network * Feature creation matrix multiplication * Transition matrix multiplication * ReLU nonlinearity\n\n\n\nFFN\n\n\n\n\n\nThe area in the architecture the above operations happen\n\n\n\n\nEmbedding can be learned during training\n\n\n\nInput Embedding\n\n\n\n\n\n\n\n\nWorking of positional Embedding\n\n\n\n\n\n\n\n\nConverting the embedding to original vocabulary\n\n\nTo get the softmax of the value x in a vector, divide the exponential of x, e^x, by the sum of the exponentials of all the values in the vector.\n\n\n\nDeembedding\n\n\n\n\n\n\n\n\nMatrix Dimensions\n\n\n\n\n\nMatrix Dimensions for Multihead attention\n\n\n\n\n\nScaled Dot-product attention\n\n\n\n\n\n\nThey help keep the gradient smooth\nThe second purpose is specific to Transformers - Preserving the original input sequence. Even with a lot of attention heads, there’s no guarantee that a word will attend to its own position. It’s possible for the attention filter to forget entirely about the most recent word in favor of watching all of the earlier words that might be relevant. A skip connection takes the original word and manually adds it back into the signal, so that there’s no way it can be dropped or forgotten.\n\n\n\n\nskip connections\n\n\n\n\n\nThe values of the matrix are shifted to have a mean of zero and scaled to have standard deviation of one\n\n\n\nCross-attention works just like self-attention with the exception that the key matrix K and value matrix V are based on the output of the final encoder layer, rather than the output of the previous decoder layer. The query matrix Q is still calculated from the results of the previous decoder layer. This is the channel by which information from the source sequence makes its way into the target sequence and steers its creation in the right direction. It’s interesting to note that the same embedded source sequence is provided to every layer of the decoder, supporting the notion that successive layers provide redundancy and are all cooperating to perform the same task.\n\n\n\ncross attention\n\n\n\n\n\n\nBrandon Rohrer blog post"
  },
  {
    "objectID": "transformers/attention.html",
    "href": "transformers/attention.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Attention\nThe context vector was a bottleneck for the RNN models. It made it challenging for models to process long sentences. In RNN models a single hidden state was passed between the encoder and the decoder.\nIn attention models the encoder passes a lot more data to the decoder.Instead of passing the last hidden state of the encoding stage, the encoder passes all the hidden states to the decoder.\nThe decoder will look at the set of hidden states it received. It will give each hidden state a score. Multiply each hidden state by its softmaxed score. This scoring is done at each time step on the decoder side.\nThis is how the deocder works:-\n\nThe attention decoder RNN takes in the embedding of the  token, and an initial decoder hidden state.\nThe RNN processes its inputs, producing an output and a new hidden state vector (h4). The output is discarded. 3.Attention Step: We use the encoder hidden states and the h4 vector to calculate a context vector (C4) for this time step.\nWe concatenate h4 and C4 into one vector.\nWe pass this vector through a feedforward neural network (one trained jointly with the model).\nThe output of the feedforward neural networks indicates the output word of this time step.\nRepeat for the next time steps\n\n\nReference\nJay Alammar Blog post"
  },
  {
    "objectID": "transformers/vision_transformer.html",
    "href": "transformers/vision_transformer.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Transformers cannot process grid-structured data. It needs sequences. We need to convert non-sequential signal to a sequence.\n\n\n\nSplit an image into patches\nFlatten the patches\nProduce lower-dimensional linear embeddings from the flattened patches\nAdd positional embeddings\nFeed the sequence as an input to standard transformer encoder\nPretrain the model with image labels\nFinetune on the downstream dataset for image classification\n\nThe architecure is the same as the original Attention is all you need paper. The number of blocks is changed.\n\n\n\nProposed Architecture for Vision Transformers\n\n\nThere is no decoder in the architecture.\n\n\n[CLS] embedding begins as a “blank slate” for each sentence in BERT. The final output from [CLS] embedding is used as the input into a classification head during pretraining.Using a “blank slate” token as the sole input to a classification head pushes the transformer to learn to encode a “general representation” of the entire sentence into that embedding. ViT applies the same logic by adding a learnable embedding.\n\n\n\nLearnable Embedding\n\n\n\n\n\nSpecifically, if ViT is trained on datasets with more than 14M (at least :P) images it can approach or beat state-of-the-art CNNs.\nViT is pretrained on the large dataset and then fine-tuned to small ones. The only modification is to discard the prediction head (MLP head) and attach a new D KD×K linear layer, where K is the number of classes of the small dataset.\nEven though many positional embedding schemes were applied, no significant difference was found. Hence, after the low-dimensional linear projection, a trainable position embedding is added to the patch representations.\nDeep learning is all about scale. Indeed, scale is a key component in pushing the state-of-the-art. In this study7 by Zhai et al. from Google Brain Research, the authors train a slightly modified ViT model with 2 billion parameters, which attains 90.45% top-1 accuracy on ImageNet7. The generalization of this over-parametrized beast is tested on few-shot learning: it reaches 84.86% top-1 accuracy on ImageNet with only 10 examples per class.\n\n\n\n\nViTs are robust against data corruptions, image occlusions and adversarial attacks\n\n\n\n\nNeed More data In CNNs the model knows how to focus, we tell them on how much to focus. In case of transformers, the model does not know how to focus. It pays attention to all the patches and learns where to focus during the training process. Due to this they need huge amounts of data.\nOverfitting to small datasets Due to their flexibility they are notorious for overfitting on small datasets. A lot of data augmentation would be required to make it work on small datasets.\n\n\n\n\nAI Summer blog Pinecone blog"
  },
  {
    "objectID": "transformers/how_to_train_ViT.html",
    "href": "transformers/how_to_train_ViT.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Without the translational equivariance of CNNs, ViT models are generally found to perform best in settings with large amounts of training data or to require strong AugReg schemes to avoid overfitting\nCarefully selected regularization and augmentations roughly correspond to a 10x increase in training data size\n\n\n\n\nRegularization used - Dropout to intermediate activations of ViT, stochastic depth regularization\nData augmentations - Mixup, RandAugment\nWeight decay\n\n\n\n\n\nFor most practical purposes, transferring a pre-trained model is both more cost-efficient and leads to better results\n\n\n\n\nOne approach is to run downstream adaptation for all available pre-trained models and then select the best performing model, based on validation score on the downstream task. (expensive)\nSelect a single pre-trained model based on the upstream validation accuracy and then only use this model for adaptation (cheaper)\nCheaper strategy works equally well as the more expensive strategy in the majority of scenarios\n\n\n\n\n\nResearch Paper"
  },
  {
    "objectID": "transformers/code_transformers.html",
    "href": "transformers/code_transformers.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "This notebook is to practice transformer code implementation. The reference for this Mark Riedl Github repo. This notebook is a replication for practice purpose.\n\n\n\nTransformer Architecture\n\n\n\n\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n\n\n\nd_embed = 512\nnum_heads = 8\nnum_batches = 1\nvocab = 50_000\nmax_len = 5000\nn_layers = 1\nd_ff = 2048\nepsilon = 1e-6\n\n\n\n\n\nx = torch.tensor([[1,2,3]]) # Input is size batch_size x sequence_length\ny = torch.tensor([[1,2,3]]) \nx_mask = torch.tensor([[1,0,1]])\ny_mask = torch.tensor([[1,0,1]])\nprint(\"x\",x.size())\nprint(\"y\",y.size())\n\nx torch.Size([1, 3])\ny torch.Size([1, 3])\n\n\n\n\n\n\n\n\nemb = nn.Embedding(vocab, d_embed)\n# We are extracting the embeddings for the tokens from the vocabulary\n# The dimensions after this operation will be batch_size x sequence_length x d_embed\nx = emb(x) \n# scale the embedding by sqrt(d_model) to make them bigger\nx = x * math.sqrt(d_embed)\nprint(x.size())\n\ntorch.Size([1, 3, 512])\n\n\n\n\n\n# start with empty tensor\npe = torch.zeros(max_len, d_embed, requires_grad=False)\n# array containing index values 0 to max_len\nposition = torch.arange(0,max_len).unsqueeze(1)\ndivisor = torch.exp(torch.arange(0,d_embed,2)) * -(math.log(10000.0)/d_embed)\n# Make overlapping sine and cosine wave inside positional embedding tensor\npe[:,0::2] = torch.sin(position * divisor)\npe[:,1::2] = torch.cos(position * divisor)\npe = pe.unsqueeze(0)\n# Add the positional embedding to the main embedding\nx = x + pe[:,:x.size(1)]\nprint(x.size())\n\ntorch.Size([1, 3, 512])\n\n\n\n\n\n\n\n\n\nx_residual = x.clone()\nprint(x.size())\n\ntorch.Size([1, 3, 512])\n\n\n\n\n\n\n# Centering all the values relative to mean\n# W and b are hyperparameters which needs tuning\nmean = x.mean(-1,keepdim=True)\nstd = x.std(-1,keepdim=True)\nW1 = nn.Parameter(torch.ones(d_embed))\nb1 = nn.Parameter(torch.zeros(d_embed))\nx = W1 * (x - mean) / (std + epsilon) + b1\nprint(x.size())\n\ntorch.Size([1, 3, 512])\n\n\n\n\n\nSelf-attention is a process of generating scores that indicate how each token is to every other token. So we would expect a seq_length x seg_length matrix of values between 0 and 1, each indicating the importance of the i-th token to the j-th token.\nThe input to self-attention is batch_size x sequence_length x embedding_size matrix.\nSelf-attention copies the input x , three tiles and calls them query(q), key(k) and values(v). Each of these matrices go through a linear layer. The marix learns to make scores in the linear layersa. It makes each matrix different. If the networks comes up with the right, different, matrices, it will get good attention scores.\nWe designate chunks of each token embedding to different heads.\nThe q and k tensors are multiplied together. This creates a batch_size x num_heads x sequence_length x sequence_length matrix. Ignoring batching and heads, one can interpret this matrix as containing the raw scores where each cell computes how related the i-th token is to the j-th token (i is the row and j is the column).\nNext we pass this matrix through a softmax layer. The secret to softmax is that it can act like an argmax—it can pick the best match. Softmax squishes all values along a particular dimenion into 0…1. But what it is really doing is trying to force one particular cell to have a number close to 1 and all the rest close to 0. If we multiply this softmaxed score matrix to the v matrix, we are in essence asking (for each head), which column is best for each row. Recall that rows and columns correspond to tokens. So we are asking, which token goes best with every other token. Again, if the earlier linear layers get their parameters right, this multiplication will make good choices and loss will improve.\nAt this point we can think of the softmaxed scores multiplied against v as tryinng to zero out everything but the most relevant token embedding (several because of multiple heads). The result, which we will store back in x for consistency is mainly the most-attended token embedding (several because of multiple heads) plus a little bit of every other embedded token sprinkled in because we can’t do an actual argmax—the best we can do is get everything irrelevant to be close to zero so it doesn’t impact anything else.\nThis multiplication of the scores against the v matrix is what we refer to as self-attention. It is essentially a dot-product with an underlying learned scoring function. It basically tells us where we should look for good information. The Decoder will use this later.\n\n# Make three versions of x for the query, key and values\nk = x\nq = x\nv = x\n# Make three linear layers\n# This is where the network learns to make scores\nlinear_k = nn.Linear(d_embed, d_embed)\nlinear_q = nn.Linear(d_embed, d_embed)\nlinear_v = nn.Linear(d_embed, d_embed)\n# We are going to fold the embedding dimensions and treat each fold as an attention head\nd_k = d_embed // num_heads\n# Pass q, k, v through their linear layers\nq = linear_q(q)\nk = linear_k(k)\nv = linear_v(v)\n# Do the fold, treating each h dimensions as a head\n# Put the head in the second position\nq = q.view(num_batches, -1, num_heads, d_k).transpose(1,2)\nk = k.view(num_batches, -1, num_heads, d_k).transpose(1,2)\nv = v.view(num_batches, -1, num_heads, d_k).transpose(1,2)\nprint(\"q\",q.size())\nprint(\"k\",k.size())\nprint(\"v\",v.size())\n\nq torch.Size([1, 8, 3, 64])\nk torch.Size([1, 8, 3, 64])\nv torch.Size([1, 8, 3, 64])\n\n\nTo produce the attention scores we multiply q and k (and normalize). We need to apply the mask so masked tokens don’t attend to themselves. Apply softmax to emulate argmax (good stuff close to 1 irrelevant stuff close to 0). You won’t see this happen if you look at attn because the linear layers aren’t trained yet. The attention scores are finally applied to v.\n\nd_k = q.size(-1)\n# compute the scores by multiplying k and q (and normalize)\nscores = torch.matmul(k,q.transpose(-2,-1)) / math.sqrt(d_k)\n# Mask out the scores\nscores = scores.masked_fill(x_mask == 0, -epsilon)\n# Softmax the scores, ideally creating one score close to 1 and the rest close to 0 \n# (Note: this won't happen if you look at the numbers because the linear layers haven't \n# learned anything yet.)\nattn = F.softmax(scores,dim = -1)\nprint(\"attention\",attn.size())\n# Apply the scores to v\nx = torch.matmul(attn,v)\nprint(\"x\",x.size())\n\nattention torch.Size([1, 8, 3, 3])\nx torch.Size([1, 8, 3, 64])\n\n\n\n# Recombine the multiple attention heads (unfold)\nx = x.transpose(1,2).contiguous().view(num_batches, -1, num_heads * (d_embed // num_heads))\nprint(\"x\",x.size())\n\nx torch.Size([1, 3, 512])\n\n\n\n\n\n\nff = nn.Linear(d_embed, d_embed)\nx = ff(x)\nprint(x.size())\n\ntorch.Size([1, 3, 512])\n\n\n\n\n\n\n# Adding the residual - This is changing the original embedding values for each token by some delta up or down\nx = x_residual + x\nprint(x.size())\n\ntorch.Size([1, 3, 512])\n\n\n\n\n\nThe output of this layer is a stack of hidden states, one for each token. The decoder will be able to look back and attend to the hidden state that will be most useful for decoding by looking just at this stack. To move the matrix toward a hidden state we expand the embeddings, giving the network some capacity, and then collapse it down again to force it to make trade-offs.\n\n\n\nx_residual = x.clone()\nprint(x.size())\n\ntorch.Size([1, 3, 512])\n\n\n\n\n\n\nmean = x.mean(-1,keepdim=True)\nstd = x.std(-1,keepdim=True)\nW2 = nn.Parameter(torch.ones(d_embed))\nb2 = nn.Parameter(torch.zeros(d_embed))\nx = W2 * (x - mean) / (std + epsilon) + b2\nprint(x.size())\n\ntorch.Size([1, 3, 512])\n\n\n\n\n\n\n# The embeddings is grown and compressed again.  This is part of process of transforming the outputs of the self-attention module into a hidden state encoding.\nlinear_expand = nn.Linear(d_embed, d_ff)\nlinear_compress = nn.Linear(d_ff, d_embed)\nx = linear_compress(F.relu(linear_expand(x)))\nprint(x.size())\n\ntorch.Size([1, 3, 512])\n\n\n\n##### 1.1.2.4 Add residual block back\nx = x_residual + x\nprint(x.size())\n\ntorch.Size([1, 3, 512])\n\n\n\n\n\n\n\n\n# After repeating the self-attention and feed forward sub-layers for N times, we apply one last layer normalization\nmean = x.mean(-1, keepdim=True)\nstd = x.std(-1, keepdim=True)\nWn = nn.Parameter(torch.ones(d_embed))\nbn = nn.Parameter(torch.zeros(d_embed))\nx = Wn * (x - mean) / (std + epsilon) + bn\nprint(x.size())\n\ntorch.Size([1, 3, 512])\n\n\nAt this point, we should have a matrix, stored in x that we can interpret as a stack of hidden states. The Decoder will attempt to attend to this stack and pick out (via softmax emulating argmax) the hidden state that is most helpful in guessing the work that goes in the masked position.\n\n# The output is the hidden state\nhidden = x\nprint(hidden.size())\n\ntorch.Size([1, 3, 512])\n\n\n\n\n\n\n\n\n\nemb_d = nn.Embedding(vocab, d_embed)\ny = emb_d(y) * math.sqrt(d_embed)\nprint(y.size())\n\ntorch.Size([1, 3, 512])\n\n\n\npe = torch.zeros(max_len,d_embed, requires_grad = False)\nposition = torch.arange(0, max_len).unsqueeze(1)\ndivisor = torch.exp(torch.arange(0, d_embed, 2) * -(math.log(10000.0) / d_embed))\npe[:,0::2] = torch.sin(position * divisor)\npe[:,1::2] = torch.cos(position * divisor)\npe = pe.unsqueeze(0)\ny = y + pe[:, :y.size(1)]\nprint(y.size())\n\ntorch.Size([1, 3, 512])\n\n\n\n\n\n\n\n\n\n\ny_residual = y.clone()\nprint(y.size())\n\ntorch.Size([1, 3, 512])\n\n\n\n\n\n\nmean = y.mean(-1, keepdim=True)\nstd = y.std(-1, keepdim=True)\nW1_d = nn.Parameter(torch.ones(d_embed))\nb1_d = nn.Parameter(torch.zeros(d_embed))\ny = W1_d * (y - mean) / (std + epsilon) + b1_d\nprint(y.size())\n\ntorch.Size([1, 3, 512])\n\n\n\n\n\n\nk = y\nq = y\nv = y\nlinear_q_self = nn.Linear(d_embed, d_embed)\nlinear_k_self = nn.Linear(d_embed, d_embed)\nlinear_v_self = nn.Linear(d_embed, d_embed)\nd_k = d_embed // num_heads\nq = linear_q_self(q)\nk = linear_k_self(k)\nv = linear_k_self(v)\nq = q.view(num_batches, -1, num_heads, d_k).transpose(1,2)\nk = k.view(num_batches, -1, num_heads, d_k).transpose(1,2)\nv = v.view(num_batches, -1, num_heads, d_k).transpose(1,2)\nprint(\"q\",q.size())\nprint(\"k\",k.size())\nprint(\"v\",v.size())\n\nq torch.Size([1, 8, 3, 64])\nk torch.Size([1, 8, 3, 64])\nv torch.Size([1, 8, 3, 64])\n\n\n\nd_k = q.size(-1)\nscores = torch.matmul(k,q.transpose(-2,-1)) / math.sqrt(d_k)\nscores = scores.masked_fill(y_mask == 0, -epsilon)\nattn = F.softmax(scores, dim=-1)\nprint(\"attention\",attn.size())\ny = torch.matmul(attn, v)\nprint(\"y\",y.size())\n\nattention torch.Size([1, 8, 3, 3])\ny torch.Size([1, 8, 3, 64])\n\n\n\n# Assemble heads\ny = y.transpose(1,2).contiguous().view(num_batches,-1,num_heads * (d_embed // num_heads))\nprint(y.size())\n\ntorch.Size([1, 3, 512])\n\n\n\n\n\n\nff_d1 = nn.Linear(d_embed, d_embed)\ny = ff_d1(y)\nprint(y.size())\n\ntorch.Size([1, 3, 512])\n\n\n\n##### 2.2.1.5 Add Residual back \ny = y_residual + y\nprint(y.size())\n\ntorch.Size([1, 3, 512])\n\n\n\n\n\n\n\n\n\n\ny_residual = y.clone()\nprint(y.size())\n\ntorch.Size([1, 3, 512])\n\n\n\n\n\n\nmean = y.mean(-1, keepdim = True)\nstd = y.std(-1, keepdim = True)\nW2_d = nn.Parameter(torch.ones(d_embed))\nb2_d = nn.Parameter(torch.ones(d_embed))\ny = W2_d * (y - mean) / (std + epsilon) + b2_d\nprint(y.size())\n\ntorch.Size([1, 3, 512])\n\n\n\n\n\nSource attention works just like self-attention, except we compute the scores using keys and values from the encoder and apply it to the query from the decoder. That is, based on what the encoder thinks we should attend to, what part of the decoder sequence should we actually attend to.\n\nq = y\nk = x # we are using x\nv = x # we are using x\nlinear_q_source = nn.Linear(d_embed,d_embed)\nlinear_k_source = nn.Linear(d_embed,d_embed)\nlinear_v_source = nn.Linear(d_embed,d_embed)\nd_k = d_embed // num_heads\nq = linear_q(q)\nk = linear_k(k)\nv = linear_v(v)\nq = q.view(num_batches, -1, num_heads, d_k).transpose(1,2)\nv = v.view(num_batches, -1, num_heads, d_k).transpose(1,2)\nk = k.view(num_batches, -1, num_heads, d_k).transpose(1,2)\nprint(\"q\",q.size())\nprint(\"k\",k.size())\nprint(\"v\",v.size())\n\nq torch.Size([1, 8, 3, 64])\nk torch.Size([1, 8, 3, 64])\nv torch.Size([1, 8, 3, 64])\n\n\n\nd_k = q.size(-1)\nscores = torch.matmul(k,q.transpose(-2,-1)) / math.sqrt(d_k)\nattn = F.softmax(scores, dim=-1)\ny = torch.matmul(attn,v)\nprint(\"y\",y.size())\n\ny torch.Size([1, 8, 3, 64])\n\n\n\n# Assemble heads\ny = y.transpose(1,2).contiguous().view(num_batches, -1, num_heads * (d_embed // num_heads))\nprint(y.size())\n\ntorch.Size([1, 3, 512])\n\n\n\n\n\n\nff_d2 = nn.Linear(d_embed,d_embed)\ny = ff_d2(y)\nprint(y.size())\n\ntorch.Size([1, 3, 512])\n\n\n\n\n\ny = y_residual + y\nprint(y.size())\n\ntorch.Size([1, 3, 512])\n\n\n\n\n\n\n\n\n\n\ny_residual = y.clone()\nprint(y.size())\n\ntorch.Size([1, 3, 512])\n\n\n\n\n\n\nmean = y.mean(-1, keepdim=True)\nstd = y.std(-1, keepdim=True)\nW3_d = nn.Parameter(torch.ones(d_embed))\nb3_d = nn.Parameter(torch.zeros(d_embed))\ny = W3_d * (y - mean) / (std + epsilon) + b3_d\nprint(y.size())\n\ntorch.Size([1, 3, 512])\n\n\n\n\n\n\nlinear_expand_d = nn.Linear(d_embed, d_ff)\nlinear_compress_d = nn.Linear(d_ff, d_embed)\ny = linear_compress_d(F.relu(linear_expand_d(y)))\nprint(y.size())\n\ntorch.Size([1, 3, 512])\n\n\n\n\n\n\ny = y_residual + y\nprint(y.size())\n\ntorch.Size([1, 3, 512])\n\n\n\n\n\n\n\nmean = y.mean(-1, keepdim=True)\nstd = y.std(-1, keepdim=True)\nWn_d = nn.Parameter(torch.ones(d_embed))\nbn_d = nn.Parameter(torch.zeros(d_embed))\ny = Wn_d * (y - mean) / (std + epsilon) + bn_d\nprint(y.size())\n\ntorch.Size([1, 3, 512])\n\n\n\n\n\n\nThis next module sits on top of the decoder and expands the decoder output into a log probability distribution over the vocabulary for each token position. This is done for all tokens, though the only ones that will matter for loss computation are the ones that are masked.\n\nlinear_scores = nn.Linear(d_embed, vocab)\nprobs = F.log_softmax(linear_scores(y), dim=-1)\nprint(probs.size())\n\ntorch.Size([1, 3, 50000])"
  },
  {
    "objectID": "transformers/object_detection_transformers.html",
    "href": "transformers/object_detection_transformers.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Object Detection with DETR\n\nNon-maximum supression and anchor generation are not required\nThey don’t require customized layers\n\n\n\n\nDETR\n\n\n\nDETR predicts all objects at once and is trained end-to-end with a set loss function which performs bipartite matching between predicted and ground-truth objects.\nPerforms better on larger objects\nA segmentation head trained on top of a pre-trained DETR works on Panoptic segmentation\nNo autogression in decoder\nUse transformers with parallel decoding\nDirectly predicting the set of detections with absolute box prediction\n\n\n\n\nDETR Architecture"
  },
  {
    "objectID": "transformers/gaps.html",
    "href": "transformers/gaps.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Considerations and pitfalls of attention for your own data Advantages and disadvantages of ViT for Images ViT Vs CNN Classification on Videos Validation for video classification other classification metrics Scene change detection in videos Continual learning - Update model Closing the presentation with summary"
  },
  {
    "objectID": "transformers/transformers.html",
    "href": "transformers/transformers.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "The Transformer is a model which uses attention to boost the speed of the training. The transformer lends itself to parallelization.\nThe transformers will consist of encoders and decoders.\nAn encoder consists two sub-layers self-attention and feed forward neural network. A decoder along with these two sub-layers will also consist of an attention layer between them to focus on relevant parts of the input sentence.\n\n\n\nComponents of Encoder and Decoder\n\n\n\n\nEmbedding happens in the bottom most encoder. The word in each position flows through its own path in the self-attention layer. The feed-forward layer does not have those dependencies and various paths can be executed in parallel.\n\n\n\nProcessing by an Encoder\n\n\n\n\n\n\n\nself attention\n\n\n\n\n\nSelf attention at a glance\n\n\n\n\n\nSteps for calculating self-attention\n\n\n\n\n\nself attention in matrix form\n\n\n\n\n\nSelf attention in a single formula\n\n\n\n\n\nIt expands the model’s ability to focus on different positions.\nIt gives the attention layer multiple “representation subspaces”\n\n\n\nMulti-headed attention\n\n\nIn the original paper, eight attention heads were used. This way we will end up with eight different Z matrices. The feed-forward layer is not expecting eight matrices – it’s expecting a single matrix (a vector for each word). So we need a way to condense these eight down into a single matrix.\n\n\n\nConcatenating Attention Heads\n\n\n\n\n\nMultiple attention heads focusing on different representations\n\n\n\n\n\nThis will account for the order of the words in the input sequence.\n\n\n\npositional encoding\n\n\n\n\n\nEach sub-layer in each encoder has a residual connection around it, and is follwed by a layer-normalization step\n\n\n\nResidual connection and layer normalization\n\n\n\n\n\n2 layer encoder decoder architecture\n\n\n\n\n\n\nThe encoder start by processing the input sequence. The output of the top encoder is then transformed into a set of attention vectors K and V. These are to be used by each decoder in its “encoder-decoder attention” layer which helps the decoder focus on appropriate places in the input sequence.\n\n\n\nDecoder\n\n\nThe following steps repeat the process until a special symbol is reached indicating the transformer decoder has completed its output. The output of each step is fed to the bottom decoder in the next time step, and the decoders bubble up their decoding results just like the encoders did. And just like we did with the encoder inputs, we embed and add positional encoding to those decoder inputs to indicate the position of each word.\nIn the decoder, the self-attention layer is only allowed to attend to earlier positions in the output sequence. This is done by masking future positions (setting them to -inf) before the softmax step in the self-attention calculation.\nThe “Encoder-Decoder Attention” layer works just like multiheaded self-attention, except it creates its Queries matrix from the layer below it, and takes the Keys and Values matrix from the output of the encoder stack.\n\n\n\nSteps in Decoding\n\n\n\n\n\nThe Linear layer is a simple fully connected neural network that projects the vector produced by the stack of decoders, into a much, much larger vector called a logits vector.\nLet’s assume that our model knows 10,000 unique English words (our model’s “output vocabulary”) that it’s learned from its training dataset. This would make the logits vector 10,000 cells wide – each cell corresponding to the score of a unique word. That is how we interpret the output of the model followed by the Linear layer.\nThe softmax layer then turns those scores into probabilities (all positive, all add up to 1.0). The cell with the highest probability is chosen, and the word associated with it is produced as the output for this time step.\n\n\n\nworking of final and softmax layer\n\n\n\n\n\nThe illustrated Transformer"
  },
  {
    "objectID": "transformers/Attention_for_vision.html#topics-covered",
    "href": "transformers/Attention_for_vision.html#topics-covered",
    "title": "Computer Vision Using Transformers",
    "section": "Topics Covered",
    "text": "Topics Covered\n\n\nCore concepts of Attention is all you need\nAdapting Attention to vision\nUse case Implementation"
  },
  {
    "objectID": "transformers/Attention_for_vision.html#attention-is-all-you-need",
    "href": "transformers/Attention_for_vision.html#attention-is-all-you-need",
    "title": "Computer Vision Using Transformers",
    "section": "Attention is all you need",
    "text": "Attention is all you need"
  },
  {
    "objectID": "transformers/Attention_for_vision.html#why-a-new-architecture-is-required",
    "href": "transformers/Attention_for_vision.html#why-a-new-architecture-is-required",
    "title": "Computer Vision Using Transformers",
    "section": "Why a new architecture is required?",
    "text": "Why a new architecture is required?\n\n\nRNNs process words sequentially\nRNN cannot consider long sequence lengths"
  },
  {
    "objectID": "transformers/Attention_for_vision.html#attention-transformer-architecture",
    "href": "transformers/Attention_for_vision.html#attention-transformer-architecture",
    "title": "Computer Vision Using Transformers",
    "section": "Attention Transformer Architecture",
    "text": "Attention Transformer Architecture"
  },
  {
    "objectID": "transformers/Attention_for_vision.html#blocks-of-the-architecture",
    "href": "transformers/Attention_for_vision.html#blocks-of-the-architecture",
    "title": "Computer Vision Using Transformers",
    "section": "Blocks of the Architecture",
    "text": "Blocks of the Architecture\n\n\nEmbedding layer\n\nReduce the dimension of word tokens\nProjection to latent space\n\nPositional Encoding\n\nTo track the relative position of the words"
  },
  {
    "objectID": "transformers/Attention_for_vision.html#self-attention",
    "href": "transformers/Attention_for_vision.html#self-attention",
    "title": "Computer Vision Using Transformers",
    "section": "Self Attention",
    "text": "Self Attention"
  },
  {
    "objectID": "transformers/Attention_for_vision.html#self-attention-1",
    "href": "transformers/Attention_for_vision.html#self-attention-1",
    "title": "Computer Vision Using Transformers",
    "section": "Self Attention",
    "text": "Self Attention"
  },
  {
    "objectID": "transformers/Attention_for_vision.html#self-attention-2",
    "href": "transformers/Attention_for_vision.html#self-attention-2",
    "title": "Computer Vision Using Transformers",
    "section": "Self Attention",
    "text": "Self Attention"
  },
  {
    "objectID": "transformers/Attention_for_vision.html#why-multi-head-attention",
    "href": "transformers/Attention_for_vision.html#why-multi-head-attention",
    "title": "Computer Vision Using Transformers",
    "section": "Why Multi-Head Attention",
    "text": "Why Multi-Head Attention\n\n\nIt expands the models ability to focus on different positions\nIt gives the attention layer multiple “representation subspaces”"
  },
  {
    "objectID": "transformers/Attention_for_vision.html#importance-of-attention",
    "href": "transformers/Attention_for_vision.html#importance-of-attention",
    "title": "Computer Vision Using Transformers",
    "section": "Importance of Attention",
    "text": "Importance of Attention\n\n\nEncoder providing a context to the decoder query by providing keys and values\nEach position in the encoder can attend to all positions in the previous layer of encoder\nEach position in decoder attending to all positions in the decoder"
  },
  {
    "objectID": "transformers/Attention_for_vision.html#skip-connections",
    "href": "transformers/Attention_for_vision.html#skip-connections",
    "title": "Computer Vision Using Transformers",
    "section": "Skip connections",
    "text": "Skip connections\n\n\nSkip connection help a word to pay attention to its own position\nKeep the gradients smooth"
  },
  {
    "objectID": "transformers/Attention_for_vision.html#steps-in-decoder",
    "href": "transformers/Attention_for_vision.html#steps-in-decoder",
    "title": "Computer Vision Using Transformers",
    "section": "Steps in Decoder",
    "text": "Steps in Decoder"
  },
  {
    "objectID": "transformers/Attention_for_vision.html#adapting-attention-to-vision",
    "href": "transformers/Attention_for_vision.html#adapting-attention-to-vision",
    "title": "Computer Vision Using Transformers",
    "section": "Adapting Attention to Vision",
    "text": "Adapting Attention to Vision"
  },
  {
    "objectID": "transformers/Attention_for_vision.html#vision-transformer",
    "href": "transformers/Attention_for_vision.html#vision-transformer",
    "title": "Computer Vision Using Transformers",
    "section": "Vision Transformer",
    "text": "Vision Transformer\n\nViT Architecture"
  },
  {
    "objectID": "transformers/Attention_for_vision.html#vit-vs-cnn",
    "href": "transformers/Attention_for_vision.html#vit-vs-cnn",
    "title": "Computer Vision Using Transformers",
    "section": "ViT Vs CNN",
    "text": "ViT Vs CNN\n\n\nInductive Bias and Locality Vs Global"
  },
  {
    "objectID": "transformers/Attention_for_vision.html#vit-vs-cnn-1",
    "href": "transformers/Attention_for_vision.html#vit-vs-cnn-1",
    "title": "Computer Vision Using Transformers",
    "section": "ViT Vs CNN",
    "text": "ViT Vs CNN\n\n\nInductive Bias and Locality Vs Global\nFlexibility\nCNN works with less amount of data than ViT\nSpecifically, if ViT is trained on datasets with more than 14M (at least) images it can approach or beat state-of-the-art CNNs."
  },
  {
    "objectID": "transformers/Attention_for_vision.html#vit-vs-cnn-2",
    "href": "transformers/Attention_for_vision.html#vit-vs-cnn-2",
    "title": "Computer Vision Using Transformers",
    "section": "ViT Vs CNN",
    "text": "ViT Vs CNN\n\n\nTransformer models are more memory efficient than ResNet models\nViT are prone to over-fitting due to their flexibility\nTransformers can learn meaningful information even in the lowest layers"
  },
  {
    "objectID": "transformers/Attention_for_vision.html#vit-vs-cnn-3",
    "href": "transformers/Attention_for_vision.html#vit-vs-cnn-3",
    "title": "Computer Vision Using Transformers",
    "section": "ViT Vs CNN",
    "text": "ViT Vs CNN\n\n\nViT reaches 84.86% top-1 accuracy on ImageNet with only 10 examples per class.\nViT is suitable for Transfer learning\nViTs are robust against data corruptions, image occlusions and adversarial attacks"
  },
  {
    "objectID": "transformers/Attention_for_vision.html#hybrid-architectures",
    "href": "transformers/Attention_for_vision.html#hybrid-architectures",
    "title": "Computer Vision Using Transformers",
    "section": "Hybrid Architectures",
    "text": "Hybrid Architectures\n\n\nCNN is used to extract features\nThe extracted features are used by the transformers"
  },
  {
    "objectID": "transformers/Attention_for_vision.html#use-case-discussion",
    "href": "transformers/Attention_for_vision.html#use-case-discussion",
    "title": "Computer Vision Using Transformers",
    "section": "Use Case Discussion",
    "text": "Use Case Discussion"
  },
  {
    "objectID": "transformers/Attention_for_vision.html#problem-statement",
    "href": "transformers/Attention_for_vision.html#problem-statement",
    "title": "Computer Vision Using Transformers",
    "section": "Problem Statement",
    "text": "Problem Statement\n\n\nMonitoring when the children are in danger of leaving the front yard\nPredict when childrean are about to leave the yard to trigger the alarm\nWe have videos of children playing sports"
  },
  {
    "objectID": "transformers/Attention_for_vision.html#considerations",
    "href": "transformers/Attention_for_vision.html#considerations",
    "title": "Computer Vision Using Transformers",
    "section": "Considerations",
    "text": "Considerations\n\n\nCollect and train the model on low quality images\nAmount of data available\nTraining time available\nImportance of Interpretability\nDeployment requirements\n\nlatency\nModel size\nInference cost"
  },
  {
    "objectID": "transformers/Attention_for_vision.html#approach",
    "href": "transformers/Attention_for_vision.html#approach",
    "title": "Computer Vision Using Transformers",
    "section": "Approach",
    "text": "Approach\n\n\nObject Detection\nTrain a ML model on the object detection output"
  },
  {
    "objectID": "transformers/Attention_for_vision.html#object-detection-with-detr",
    "href": "transformers/Attention_for_vision.html#object-detection-with-detr",
    "title": "Computer Vision Using Transformers",
    "section": "Object Detection with DETR",
    "text": "Object Detection with DETR\n\nDEtection TRansformer(DETR)"
  },
  {
    "objectID": "transformers/Attention_for_vision.html#object-detection-with-detr-1",
    "href": "transformers/Attention_for_vision.html#object-detection-with-detr-1",
    "title": "Computer Vision Using Transformers",
    "section": "Object Detection with DETR",
    "text": "Object Detection with DETR"
  },
  {
    "objectID": "transformers/Attention_for_vision.html#object-detection-with-detr-2",
    "href": "transformers/Attention_for_vision.html#object-detection-with-detr-2",
    "title": "Computer Vision Using Transformers",
    "section": "Object Detection with DETR",
    "text": "Object Detection with DETR"
  },
  {
    "objectID": "transformers/Attention_for_vision.html#object-detection-with-detr-3",
    "href": "transformers/Attention_for_vision.html#object-detection-with-detr-3",
    "title": "Computer Vision Using Transformers",
    "section": "Object Detection with DETR",
    "text": "Object Detection with DETR"
  },
  {
    "objectID": "transformers/Attention_for_vision.html#why-detr",
    "href": "transformers/Attention_for_vision.html#why-detr",
    "title": "Computer Vision Using Transformers",
    "section": "Why DETR",
    "text": "Why DETR\n\n\nHand-crafted anchors not required\nThey don’t require customized layers\nPredict all objects at once\nPost-processing not required for predicting bounding boxes\nAttention maps can be used for Interpretation"
  },
  {
    "objectID": "transformers/Attention_for_vision.html#why-detr-1",
    "href": "transformers/Attention_for_vision.html#why-detr-1",
    "title": "Computer Vision Using Transformers",
    "section": "Why DETR",
    "text": "Why DETR"
  },
  {
    "objectID": "transformers/Attention_for_vision.html#why-detr-2",
    "href": "transformers/Attention_for_vision.html#why-detr-2",
    "title": "Computer Vision Using Transformers",
    "section": "Why DETR",
    "text": "Why DETR\n\n\nWhat if object detection model doesn’t work?\nA segmentation head can be trained on top of a pre-trained DETR"
  },
  {
    "objectID": "transformers/Attention_for_vision.html#why-detr-3",
    "href": "transformers/Attention_for_vision.html#why-detr-3",
    "title": "Computer Vision Using Transformers",
    "section": "Why DETR",
    "text": "Why DETR"
  },
  {
    "objectID": "transformers/Attention_for_vision.html#why-detr-4",
    "href": "transformers/Attention_for_vision.html#why-detr-4",
    "title": "Computer Vision Using Transformers",
    "section": "Why DETR",
    "text": "Why DETR"
  },
  {
    "objectID": "transformers/Attention_for_vision.html#why-detr-5",
    "href": "transformers/Attention_for_vision.html#why-detr-5",
    "title": "Computer Vision Using Transformers",
    "section": "Why DETR",
    "text": "Why DETR\n\n\nWe can get the FPS for processing videos\nPre-trained Pytorch models and code available"
  },
  {
    "objectID": "transformers/Attention_for_vision.html#data-collection",
    "href": "transformers/Attention_for_vision.html#data-collection",
    "title": "Computer Vision Using Transformers",
    "section": "Data Collection",
    "text": "Data Collection\n\n\nBrainstorming how data needs to be annotated\nStandardizing the definitions\nCollecting Diversified data - Different yards, balls, walls, seasons etc\nLabeling the data - Quality Vs Quantity\nDiscussing ambigious cases with labelers and keep improving"
  },
  {
    "objectID": "transformers/Attention_for_vision.html#modelling",
    "href": "transformers/Attention_for_vision.html#modelling",
    "title": "Computer Vision Using Transformers",
    "section": "Modelling",
    "text": "Modelling\n\nUsing a pre-trained model is both more cost-efficient and leads to better results"
  },
  {
    "objectID": "transformers/Attention_for_vision.html#how-to-select-a-pre-trained-model",
    "href": "transformers/Attention_for_vision.html#how-to-select-a-pre-trained-model",
    "title": "Computer Vision Using Transformers",
    "section": "How to select a pre-trained model",
    "text": "How to select a pre-trained model\n\n\nSpot check all the available pre-trained models (expensive)\nSelect a single pre-trained model based on\n\nAmount of data used for training\nVaried upstream data\nBest upstream validation performance\n\n\nCheaper strategy works equally well as the more expensive strategy in the majority of scenarios\nHow to train your ViT - Research paper"
  },
  {
    "objectID": "transformers/Attention_for_vision.html#validation",
    "href": "transformers/Attention_for_vision.html#validation",
    "title": "Computer Vision Using Transformers",
    "section": "Validation",
    "text": "Validation\n\n\nFalse alarms are better than not raising alarm when necessary\nRecall is more important in this case\nToo many false alarms will reduce the customer satisfaction\nImprove Recall while maintaining Precision at an acceptable level"
  },
  {
    "objectID": "transformers/Attention_for_vision.html#validation-1",
    "href": "transformers/Attention_for_vision.html#validation-1",
    "title": "Computer Vision Using Transformers",
    "section": "Validation",
    "text": "Validation\n\n\nFPS\nmAP for object detection\nRecall, Precision and F1 for the classification"
  },
  {
    "objectID": "transformers/Attention_for_vision.html#deployment",
    "href": "transformers/Attention_for_vision.html#deployment",
    "title": "Computer Vision Using Transformers",
    "section": "Deployment",
    "text": "Deployment\n\n\nFrame sampling instead of predicting on all frames?\nDeployment using platforms like Ray for effective GPU utilization\nDeployment at Edge to meet latency requirements - TensorRT\nUse AB testing framework to deploy new models"
  },
  {
    "objectID": "transformers/Attention_for_vision.html#continual-learning",
    "href": "transformers/Attention_for_vision.html#continual-learning",
    "title": "Computer Vision Using Transformers",
    "section": "Continual Learning",
    "text": "Continual Learning\n\n\nContinual learning suits deep learning models\nIncentivize customers to label the data in real time\nUpdate the model parameters in real time\nContinously monitor the model performance"
  },
  {
    "objectID": "transformers/Attention_for_vision.html#tool-suggestion-for-monitoring",
    "href": "transformers/Attention_for_vision.html#tool-suggestion-for-monitoring",
    "title": "Computer Vision Using Transformers",
    "section": "Tool Suggestion for Monitoring",
    "text": "Tool Suggestion for Monitoring\n\n\nFiftyone"
  },
  {
    "objectID": "transformers/Attention_for_vision.html#summary",
    "href": "transformers/Attention_for_vision.html#summary",
    "title": "Computer Vision Using Transformers",
    "section": "Summary",
    "text": "Summary\n\n\nStart simple\nSmall improvements on regular basis\nLookout for new discovery in the field\nGet Feedback, Iterate, Improve\nKeep the cycle going"
  },
  {
    "objectID": "transformers/attention_is_all_you_need.html",
    "href": "transformers/attention_is_all_you_need.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Sequential nature of RNN precludes parallelization within training examples which becomes critical at longer sequence lengths.\nTransformer allows for significantly more parallelization\n\n\n\n\nTransformer Architecture\n\n\n\n\n\nEncoder * stack of 6 identical layers * To faciliate residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension 512.\nDecoder * Stack of 6 indentical layers\n\n\n\n\n8 attention heads are used\n\n\n\n\nAttention and Multi-head Attention\n\n\n\nFor large values of dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients.To counteract this effect scaling of dot product is done.\n\nMulti-head attention allows the model to jointly attend to information from different representation subspaces at different positions.\n\n\n\n\nIn “encoder-decoder attention” layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence.\nEach position in the encoder can attend to all positions in the previous layer of the encoder.\nSimilarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position.\n\n\n\n\n\nThe input and output dimensions are 512\nThe inner layer has dimensionality of 2048\n\n\n\n\nPositional Embeddings\n\n\nSelf attention could yield more interpretable models"
  },
  {
    "objectID": "diffusion_models/Introduction.html",
    "href": "diffusion_models/Introduction.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Unet - Input is Somewhat a noisy image, and it output the noise  \n\n\n\n\n \n\n\n\n\nPaperspace Gradient\nLambda labs\nJarvis labs\n\n\n\n\nJeremy Howard video"
  },
  {
    "objectID": "probability/Probability.html",
    "href": "probability/Probability.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Probability\n\nprobability = preferred events / All events\nUnion is similar to ‘OR’ condition\nIntersection is similar to ‘AND’ condition\nProbability of getting event A or B is\n\nP(A U B) = P(A) + P(B) - P(A and B)\n\nWe are subtracting P (A and B) because some elements will be common to both A and B and we will be double counting them. So we subtract P (A and B) to eleminate the double count\n\nWe can use Venn diagram to draw intersections, mutually exclusive probabilities etc. Venn diagrams can’t be used for conditional probabillity or to show dependence\nWe can use tree diagrams to visualize conditional probability and dependence\nProbability of A given B - P(A | B) \nIf two probabilities are independent then:\n\nP(A|B) = P(A) because A and B are independent and A does not depend on occurance of B, we can use this expression to check if two events are independent\nP( A and B) = P(A|B)* P(B) as P(A|B) = P(A)\nP(A and B) = P(A) * P(B) - when two events are independent then we need to multiply their probabilities\nIf two probabilities are independent then they are not mutually exclusive. If probabilities are mutually exclusive then they are not independent\n\nMutually exclusive events - Add the probabilities - P(A U B) = P(A) + P(B) because P(A and B) will be zero\n\n\n\nBayes Thereom\n\nWhen we are provided information about a conditional probability eg P(A|B) and if we need to find the conditional probability P(B|A) then we should use Bayes theorem"
  },
  {
    "objectID": "ds_lifecycle/Feature_Engineering.html",
    "href": "ds_lifecycle/Feature_Engineering.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Deep learning is also called feature learning, many features can be automatically learned and extracted\n\n\n\n\nNot all types of missing values are equal\nMissing Not At Random (MNAR) - The values are missing for reasons related to the values themselves - Ex - Income not available for high earners\nMissing At Random (MAR) - The reason for missing is not due to the value itself, but due to another observed variable - Ex - Age missing for a gender value\nMissing completely at Random (MCAR) - No pattern in when the value is missing.\nThere is no perfect way to handle missing values. With deletion, you risk losing important information or accentuating biases. With imputation, you risk injecting your own bias into and adding noise to your data, or worse, data leakage\n\n\n\n\n\nA range of [-1,1] work better than the range [0,1]\nuse standardization if the variable might follow normal distribution\nIf there is skew in the distribution, log transformation may work\nScaling is a common source of data leakage\nScaling often requires global statistics. If the new data has changed significantly compared to the training, these statistics won’t be very useful. Therefore, it’s important to retrain your model often to account for these changes.\n\n\n\n\n\nThis will rarely help\nTurning a continuous feature into a discrete feature\nHistograms can help to choose the values for the categories\n\n\n\n\n\nCategories change in production - The model will crash if it encounters a brand not seen in training\nIf you are using a category called ‘UNKNOWN’ in production, this category should also be a part of training\nPeople who haven’t worked with data in production tend to assume that categories are static, which means the categories don’t change over time. This is true for many categories. For example, age brackets and income brackets are unlikely to change, and you know exactly how many categories there are in advance. Handling these categories is straightforward. You can just give each category a number and you’re done.\nHowever, in production, categories change. Imagine you’re building a recommender system to predict what products users might want to buy from Amazon. One of the features you want to use is the product brand. When looking at Amazon’s historical data, you realize that there are a lot of brands. Even back in 2019, there were already over two million brands on Amazon. The number of brands is overwhelming, but you think: “I can still handle this.” You encode each brand as a number, so now you have two million numbers, from 0 to 1,999,999, corresponding to two million brands. Your model does spectacularly on the historical test set, and you get approval to test it on 1% of today’s traffic. In production, your model crashes because it encounters a brand it hasn’t seen before and therefore can’t encode. New brands join Amazon all the time. To address this, you create a category UNKNOWN with the value of 2,000,000 to catch all the brands your model hasn’t seen during training. Your model doesn’t crash anymore, but your sellers complain that their new brands are not getting any traffic. It’s because your model didn’t see the category UNKNOWN in the train set, so it just doesn’t recommend any product of the UNKNOWN brand. You fix this by encoding only the top 99% most popular brands and encode the bottom 1% brand as UNKNOWN. This way, at least your model knows how to deal with UNKNOWN brands. Your model seems to work fine for about one hour, then the click-through rate on product recommendations plummets. Over the last hour, 20 new brands joined your site; some of them are new luxury brands, some of them are sketchy knockoff brands, some of them are established brands. However, your model treats them all the same way it treats unpopular brands in the training data. This isn’t an extreme example that only happens if you work at Amazon. This problem happens quite a lot. For example, if you want to predict whether a comment is spam, you might want to use the account that posted this comment as a feature, and new accounts are being created all the time. The same goes for new product types, new website domains, new restaurants, new companies, new IP addresses, and so on. If you work with any of them, you’ll have to deal with this problem. One solution to this problem is the hashing trick, popularized by the package Vowpal Wabbit developed at Microsoft.One solution to this problem is the hashing trick, popularized by the package Vowpal Wabbit developed at Microsoft. The gist of this trick is that you use a hash function to generate a hashed value of each category. The hashed value will become the index of that category. Because you can specify the hash space, you can fix the number of encoded values for a feature in advance, without having to know how many categories there will be. For example, if you choose a hash space of 18 bits, which corresponds to 218 = 262,144 possible hashed values, all the categories, even the ones that your model has never seen before, will be encoded by an index between 0 and 262,143.One problem with hashed functions is collision: two categories being assigned the same index. However, with many hash functions, the collisions are random; new brands can share an index with any of the existing brands instead of always sharing an index with unpopular brands, which is what happens when we use the preceding UNKNOWN category. The impact of colliding hashed features is, fortunately, not that bad. In research done by Booking.com, even for 50% colliding features, the performance loss is less than 0.5%\n\n\n\n\n\n4 categorical encoding concepts\nCategorical features for high cardinality\nCategorical encoding in Python\nKaggle Tutorial\n\n\n\n\nFeature crossing is the technique to combine two or more features to generate new features. This technique is useful to model the nonlinear relationships between features. Because feature crossing helps model nonlinear relationships between variables, it’s essential for models that can’t learn or are bad at learning nonlinear relationships, such as linear regression, logistic regression, and tree-based models.\nIt will blow-up the number of features and make the model overfit the data due to high features\n\n\n\n\n\nData leakage refers to the phenomenon when a form of the label “leaks” into the set of features used for making predictions, and this same information is not available during inference.\nCauses\n\nSplitting time-correlated data randomly instead of by time. To prevent future information from leaking into the training process and allowing models to cheat during evaluation, split your data by time, instead of splitting randomly, whenever possible\nScaling before splitting - This will leak mean and variance of the test samples into the training process, allowing a model to adjust its predictions for the test samples. To avoid this type of leakage, always split your data first before scaling, then use the statistics from the train split to scale all the splits.\nFilling in missing data with statistics from the test split - This type of leakage is similar to the type of leakage caused by scaling, and it can be prevented by using only statistics from the train split to fill in missing values in all the splits.\nPoor handling of data duplication before splitting - If you have duplicates or near-duplicates in your data, failing to remove them before splitting your data might cause the same samples to appear in both train and validation/test splits. If you oversample your data, do it after splitting\n\n\n\n\n\n\nA group of examples have strongly correlated labels but are divided into different splits. For example, a patient might have two lung CT scans that are a week apart, which likely have the same labels on whether they contain signs of lung cancer, but one of them is in the train split and the second is in the test split.\n\n\n\n\n\nMeasure the predictive power of each feature or a set of features with respect to the target variable (label). If a feature has unusually high correlation, investigate how this feature is generated and whether the correlation makes sense.\n\n\n\n\n\nSHAP\nBuilt-In feature importance functions of XGBoost\nInterpretML is a great open-source package that leverages feature importance to help understand how a model makes predictions\n\n\n\n\n\nSplit data by time into train/valid/test splits instead of doing it randomly.\n\nIf you oversample your data, do it after splitting.\n\nScale and normalize your data after splitting to avoid data leakage.\n\nUse statistics from only the train split, instead of the entire data, to scale your features and handle missing values.\n\nUnderstand how your data is generated, collected, and processed. Involve domain experts if possible.\n\nKeep track of your data’s lineage.\n\nUnderstand feature importance to your model.\n\nUse features that generalize well.\n\nRemove no longer useful features from your models"
  },
  {
    "objectID": "ds_lifecycle/Resources.html",
    "href": "ds_lifecycle/Resources.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Resources\n\nEnjoy algorithms\nGenerate Images from text\ncode generation models\nDall-e2 explained\nopen source data labeling platform\nIntroduction to ML interviews book\nA Recipe for training neural networks\nModel calibration\nKaggle solutions\nXGBoost resources\nHyperparameter tuning using Milano\nChip Huyen’s Designing ML System book github repo\nGoogle’s rules of ML\nBias of Mortgage\nMachine learning systems design slide deck #mustcheck\nAI Ethics videos by Timnit Gebru\nAI Ethics videos\nAI Incident Database\nH2o Admissible machine learning\nGoogle responsible AI practices\nIBM AI fairness 360\nAI Now\nStitchfix model deployment slide deck\nKubernetes explained\nTrustworthy ML Tutorials\nMadewithml\nPython packaging and dependency management"
  },
  {
    "objectID": "ds_lifecycle/Model_Evaluation.html",
    "href": "ds_lifecycle/Model_Evaluation.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Random baseline\nSimple heuristic\nZero rule baseline - Baseline model always predicts the most common class\nHuman baseline\nExisting solutions\n\n\n\n\nPerturbation tests\n\nThe production data will be noisy compared to training data. So we can make small changes to your test splits to see how these changes affect your model’s performance.\n\nInvariance tests\n\nCertain changes to the sensitive inputs shouldn’t lead to changes in the output. changes to race information shouldn’t affect the mortgage outcome. Similarly, changes to applicants’ names shouldn’t affect their resume screening results nor should someone’s gender affect how much they should be paid. If these happen, there are biases in your model, which might render it unusable no matter how good its performance is.\n\nDirectional expectation tests\n\nCertain changes to the inputs should, however, cause predictable changes in outputs. For example, when developing a model to predict housing prices, keeping all the features the same but increasing the lot size shouldn’t decrease the predicted price, and decreasing the square footage shouldn’t increase it. If the outputs change in the opposite expected direction, your model might not be learning the right thing, and you need to investigate it further before deploying it.\n\nModel calibration\n\nChapter 6. Model Development and Offline Evaluation In Chapter 4, we discussed how to create training data for your model, and in Chapter 5, we discussed how to engineer features from that training data. With the initial set of features, we’ll move to the ML algorithm part of ML systems. For me, this has always been the most fun step, as it allows me to play around with different algorithms and techniques, even the latest ones. This is also the first step where I can see all the hard work I’ve put into data and feature engineering transformed into a system whose outputs (predictions) I can use to evaluate the success of my effort.\n\n\nTo build an ML model, we first need to select the ML model to build. There are so many ML algorithms out there, with more actively being developed. This chapter starts with six tips for selecting the best algorithms for your task.\nThe section that follows discusses different aspects of model development, such as debugging, experiment tracking and versioning, distributed training, and AutoML.\nModel development is an iterative process. After each iteration, you’ll want to compare your model’s performance against its performance in previous iterations and evaluate how suitable this iteration is for production. The last section of this chapter is dedicated to how to evaluate your model before deploying it to production, covering a range of evaluation techniques including perturbation tests, invariance tests, model calibration, and slide-based evaluation.\nI expect that most readers already have an understanding of common ML algorithms such as linear models, decision trees, k-nearest neighbors, and different types of neural networks. This chapter will discuss techniques surrounding these algorithms but won’t go into details of how they work. Because this chapter deals with ML algorithms, it requires a lot more ML knowledge than other chapters. If you’re not familiar with them, I recommend taking an online course or reading a book on ML algorithms before reading this chapter. Readers wanting a quick refresh on basic ML concepts might find helpful the section “Basic ML Reviews” in the book’s GitHub repository.\nModel Development and Training In this section, we’ll discuss necessary aspects to help you develop and train your model, including how to evaluate different ML models for your problem, creating ensembles of models, experiment tracking and versioning, and distributed training, which is necessary for the scale at which models today are usually trained at. We’ll end this section with the more advanced topic of AutoML—using ML to automatically choose a model best for your problem.\nEvaluating ML Models There are many possible solutions to any given problem. Given a task that can leverage ML in its solution, you might wonder what ML algorithm you should use for it. For example, should you start with logistic regression, an algorithm that you’re already familiar with? Or should you try out a new fancy model that is supposed to be the new state of the art for your problem? A more senior colleague mentioned that gradient-boosted trees have always worked for her for this task in the past—should you listen to her advice?\nIf you had unlimited time and compute power, the rational thing to do would be to try all possible solutions and see what is best for you. However, time and compute power are limited resources, and you have to be strategic about what models you select.\nWhen talking about ML algorithms, many people think in terms of classical ML algorithms versus neural networks. There are a lot of interests and media coverage for neural networks, especially deep learning, which is understandable given that most of the AI progress in the last decade happened due to neural networks getting bigger and deeper.\nThese interests and coverage might give off the impression that deep learning is replacing classical ML algorithms. However, even though deep learning is finding more use cases in production, classical ML algorithms are not going away. Many recommender systems still rely on collaborative filtering and matrix factorization. Tree-based algorithms, including gradient-boosted trees, still power many classification tasks with strict latency requirements.\nEven in applications where neural networks are deployed, classic ML algorithms are still being used in tandem. For example, neural networks and decision trees might be used together in an ensemble. A k-means clustering model might be used to extract features to input into a neural network. Vice versa, a pretrained neural network (like BERT or GPT-3) might be used to generate embeddings to input into a logistic regression model.\nWhen selecting a model for your problem, you don’t choose from every possible model out there, but usually focus on a set of models suitable for your problem. For example, if your boss tells you to build a system to detect toxic tweets, you know that this is a text classification problem—given a piece of text, classify whether it’s toxic or not—and common models for text classification include naive Bayes, logistic regression, recurrent neural networks, and transformer-based models such as BERT, GPT, and their variants.\nIf your client wants you to build a system to detect fraudulent transactions, you know that this is the classic abnormality detection problem—fraudulent transactions are abnormalities that you want to detect—and common algorithms for this problem are many, including k-nearest neighbors, isolation forest, clustering, and neural networks.\nKnowledge of common ML tasks and the typical approaches to solve them is essential in this process.\nDifferent types of algorithms require different numbers of labels as well as different amounts of compute power. Some take longer to train than others, whereas some take longer to make predictions. Non-neural network algorithms tend to be more explainable (e.g., what features contributed the most to an email being classified as spam) than neural networks.\nWhen considering what model to use, it’s important to consider not only the model’s performance, measured by metrics such as accuracy, F1 score, and log loss, but also its other properties, such as how much data, compute, and time it needs to train, what’s its inference latency, and interpretability. For example, a simple logistic regression model might have lower accuracy than a complex neural network, but it requires less labeled data to start, it’s much faster to train, it’s much easier to deploy, and it’s also much easier to explain why it’s making certain predictions.\nComparing ML algorithms is out of the scope of this book. No matter how good a comparison is, it will be outdated as soon as new algorithms come out. Back in 2016, LSTM-RNNs were all the rage and the backbone of the architecture seq2seq (Sequence-to-Sequence) that powered many NLP tasks from machine translation to text summarization to text classification. However, just two years later, recurrent architectures were largely replaced by transformer architectures for NLP tasks.\nTo understand different algorithms, the best way is to equip yourself with basic ML knowledge and run experiments with the algorithms you’re interested in. To keep up to date with so many new ML techniques and models, I find it helpful to monitor trends at major ML conferences such as NeurIPS, ICLR, and ICML, as well as following researchers whose work has a high signal-to-noise ratio on Twitter.\nSix tips for model selection Without getting into specifics of different algorithms, here are six tips that might help you decide what ML algorithms to work on next.\nAvoid the state-of-the-art trap While helping companies as well as recent graduates get started in ML, I usually have to spend a nontrivial amount of time steering them away from jumping straight into state-of-the-art models. I can see why people want state-of-the-art models. Many believe that these models would be the best solutions for their problems—why try an old solution if you believe that a newer and superior solution exists? Many business leaders also want to use state-of-the-art models because they want to make their businesses appear cutting edge. Developers might also be more excited getting their hands on new models than getting stuck into the same old things over and over again.\nResearchers often only evaluate models in academic settings, which means that a model being state of the art often means that it performs better than existing models on some static datasets. It doesn’t mean that this model will be fast enough or cheap enough for you to implement. It doesn’t even mean that this model will perform better than other models on your data.\nWhile it’s essential to stay up to date with new technologies and beneficial to evaluate them for your business, the most important thing to do when solving a problem is finding solutions that can solve that problem. If there’s a solution that can solve your problem that is much cheaper and simpler than state-of-the-art models, use the simpler solution.\nStart with the simplest models Zen of Python states that “simple is better than complex,” and this principle is applicable to ML as well. Simplicity serves three purposes. First, simpler models are easier to deploy, and deploying your model early allows you to validate that your prediction pipeline is consistent with your training pipeline. Second, starting with something simple and adding more complex components step-by-step makes it easier to understand your model and debug it. Third, the simplest model serves as a baseline to which you can compare your more complex models.\nSimplest models are not always the same as models with the least effort. For example, pretrained BERT models are complex, but they require little effort to get started with, especially if you use a ready-made implementation like the one in Hugging Face’s Transformer. In this case, it’s not a bad idea to use the complex solution, given that the community around this solution is well developed enough to help you get through any problems you might encounter. However, you might still want to experiment with simpler solutions to ensure that pretrained BERT is indeed better than those simpler solutions for your problem. Pretrained BERT might be low effort to start with, but it can be quite high effort to improve upon. Whereas if you start with a simpler model, there’ll be a lot of room for you to improve upon your model.\nAvoid human biases in selecting models Imagine an engineer on your team is assigned the task of evaluating which model is better for your problem: a gradient-boosted tree or a pretrained BERT model. After two weeks, this engineer announces that the best BERT model outperforms the best gradient-boosted tree by 5%. Your team decides to go with the pretrained BERT model.\nA few months later, however, a seasoned engineer joins your team. She decides to look into gradient-boosted trees again and finds out that this time, the best gradient-boosted tree outperforms the pretrained BERT model you currently have in production. What happened?\nThere are a lot of human biases in evaluating models. Part of the process of evaluating an ML architecture is to experiment with different features and different sets of hyperparameters to find the best model of that architecture. If an engineer is more excited about an architecture, they will likely spend a lot more time experimenting with it, which might result in better-performing models for that architecture.\nWhen comparing different architectures, it’s important to compare them under comparable setups. If you run 100 experiments for an architecture, it’s not fair to only run a couple of experiments for the architecture you’re evaluating it against. You might need to run 100 experiments for the other architecture too.\nBecause the performance of a model architecture depends heavily on the context it’s evaluated in—e.g., the task, the training data, the test data, the hyperparameters, etc.—it’s extremely difficult to make claims that a model architecture is better than another architecture. The claim might be true in a context, but unlikely true for all possible contexts.\nEvaluate good performance now versus good performance later The best model now does not always mean the best model two months from now. For example, a tree-based model might work better now because you don’t have a ton of data yet, but two months from now, you might be able to double your amount of training data, and your neural network might perform much better.1\nA simple way to estimate how your model’s performance might change with more data is to use learning curves. A learning curve of a model is a plot of its performance—e.g., training loss, training accuracy, validation accuracy—against the number of training samples it uses. The learning curve won’t help you estimate exactly how much performance gain you can get from having more training data, but it can give you a sense of whether you can expect any performance gain at all from more training data.\nA situation that I’ve encountered is when a team evaluates a simple neural network against a collaborative filtering model for making recommendations. When evaluating both models offline, the collaborative filtering model outperformed. However, the simple neural network can update itself with each incoming example, whereas the collaborative filtering has to look at all the data to update its underlying matrix. The team decided to deploy both the collaborative filtering model and the simple neural network. They used the collaborative filtering model to make predictions for users, and continually trained the simple neural network in production with new, incoming data. After two weeks, the simple neural network was able to outperform the collaborative filtering model.\nWhile evaluating models, you might want to take into account their potential for improvements in the near future, and how easy/difficult it is to achieve those improvements.\nEvaluate trade-offs There are many trade-offs you have to make when selecting models. Understanding what’s more important in the performance of your ML system will help you choose the most suitable model.\nOne classic example of trade-off is the false positives and false negatives trade-off. Reducing the number of false positives might increase the number of false negatives, and vice versa. In a task where false positives are more dangerous than false negatives, such as fingerprint unlocking (unauthorized people shouldn’t be classified as authorized and given access), you might prefer a model that makes fewer false positives. Similarly, in a task where false negatives are more dangerous than false positives, such as COVID-19 screening (patients with COVID-19 shouldn’t be classified as no COVID-19), you might prefer a model that makes fewer false negatives.\nAnother example of trade-off is compute requirement and accuracy—a more complex model might deliver higher accuracy but might require a more powerful machine, such as a GPU instead of a CPU, to generate predictions with acceptable inference latency. Many people also care about the interpretability and performance trade-off. A more complex model can give a better performance, but its results are less interpretable.\nUnderstand your model’s assumptions The statistician George Box said in 1976 that “all models are wrong, but some are useful.” The real world is intractably complex, and models can only approximate using assumptions. Every single model comes with its own assumptions. Understanding what assumptions a model makes and whether our data satisfies those assumptions can help you evaluate which model works best for your use case.\nFollowing are some of the common assumptions. It’s not meant to be an exhaustive list, but just a demonstration:\nPrediction assumption Every model that aims to predict an output Y from an input X makes the assumption that it’s possible to predict Y based on X.\nIID Neural networks assume that the examples are independent and identically distributed, which means that all the examples are independently drawn from the same joint distribution.\nSmoothness Every supervised machine learning method assumes that there’s a set of functions that can transform inputs into outputs such that similar inputs are transformed into similar outputs. If an input X produces an output Y, then an input close to X would produce an output proportionally close to Y.\nTractability Let X be the input and Z be the latent representation of X. Every generative model makes the assumption that it’s tractable to compute the probability P(Z|X).\nBoundaries A linear classifier assumes that decision boundaries are linear.\nConditional independence A naive Bayes classifier assumes that the attribute values are independent of each other given the class.\nNormally distributed Many statistical methods assume that data is normally distributed.\nEnsembles When considering an ML solution to your problem, you might want to start with a system that contains just one model (the process of selecting one model for your problem was discussed earlier in the chapter). After developing one single model, you might think about how to continue improving its performance. One method that has consistently given a performance boost is to use an ensemble of multiple models instead of just an individual model to make predictions. Each model in the ensemble is called a base learner. For example, for the task of predicting whether an email is SPAM or NOT SPAM, you might have three different models. The final prediction for each email is the majority vote of all three models. So if at least two base learners output SPAM, the email will be classified as SPAM.\nTwenty out of 22 winning solutions on Kaggle competitions in 2021, as of August 2021, use ensembles.2 As of January 2022, 20 top solutions on SQuAD 2.0, the Stanford Question Answering Dataset, are ensembles, as shown in Figure 6-2.\nEnsembling methods are less favored in production because ensembles are more complex to deploy and harder to maintain. However, they are still common for tasks where a small performance boost can lead to a huge financial gain, such as predicting click-through rate for ads.\nWe’ll go over an example to give you the intuition of why ensembling works. Imagine you have three email spam classifiers, each with an accuracy of 70%. Assuming that each classifier has an equal probability of making a correct prediction for each email, and that these three classifiers are not correlated, we’ll show that by taking the majority vote of these three classifiers, we can get an accuracy of 78.4%.\nFor each email, each classifier has a 70% chance of being correct. The ensemble will be correct if at least two classifiers are correct. Table 6-1 shows the probabilities of different possible outcomes of the ensemble given an email. This ensemble will have an accuracy of 0.343 + 0.441 = 0.784, or 78.4%.\nOutputs of three models Probability Ensemble’s output All three are correct 0.7 * 0.7 * 0.7 = 0.343 Correct Only two are correct (0.7 * 0.7 * 0.3) * 3 = 0.441 Correct Only one is correct (0.3 * 0.3 * 0.7) * 3 = 0.189 Wrong None are correct 0.3 * 0.3 * 0.3 = 0.027 Wrong This calculation only holds if the classifiers in an ensemble are uncorrelated. If all classifiers are perfectly correlated—all three of them make the same prediction for every email—the ensemble will have the same accuracy as each individual classifier. When creating an ensemble, the less correlation there is among base learners, the better the ensemble will be. Therefore, it’s common to choose very different types of models for an ensemble. For example, you might create an ensemble that consists of one transformer model, one recurrent neural network, and one gradient-boosted tree.\nThere are three ways to create an ensemble: bagging, boosting, and stacking. In addition to helping boost performance, according to several survey papers, ensemble methods such as boosting and bagging, together with resampling, have shown to help with imbalanced datasets.3 We’ll go over each of these three methods, starting with bagging.\nBagging Bagging, shortened from bootstrap aggregating, is designed to improve both the training stability and accuracy of ML algorithms.4 It reduces variance and helps to avoid overfitting.\nGiven a dataset, instead of training one classifier on the entire dataset, you sample with replacement to create different datasets, called bootstraps, and train a classification or regression model on each of these bootstraps. Sampling with replacement ensures that each bootstrap is created independently from its peers. Figure 6-3 shows an illustration of bagging.\nFigure 6-3. Bagging illustration. Source: Adapted from an image by Sirakorn If the problem is classification, the final prediction is decided by the majority vote of all models. For example, if 10 classifiers vote SPAM and 6 models vote NOT SPAM, the final prediction is SPAM.\nIf the problem is regression, the final prediction is the average of all models’ predictions.\nBagging generally improves unstable methods, such as neural networks, classification and regression trees, and subset selection in linear regression. However, it can mildly degrade the performance of stable methods such as k-nearest neighbors.5\nA random forest is an example of bagging. A random forest is a collection of decision trees constructed by both bagging and feature randomness, where each tree can pick only from a random subset of features to use.\nBoosting Boosting is a family of iterative ensemble algorithms that convert weak learners to strong ones. Each learner in this ensemble is trained on the same set of samples, but the samples are weighted differently among iterations. As a result, future weak learners focus more on the examples that previous weak learners misclassified. Figure 6-4 shows an illustration of boosting, which involves the steps that follow.\nFigure 6-4. Boosting illustration. Source: Adapted from an image by Sirakorn You start by training the first weak classifier on the original dataset.\nSamples are reweighted based on how well the first classifier classifies them, e.g., misclassified samples are given higher weight.\nTrain the second classifier on this reweighted dataset. Your ensemble now consists of the first and the second classifiers.\nSamples are weighted based on how well the ensemble classifies them.\nTrain the third classifier on this reweighted dataset. Add the third classifier to the ensemble.\nRepeat for as many iterations as needed.\nForm the final strong classifier as a weighted combination of the existing classifiers—classifiers with smaller training errors have higher weights.\nAn example of a boosting algorithm is a gradient boosting machine (GBM), which produces a prediction model typically from weak decision trees. It builds the model in a stage-wise fashion like other boosting methods do, and it generalizes them by allowing optimization of an arbitrary differentiable loss function.\nXGBoost, a variant of GBM, used to be the algorithm of choice for many winning teams of ML competitions.6 It’s been used in a wide range of tasks from classification, ranking, to the discovery of the Higgs Boson.7 However, many teams have been opting for LightGBM, a distributed gradient boosting framework that allows parallel learning, which generally allows faster training on large datasets.\nStacking Stacking means that you train base learners from the training data then create a meta-learner that combines the outputs of the base learners to output final predictions, as shown in Figure 6-5. The meta-learner can be as simple as a heuristic: you take the majority vote (for classification tasks) or the average vote (for regression tasks) from all base learners. It can be another model, such as a logistic regression model or a linear regression model.\nFigure 6-5. A visualization of a stacked ensemble from three base learners For more great advice on how to create an ensemble, refer to the awesome ensemble guide by one of Kaggle’s legendary teams, MLWave.\nExperiment Tracking and Versioning During the model development process, you often have to experiment with many architectures and many different models to choose the best one for your problem. Some models might seem similar to each other and differ in only one hyperparameter—such as one model using a learning rate of 0.003 and another model using a learning rate of 0.002—and yet their performances are dramatically different. It’s important to keep track of all the definitions needed to re-create an experiment and its relevant artifacts. An artifact is a file generated during an experiment—examples of artifacts can be files that show the loss curve, evaluation loss graph, logs, or intermediate results of a model throughout a training process. This enables you to compare different experiments and choose the one best suited for your needs. Comparing different experiments can also help you understand how small changes affect your model’s performance, which, in turn, gives you more visibility into how your model works.\nThe process of tracking the progress and results of an experiment is called experiment tracking. The process of logging all the details of an experiment for the purpose of possibly recreating it later or comparing it with other experiments is called versioning. These two go hand in hand with each other. Many tools originally set out to be experiment tracking tools, such as MLflow and Weights & Biases, have grown to incorporate versioning. Many tools originally set out to be versioning tools, such as DVC, have also incorporated experiment tracking.\nExperiment tracking A large part of training an ML model is babysitting the learning processes. Many problems can arise during the training process, including loss not decreasing, overfitting, underfitting, fluctuating weight values, dead neurons, and running out of memory. It’s important to track what’s going on during training not only to detect and address these issues but also to evaluate whether your model is learning anything useful.\nWhen I just started getting into ML, all I was told to track was loss and speed. Fast-forward several years, and people are tracking so many things that their experiment tracking boards look both beautiful and terrifying at the same time. Following is just a short list of things you might want to consider tracking for each experiment during its training process:\nThe loss curve corresponding to the train split and each of the eval splits.\nThe model performance metrics that you care about on all nontest splits, such as accuracy, F1, perplexity.\nThe log of corresponding sample, prediction, and ground truth label. This comes in handy for ad hoc analytics and sanity check.\nThe speed of your model, evaluated by the number of steps per second or, if your data is text, the number of tokens processed per second.\nSystem performance metrics such as memory usage and CPU/GPU utilization. They’re important to identify bottlenecks and avoid wasting system resources.\nThe values over time of any parameter and hyperparameter whose changes can affect your model’s performance, such as the learning rate if you use a learning rate schedule; gradient norms (both globally and per layer), especially if you’re clipping your gradient norms; and weight norm, especially if you’re doing weight decay.\nIn theory, it’s not a bad idea to track everything you can. Most of the time, you probably don’t need to look at most of them. But when something does happen, one or more of them might give you clues to understand and/or debug your model. In general, tracking gives you observability into the state of your model.8 However, in practice, due to the limitations of tooling today, it can be overwhelming to track too many things, and tracking less important things can distract you from tracking what is really important.\nExperiment tracking enables comparison across experiments. By observing how a certain change in a component affects the model’s performance, you gain some understanding into what that component does.\nA simple way to track your experiments is to automatically make copies of all the code files needed for an experiment and log all outputs with their timestamps.9 Using third-party experiment tracking tools, however, can give you nice dashboards and allow you to share your experiments with your coworkers.\nVersioning Imagine this scenario. You and your team spent the last few weeks tweaking your model, and one of the runs finally showed promising results. You wanted to use it for more extensive tests, so you tried to replicate it using the set of hyperparameters you’d noted down somewhere, only to find out that the results weren’t quite the same. You remembered that you’d made some changes to the code between that run and the next, so you tried your best to undo the changes from memory because your reckless past self had decided that the change was too minimal to be committed. But you still couldn’t replicate the promising result because there are just too many possible ways to make changes.\nThis problem could have been avoided if you versioned your ML experiments. ML systems are part code, part data, so you need to not only version your code but your data as well. Code versioning has more or less become a standard in the industry. However, at this point, data versioning is like flossing. Everyone agrees it’s a good thing to do, but few do it.\nThere are a few reasons why data versioning is challenging. One reason is that because data is often much larger than code, we can’t use the same strategy that people usually use to version code to version data.\nFor example, code versioning is done by keeping track of all the changes made to a codebase. A change is known as a diff, short for difference. Each change is measured by line-by-line comparison. A line of code is usually short enough for line-by-line comparison to make sense. However, a line of your data, especially if it’s stored in a binary format, can be indefinitely long. Saying that this line of 1,000,000 characters is different from the other line of 1,000,000 characters isn’t going to be that helpful.\nCode versioning tools allow users to revert to a previous version of the codebase by keeping copies of all the old files. However, a dataset used might be so large that duplicating it multiple times might be unfeasible.\nCode versioning tools allow for multiple people to work on the same codebase at the same time by duplicating the codebase on each person’s local machine. However, a dataset might not fit into a local machine.\nSecond, there’s still confusion in what exactly constitutes a diff when we version data. Would diffs mean changes in the content of any file in your data repository, only when a file is removed or added, or when the checksum of the whole repository has changed?\nAs of 2021, data versioning tools like DVC only register a diff if the checksum of the total directory has changed and if a file is removed or added.\nAnother confusion is in how to resolve merge conflicts: if developer 1 uses data version X to train model A and developer 2 uses data version Y to train model B, it doesn’t make sense to merge data versions X and Y to create Z, since there’s no model corresponding with Z.\nThird, if you use user data to train your model, regulations like General Data Protection Regulation (GDPR) might make versioning this data complicated. For example, regulations might mandate that you delete user data if requested, making it legally impossible to recover older versions of your data.\nAggressive experiment tracking and versioning helps with reproducibility, but it doesn’t ensure reproducibility. The frameworks and hardware you use might introduce nondeterminism to your experiment results,10 making it impossible to replicate the result of an experiment without knowing everything about the environment your experiment runs in.\nThe way we have to run so many experiments right now to find the best possible model is the result of us treating ML as a black box. Because we can’t predict which configuration will work best, we have to experiment with multiple configurations. However, I hope that as the field progresses, we’ll gain more understanding into different models and can reason about what model will work best instead of running hundreds or thousands of experiments.\nDEBUGGING ML MODELS Debugging is an inherent part of developing any piece of software. ML models aren’t an exception. Debugging is never fun, and debugging ML models can be especially frustrating for the following three reasons.\nFirst, ML models fail silently, a topic we’ll cover in depth in Chapter 8. The code compiles. The loss decreases as it should. The correct functions are called. The predictions are made, but the predictions are wrong. The developers don’t notice the errors. And worse, users don’t either and use the predictions as if the application was functioning as it should.\nSecond, even when you think you’ve found the bug, it can be frustratingly slow to validate whether the bug has been fixed. When debugging a traditional software program, you might be able to make changes to the buggy code and see the result immediately. However, when making changes to an ML model, you might have to retrain the model and wait until it converges to see whether the bug is fixed, which can take hours. In some cases, you can’t even be sure whether the bugs are fixed until the model is deployed to the users.\nThird, debugging ML models is hard because of their cross-functional complexity. There are many components in an ML system: data, labels, features, ML algorithms, code, infrastructure, etc. These different components might be owned by different teams. For example, data is managed by data engineers, labels by subject matter experts, ML algorithms by data scientists, and infrastructure by ML engineers or the ML platform team. When an error occurs, it could be because of any of these components or a combination of them, making it hard to know where to look or who should be looking into it.\nHere are some of the things that might cause an ML model to fail:\nTheoretical constraints As discussed previously, each model comes with its own assumptions about the data and the features it uses. A model might fail because the data it learns from doesn’t conform to its assumptions. For example, you use a linear model for the data whose decision boundaries aren’t linear.\nPoor implementation of model The model might be a good fit for the data, but the bugs are in the implementation of the model. For example, if you use PyTorch, you might have forgotten to stop gradient updates during evaluation when you should. The more components a model has, the more things that can go wrong, and the harder it is to figure out which goes wrong. However, with models being increasingly commoditized and more and more companies using off-the-shelf models, this is becoming less of a problem.\nPoor choice of hyperparameters With the same model, one set of hyperparameters can give you the state-of-the-art result but another set of hyperparameters might cause the model to never converge. The model is a great fit for your data, and its implementation is correct, but a poor set of hyperparameters might render your model useless.\nData problems There are many things that could go wrong in data collection and preprocessing that might cause your models to perform poorly, such as data samples and labels being incorrectly paired, noisy labels, features normalized using outdated statistics, and more.\nPoor choice of features There might be many possible features for your models to learn from. Too many features might cause your models to overfit to the training data or cause data leakage. Too few features might lack predictive power to allow your models to make good predictions.\nDebugging should be both preventive and curative. You should have healthy practices to minimize the opportunities for bugs to proliferate as well as a procedure for detecting, locating, and fixing bugs. Having the discipline to follow both the best practices and the debugging procedure is crucial in developing, implementing, and deploying ML models.\nThere is, unfortunately, still no scientific approach to debugging in ML. However, there have been a number of tried-and-true debugging techniques published by experienced ML engineers and researchers. The following are three of them. Readers interested in learning more might want to check out Andrej Karpathy’s awesome post “A Recipe for Training Neural Networks”.\nStart simple and gradually add more components Start with the simplest model and then slowly add more components to see if it helps or hurts the performance. For example, if you want to build a recurrent neural network (RNN), start with just one level of RNN cell before stacking multiple together or adding more regularization. If you want to use a BERT-like model (Devlin et al. 2018), which uses both a masked language model (MLM) and next sentence prediction (NSP) loss, you might want to use only the MLM loss before adding NSP loss.\nCurrently, many people start out by cloning an open source implementation of a state-of-the-art model and plugging in their own data. On the off-chance that it works, it’s great. But if it doesn’t, it’s very hard to debug the system because the problem could have been caused by any of the many components in the model.\nOverfit a single batch After you have a simple implementation of your model, try to overfit a small amount of training data and run evaluation on the same data to make sure that it gets to the smallest possible loss. If it’s for image recognition, overfit on 10 images and see if you can get the accuracy to be 100%, or if it’s for machine translation, overfit on 100 sentence pairs and see if you can get to a BLEU score of near 100. If it can’t overfit a small amount of data, there might be something wrong with your implementation.\nSet a random seed There are so many factors that contribute to the randomness of your model: weight initialization, dropout, data shuffling, etc. Randomness makes it hard to compare results across different experiments—you have no idea if the change in performance is due to a change in the model or a different random seed. Setting a random seed ensures consistency between different runs. It also allows you to reproduce errors and other people to reproduce your results.\nDistributed Training As models are getting bigger and more resource-intensive, companies care a lot more about training at scale.11 Expertise in scalability is hard to acquire because it requires having regular access to massive compute resources. Scalability is a topic that merits a series of books. This section covers some notable issues to highlight the challenges of doing ML at scale and provide a scaffold to help you plan the resources for your project accordingly.\nIt’s common to train a model using data that doesn’t fit into memory. It’s especially common when dealing with medical data such as CT scans or genome sequences. It can also happen with text data if you work for teams that train large language models (cue OpenAI, Google, NVIDIA, Cohere).\nWhen your data doesn’t fit into memory, your algorithms for preprocessing (e.g., zero-centering, normalizing, whitening), shuffling, and batching data will need to run out of core and in parallel.12 When a sample of your data is large, e.g., one machine can handle a few samples at a time, you might only be able to work with a small batch size, which leads to instability for gradient descent-based optimization.\nIn some cases, a data sample is so large it can’t even fit into memory and you will have to use something like gradient checkpointing, a technique that leverages the memory footprint and compute trade-off to make your system do more computation with less memory. According to the authors of the open source package gradient-checkpointing, “For feed-forward models we were able to fit more than 10x larger models onto our GPU, at only a 20% increase in computation time.”13 Even when a sample fits into memory, using checkpointing can allow you to fit more samples into a batch, which might allow you to train your model faster.\nData parallelism It’s now the norm to train ML models on multiple machines. The most common parallelization method supported by modern ML frameworks is data parallelism: you split your data on multiple machines, train your model on all of them, and accumulate gradients. This gives rise to a couple of issues.\nA challenging problem is how to accurately and effectively accumulate gradients from different machines. As each machine produces its own gradient, if your model waits for all of them to finish a run—synchronous stochastic gradient descent (SGD)—stragglers will cause the entire system to slow down, wasting time and resources.14 The straggler problem grows with the number of machines, as the more workers, the more likely that at least one worker will run unusually slowly in a given iteration. However, there have been many algorithms that effectively address this problem.15\nIf your model updates the weight using the gradient from each machine separately—asynchronous SGD—gradient staleness might become a problem because the gradients from one machine have caused the weights to change before the gradients from another machine have come in.16\nThe difference between synchronous SGD and asynchronous SGD is illustrated in Figure 6-6.\nFigure 6-6. Synchronous SGD versus asynchronous SGD for data parallelism. Source: Adapted from an image by Jim Dowling17 In theory, asynchronous SGD converges but requires more steps than synchronous SGD. However, in practice, when the number of weights is large, gradient updates tend to be sparse, meaning most gradient updates only modify small fractions of the parameters, and it’s less likely that two gradient updates from different machines will modify the same weights. When gradient updates are sparse, gradient staleness becomes less of a problem and the model converges similarly for both synchronous and asynchronous SGD.18\nAnother problem is that spreading your model on multiple machines can cause your batch size to be very big. If a machine processes a batch size of 1,000, then 1,000 machines process a batch size of 1M (OpenAI’s GPT-3 175B uses a batch size of 3.2M in 2020).19 To oversimplify the calculation, if training an epoch on a machine takes 1M steps, training on 1,000 machines might take only 1,000 steps. An intuitive approach is to scale up the learning rate to account for more learning at each step, but we also can’t make the learning rate too big as it will lead to unstable convergence. In practice, increasing the batch size past a certain point yields diminishing returns.20\nLast but not least, with the same model setup, the main worker sometimes uses a lot more resources than other workers. If that’s the case, to make the most use out of all machines, you need to figure out a way to balance out the workload among them. The easiest way, but not the most effective way, is to use a smaller batch size on the main worker and a larger batch size on other workers.\nModel parallelism With data parallelism, each worker has its own copy of the whole model and does all the computation necessary for its copy of the model. Model parallelism is when different components of your model are trained on different machines, as shown in Figure 6-7. For example, machine 0 handles the computation for the first two layers while machine 1 handles the next two layers, or some machines can handle the forward pass while several others handle the backward pass.\nFigure 6-7. Data parallelism and model parallelism. Source: Adapted from an image by Jure Leskovec21 Model parallelism can be misleading because in some cases parallelism doesn’t mean that different parts of the model in different machines are executed in parallel. For example, if your model is a massive matrix and the matrix is split into two halves on two machines, then these two halves might be executed in parallel. However, if your model is a neural network and you put the first layer on machine 1 and the second layer on machine 2, and layer 2 needs outputs from layer 1 to execute, then machine 2 has to wait for machine 1 to finish first to run.\nPipeline parallelism is a clever technique to make different components of a model on different machines run more in parallel. There are multiple variants to this, but the key idea is to break the computation of each machine into multiple parts. When machine 1 finishes the first part of its computation, it passes the result onto machine 2, then continues to the second part, and so on. Machine 2 now can execute its computation on the first part while machine 1 executes its computation on the second part.\nTo make this concrete, consider you have four different machines and the first, second, third, and fourth layers are on machine 1, 2, 3, and 4 respectively. With pipeline parallelism, each mini-batch is broken into four micro-batches. Machine 1 computes the first layer on the first micro-batch, then machine 2 computes the second layer on machine 1’s results while machine 1 computes the first layer on the second micro-batch, and so on. Figure 6-8 shows what pipeline parallelism looks like on four machines; each machine runs both the forward pass and the backward pass for one component of a neural network.\nFigure 6-8. Pipeline parallelism for a neural network on four machines; each machine runs both the forward pass (F) and the backward pass (B) for one component of the neural network. Source: Adapted from an image by Huang et al.22 Model parallelism and data parallelism aren’t mutually exclusive. Many companies use both methods for better utilization of their hardware, even though the setup to use both methods can require significant engineering effort.\nAutoML There’s a joke that a good ML researcher is someone who will automate themselves out of job, designing an AI algorithm intelligent enough to design itself. It was funny until the TensorFlow Dev Summit 2018, where Jeff Dean took the stage and declared that Google intended on replacing ML expertise with 100 times more computational power, introducing AutoML to the excitement and horror of the community. Instead of paying a group of 100 ML researchers/engineers to fiddle with various models and eventually select a suboptimal one, why not use that money on compute to search for the optimal model? A screenshot from the recording of the event is shown in Figure 6-9.\nFigure 6-9. Jeff Dean unveiling Google’s AutoML at TensorFlow Dev Summit 2018 Soft AutoML: Hyperparameter tuning AutoML refers to automating the process of finding ML algorithms to solve real-world problems. One mild form, and the most popular form, of AutoML in production is hyperparameter tuning. A hyperparameter is a parameter supplied by users whose value is used to control the learning process, e.g., learning rate, batch size, number of hidden layers, number of hidden units, dropout probability, β1 and β2 in Adam optimizer, etc. Even quantization—e.g., whether to use 32 bits, 16 bits, or 8 bits to represent a number or a mixture of these representations—can be considered a hyperparameter to tune.23\nWith different sets of hyperparameters, the same model can give drastically different performances on the same dataset. Melis et al. showed in their 2018 paper “On the State of the Art of Evaluation in Neural Language Models” that weaker models with well-tuned hyperparameters can outperform stronger, fancier models. The goal of hyperparameter tuning is to find the optimal set of hyperparameters for a given model within a search space—the performance of each set evaluated on a validation set.\nDespite knowing its importance, many still ignore systematic approaches to hyperparameter tuning in favor of a manual, gut-feeling approach. The most popular is arguably graduate student descent (GSD), a technique in which a graduate student fiddles around with the hyperparameters until the model works.24\nHowever, more and more people are adopting hyperparameter tuning as part of their standard pipelines. Popular ML frameworks either come with built-in utilities or have third-party utilities for hyperparameter tuning—for example, scikit-learn with auto-sklearn,25 TensorFlow with Keras Tuner, and Ray with Tune. Popular methods for hyperparameter tuning include random search,26 grid search, and Bayesian optimization.27 The book AutoML: Methods, Systems, Challenges by the AutoML group at the University of Freiburg dedicates its first chapter (which you can read online for free) to hyperparameter optimization.\nWhen tuning hyperparameters, keep in mind that a model’s performance might be more sensitive to the change in one hyperparameter than another, and therefore sensitive hyperparameters should be more carefully tuned.\nWARNING It’s crucial to never use your test split to tune hyperparameters. Choose the best set of hyperparameters for a model based on its performance on a validation split, then report the model’s final performance on the test split. If you use your test split to tune hyperparameters, you risk overfitting your model to the test split.\nHard AutoML: Architecture search and learned optimizer Some teams take hyperparameter tuning to the next level: what if we treat other components of a model or the entire model as hyperparameters. The size of a convolution layer or whether or not to have a skip layer can be considered a hyperparameter. Instead of manually putting a pooling layer after a convolutional layer or ReLu (rectified linear unit) after linear, you give your algorithm these building blocks and let it figure out how to combine them. This area of research is known as architectural search, or neural architecture search (NAS) for neural networks, as it searches for the optimal model architecture.\nA NAS setup consists of three components:\nA search space Defines possible model architectures—i.e., building blocks to choose from and constraints on how they can be combined.\nA performance estimation strategy To evaluate the performance of a candidate architecture without having to train each candidate architecture from scratch until convergence. When we have a large number of candidate architectures, say 1,000, training all of them until convergence can be costly.\nA search strategy To explore the search space. A simple approach is random search—randomly choosing from all possible configurations—which is unpopular because it’s prohibitively expensive even for NAS. Common approaches include reinforcement learning (rewarding the choices that improve the performance estimation) and evolution (adding mutations to an architecture, choosing the best-performing ones, adding mutations to them, and so on).28\nFor NAS, the search space is discrete—the final architecture uses only one of the available options for each layer/operation,29 and you have to provide the set of building blocks. The common building blocks are various convolutions of different sizes, linear, various activations, pooling, identity, zero, etc. The set of building blocks varies based on the base architecture, e.g., convolutional neural networks or transformers.\nIn a typical ML training process, you have a model and then a learning procedure, an algorithm that helps your model find the set of parameters that minimize a given objective function for a given set of data. The most common learning procedure for neural networks today is gradient descent, which leverages an optimizer to specify how to update a model’s weights given gradient updates.30 Popular optimizers are, as you probably already know, Adam, Momentum, SGD, etc. In theory, you can include optimizers as building blocks in NAS and search for one that works best. In practice, this is difficult to do, since optimizers are sensitive to the setting of their hyperparameters, and the default hyperparameters don’t often work well across architectures.\nThis leads to an exciting research direction: what if we replace the functions that specify the update rule with a neural network? How much to update the model’s weights will be calculated by this neural network. This approach results in learned optimizers, as opposed to hand-designed optimizers.\nSince learned optimizers are neural networks, they need to be trained. You can train your learned optimizer on the same dataset you’re training the rest of your neural network on, but this requires you to train an optimizer every time you have a task.\nAnother approach is to train a learned optimizer once on a set of existing tasks—using aggregated loss on those tasks as the loss function and existing designed optimizers as the learning rule—and use it for every new task after that. For example, Metz et al. constructed a set of thousands of tasks to train learned optimizers. Their learned optimizer was able to generalize to both new datasets and domains as well as new architectures.31 And the beauty of this approach is that the learned optimizer can then be used to train a better-learned optimizer, an algorithm that improves on itself.\nWhether it’s architecture search or meta-learning learning rules, the up-front training cost is expensive enough that only a handful of companies in the world can afford to pursue them. However, it’s important for people interested in ML in production to be aware of the progress in AutoML for two reasons. First, the resulting architectures and learned optimizers can allow ML algorithms to work off-the-shelf on multiple real-world tasks, saving production time and cost, during both training and inferencing. For example, EfficientNets, a family of models produced by Google’s AutoML team, surpass state-of-the-art accuracy with up to 10x better efficiency.32 Second, they might be able to solve many real-world tasks previously impossible with existing architectures and optimizers.\nFOUR PHASES OF ML MODEL DEVELOPMENT Before we transition to model training, let’s take a look at the four phases of ML model development. Once you’ve decided to explore ML, your strategy depends on which phase of ML adoption you are in. There are four phases of adopting ML. The solutions from a phase can be used as baselines to evaluate the solutions from the next phase:\nPhase 1. Before machine learning If this is your first time trying to make this type of prediction from this type of data, start with non-ML solutions. Your first stab at the problem can be the simplest heuristics. For example, to predict what letter users are going to type next in English, you can show the top three most common English letters, “e,” “t,” and “a,” which might get your accuracy to be 30%.\nFacebook newsfeed was introduced in 2006 without any intelligent algorithms—posts were shown in chronological order, as shown in Figure 6-10.33 It wasn’t until 2011 that Facebook started displaying news updates you were most interested in at the top of the feed.\nFigure 6-10. Facebook newsfeed circa 2006. Source: Iveta Ryšavá34 According to Martin Zinkevich in his magnificent “Rules of Machine Learning: Best Practices for ML Engineering”: “If you think that machine learning will give you a 100% boost, then a heuristic will get you 50% of the way there.”35 You might even find that non-ML solutions work fine and you don’t need ML yet.\nPhase 2. Simplest machine learning models For your first ML model, you want to start with a simple algorithm, something that gives you visibility into its working to allow you to validate the usefulness of your problem framing and your data. Logistic regression, gradient-boosted trees, k-nearest neighbors can be great for that. They are also easier to implement and deploy, which allows you to quickly build out a framework from data engineering to development to deployment that you can test out and gain confidence on.\nPhase 3. Optimizing simple models Once you have your ML framework in place, you can focus on optimizing the simple ML models with different objective functions, hyperparameter search, feature engineering, more data, and ensembles.\nPhase 4. Complex models Once you’ve reached the limit of your simple models and your use case demands significant model improvement, experiment with more complex models.\nYou’ll also want to experiment to figure out how quickly your model decays in production (e.g., how often it’ll need to be retrained) so that you can build out your infrastructure to support this retraining requirement.36\nModel Offline Evaluation One common but quite difficult question I often encounter when helping companies with their ML strategies is: “How do I know that our ML models are any good?” In one case, a company deployed ML to detect intrusions to 100 surveillance drones, but they had no way of measuring how many intrusions their system failed to detect, and they couldn’t decide if one ML algorithm was better than another for their needs.\nLacking a clear understanding of how to evaluate your ML systems is not necessarily a reason for your ML project to fail, but it might make it impossible to find the best solution for your need, and make it harder to convince your managers to adopt ML. You might want to partner with the business team to develop metrics for model evaluation that are more relevant to your company’s business.37\nIdeally, the evaluation methods should be the same during both development and production. But in many cases, the ideal is impossible because during development, you have ground truth labels, but in production, you don’t.\nFor certain tasks, it’s possible to infer or approximate labels in production based on users’ feedback, as covered in the section “Natural Labels”. For example, for the recommendation task, it’s possible to infer if a recommendation is good by whether users click on it. However, there are many biases associated with this.\nFor other tasks, you might not be able to evaluate your model’s performance in production directly and might have to rely on extensive monitoring to detect changes and failures in your ML system’s performance. We’ll cover monitoring in Chapter 8.\nOnce your model is deployed, you’ll need to continue monitoring and testing your model in production. In this section, we’ll discuss methods to evaluate your model’s performance before it’s deployed. We’ll start with the baselines against which we will evaluate our models. Then we’ll cover some of the common methods to evaluate your model beyond overall accuracy metrics.\nBaselines Someone once told me that her new generative model achieved the FID score of 10.3 on ImageNet.38 I had no idea what this number meant or whether her model would be useful for my problem.\nAnother time, I helped a company implement a classification model where the positive class appears 90% of the time. An ML engineer on the team told me, all excited, that their initial model achieved an F1 score of 0.90. I asked him how it was compared to random. He had no idea. It turned out that because for his task the POSITIVE class accounts for 90% of the labels, if his model randomly outputs the positive class 90% of the time, its F1 score would also be around 0.90.39 His model might as well be making predictions at random.40\nEvaluation metrics, by themselves, mean little. When evaluating your model, it’s essential to know the baseline you’re evaluating it against. The exact baselines should vary from one use case to another, but here are the five baselines that might be useful across use cases:\nRandom baseline If our model just predicts at random, what’s the expected performance? The predictions are generated at random following a specific distribution, which can be the uniform distribution or the task’s label distribution.\nFor example, consider the task that has two labels, NEGATIVE that appears 90% of the time and POSITIVE that appears 10% of the time. Table 6-2 shows the F1 and accuracy scores of baseline models making predictions at random. However, as an exercise to see how challenging it is for most people to have an intuition for these values, try to calculate these raw numbers in your head before looking at the table.\nTable 6-2. F1 and accuracy scores of a baseline model predicting at random Random distribution Meaning F1 Accuracy Uniform random Predicting each label with equal probability (50%) 0.167 0.5 Task’s label distribution Predicting NEGATIVE 90% of the time, and POSITIVE 10% of the time 0.1 0.82 Simple heuristic Forget ML. If you just make predictions based on simple heuristics, what performance would you expect? For example, if you want to build a ranking system to rank items on a user’s newsfeed with the goal of getting that user to spend more time on the newsfeed, how much time would a user spend if you just rank all the items in reverse chronological order, showing the latest one first?\nZero rule baseline The zero rule baseline is a special case of the simple heuristic baseline when your baseline model always predicts the most common class.\nFor example, for the task of recommending the app a user is most likely to use next on their phone, the simplest model would be to recommend their most frequently used app. If this simple heuristic can predict the next app accurately 70% of the time, any model you build has to outperform it significantly to justify the added complexity.\nHuman baseline In many cases, the goal of ML is to automate what would have been otherwise done by humans, so it’s useful to know how your model performs compared to human experts. For example, if you work on a self-driving system, it’s crucial to measure your system’s progress compared to human drivers, because otherwise you might never be able to convince your users to trust this system. Even if your system isn’t meant to replace human experts and only to aid them in improving their productivity, it’s still important to know in what scenarios this system would be useful to humans.\nExisting solutions In many cases, ML systems are designed to replace existing solutions, which might be business logic with a lot of if/else statements or third-party solutions. It’s crucial to compare your new model to these existing solutions. Your ML model doesn’t always have to be better than existing solutions to be useful. A model whose performance is a little bit inferior can still be useful if it’s much easier or cheaper to use.\nWhen evaluating a model, it’s important to differentiate between “a good system” and “a useful system.” A good system isn’t necessarily useful, and a bad system isn’t necessarily useless. A self-driving vehicle might be good if it’s a significant improvement from previous self-driving systems, but it might not be useful if it doesn’t perform at least as well as human drivers. In some cases, even if an ML system drives better than an average human, people might still not trust it, which renders it not useful. On the other hand, a system that predicts what word a user will type next on their phone might be considered bad if it’s much worse than a native speaker. However, it might still be useful if its predictions can help users type faster some of the time."
  },
  {
    "objectID": "ds_lifecycle/Model_Evaluation.html#evaluation-methods-1",
    "href": "ds_lifecycle/Model_Evaluation.html#evaluation-methods-1",
    "title": "My Datascience Journey",
    "section": "Evaluation Methods",
    "text": "Evaluation Methods\nIn academic settings, when evaluating ML models, people tend to fixate on their performance metrics. However, in production, we also want our models to be robust, fair, calibrated, and overall make sense. We’ll introduce some evaluation methods that help with measuring these characteristics of a model.\nPerturbation tests A group of my students wanted to build an app to predict whether someone has COVID-19 through their cough. Their best model worked great on the training data, which consisted of two-second long cough segments collected by hospitals. However, when they deployed it to actual users, this model’s predictions were close to random.\nOne of the reasons is that actual users’ coughs contain a lot of noise compared to the coughs collected in hospitals. Users’ recordings might contain background music or nearby chatter. The microphones they use are of varying quality. They might start recording their coughs as soon as recording is enabled or wait for a fraction of a second.\nIdeally, the inputs used to develop your model should be similar to the inputs your model will have to work with in production, but it’s not possible in many cases. This is especially true when data collection is expensive or difficult and the best available data you have access to for training is still very different from your real-world data. The inputs your models have to work with in production are often noisy compared to inputs in development.41 The model that performs best on training data isn’t necessarily the model that performs best on noisy data.\nTo get a sense of how well your model might perform with noisy data, you can make small changes to your test splits to see how these changes affect your model’s performance. For the task of predicting whether someone has COVID-19 from their cough, you could randomly add some background noise or randomly clip the testing clips to simulate the variance in your users’ recordings. You might want to choose the model that works best on the perturbed data instead of the one that works best on the clean data.\nThe more sensitive your model is to noise, the harder it will be to maintain it, since if your users’ behaviors change just slightly, such as they change their phones, your model’s performance might degrade. It also makes your model susceptible to adversarial attack.\nInvariance tests A Berkeley study found that between 2008 and 2015, 1.3 million creditworthy Black and Latino applicants had their mortgage applications rejected because of their races.42 When the researchers used the income and credit scores of the rejected applications but deleted the race-identifying features, the applications were accepted.\nCertain changes to the inputs shouldn’t lead to changes in the output. In the preceding case, changes to race information shouldn’t affect the mortgage outcome. Similarly, changes to applicants’ names shouldn’t affect their resume screening results nor should someone’s gender affect how much they should be paid. If these happen, there are biases in your model, which might render it unusable no matter how good its performance is.\nTo avoid these biases, one solution is to do the same process that helped the Berkeley researchers discover the biases: keep the inputs the same but change the sensitive information to see if the outputs change. Better, you should exclude the sensitive information from the features used to train the model in the first place.43\nDirectional expectation tests Certain changes to the inputs should, however, cause predictable changes in outputs. For example, when developing a model to predict housing prices, keeping all the features the same but increasing the lot size shouldn’t decrease the predicted price, and decreasing the square footage shouldn’t increase it. If the outputs change in the opposite expected direction, your model might not be learning the right thing, and you need to investigate it further before deploying it.\nModel calibration Model calibration is a subtle but crucial concept to grasp. Imagine that someone makes a prediction that something will happen with a probability of 70%. What this prediction means is that out of all the times this prediction is made, the predicted outcome matches the actual outcome 70% of the time. If a model predicts that team A will beat team B with a 70% probability, and out of the 1,000 times these two teams play together, team A only wins 60% of the time, then we say that this model isn’t calibrated. A calibrated model should predict that team A wins with a 60% probability.\nTo measure a model’s calibration, a simple method is counting: you count the number of times your model outputs the probability X and the frequency Y of that prediction coming true, and plot X against Y. The graph for a perfectly calibrated model will have X equal Y at all data points. In scikit-learn, you can plot the calibration curve of a binary classifier with the method sklearn.calibration.calibration_curve,\nTo calibrate your models, a common method is Platt scaling, which is implemented in scikit-learn with sklearn.calibration.CalibratedClassifierCV. Another good open source implementation by Geoff Pleiss can be found on GitHub. For readers who want to learn more about the importance of model calibration and how to calibrate neural networks, Lee Richardson and Taylor Pospisil have an excellent blog post based on their work at Google.\nconfidence measurement * Confidence measurement can be considered a way to think about the usefulness threshold for each individual prediction. Indiscriminately showing all a model’s predictions to users, even the predictions that the model is unsure about, can, at best, cause annoyance and make users lose trust in the system * confidence measurement is a metric for each individual sample.\nslice based evaluation * Slicing means to separate your data into subsets and look at your model’s performance on each subset separately. * To overcome Simpson’s paradox - aggregation can conceal and contradict actual situations\nHow to identify critical slices * Heuristics-based * Error Analysis - Manually go through misclassified examples and find patterns among them * Slice Finder * slice Finder: Automated data slicing for model validation"
  },
  {
    "objectID": "ds_lifecycle/Training_Data.html",
    "href": "ds_lifecycle/Training_Data.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "It’s a good practice to keep track of the origin of the data samples and labels\n\n\n\n\nNatural ground truth labels\nFeedback loop length is important to consider\n\n\n\n\n\nweak supervision\nSemi-supervision\nTransfer learning\nActive learning\n\n\n\n\n\n\nSnorkel\nHeuristics are used to develop the label data which are called label fuctions (LF)\nLFs are combined, denoised and reweighted to get a set of most likely correct labels\n\n\n\n\n\n\n\nLeverages structural assumptions to generate new labels based on a small set of initial labels\nSelf-training - Train a model on existing set of labeled data and use this model to make predictions for unlabeled samples\nA semi-supervision method that has gained popularity in recent years is the perturbation-based method. It’s based on the assumption that small perturbations to a sample shouldn’t change its label. So you apply small perturbations to your training instances to obtain new training instances.\nliterature survey\n\n\n\n\n\nInstead of randomly labeling data samples, you label the samples that are most helpful to your models according to some metrics or heuristics. The most straightforward metric is uncertainty measurement—label the examples that your model is the least certain about, hoping that they will help your model learn the decision boundary better.\nliterature survey\n\n\n\n\n\nUsing the right evaluation metrics\n\nUse precision-recall curve in place of ROC curve for imbalance datasets\n\nChanging the data distribution\n\nTomek links - A popular method of undersampling low-dimensional data that was developed back in 1976 is Tomek links.With this technique, you find pairs of samples from opposite classes that are close in proximity and remove the sample of the majority class in each pair.\noversampling low-dimensional data - SMOTE (synthetic minority oversampling technique)\nWhen you resample your training data, never evaluate your model on resampled data, since it will cause your model to overfit to that resampled distribution\nUndersampling runs the risk of losing important data from removing data. Oversampling runs the risk of overfitting on training data, especially if the added copies of the minority class are replicas of existing data.\nTwo phase learning - Train the model on resampled data by undersampling large classes until each class has only N instances. Then fine-tune the model on the original data\nDynamic sampling - oversample the low-performing classes and undersample the high-performing classes during the training process.The method aims to show the model less of what it has already learned and more of what it has not.\n\nAlgorithm-level methods\n\nAdjustment to the loss function\nCost-sensitive learning\n\nClass-balanced loss\nFocal loss\n\nIf a sample has a lower probability of being right, it’ll have a higher weight\n\n\n\n\n\n\n\nUseful even when we have a lot of data, can make our models more robust to noise and adversarial attacks\nSimple label-preserving transformations\nPerturbations - NN are sensitive to noise. 67.97% of the natural images in the Kaggle CIFAR-10 test dataset and 16.04% of the ImageNet test images can be misclassified by changing just one pixel. Adding noisy samples to training data can help models recognize the weak spots in their learned decision boundary and improve their performance. Moosavi-Dezfooli et al. proposed an algorithm, called DeepFool, that finds the minimum possible noise injection needed to cause a misclassification with high confidence.\nData synthesis - In computer vision, a straightforward way to synthesize new data is to combine existing examples with discrete labels to generate continuous labels. Consider a task of classifying images with two possible labels: DOG (encoded as 0) and CAT (encoded as 1). From example _x_1 of label DOG and example _x_2 of label CAT, you can generate x’ such as:\n\nx’=γx1+(1-γ)x2\nThe label of x’ is a combination of the labels of _x_1 and _x_2: γ×0+(1-γ)×1. This method is called mixup. The authors showed that mixup improves models’ generalization, reduces their memorization of corrupt labels, increases their robustness to adversarial examples, and stabilizes the training of generative adversarial networks."
  },
  {
    "objectID": "ds_lifecycle/Regularization.html",
    "href": "ds_lifecycle/Regularization.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "L2 regularization or squared\nWe add penalty term along with SSR to optimize \nRigde regression does not make slope of any variable to zero\nIf a variable is not very important then it’s slope will be closer to zero and its parameter will be shrinked\nRidge regression is useful when all the variables in a model are useful\n\n\n\n\n\nL1 regularization or absolute\nWe add absolute value of the slope to the SSR to optimize\nThe value of slope can become zero\nWe can use lasso where unimportant variables are included in the model\n\n\nUse both Lasso and Ridge regression to get best of the both worlds\n\nElastic Net combines both types of regularization"
  },
  {
    "objectID": "ds_lifecycle/gradient_descent.html",
    "href": "ds_lifecycle/gradient_descent.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Gradient Descent\n\nThe algorithm estimates the direction of steepest descent by computing the gradient of the loss function. The gradient points uphill, so the algorithm steps in the opposite direction by subtracting a fraction of the gradient\nToo bad your phone is out of juice, because the algorithm may not have propelled you to the bottom of a convex mountain. Instead, you may be stuck in a nonconvex landscape of multiple valleys (local minima), peaks (local maxima), saddles (saddle points), and plateaus. In fact, tasks like image recognition, text generation, and speech recognition are nonconvex, and many variations on gradient descent have emerged to handle such situations. For example, the algorithm may have momentum that helps it zoom over small rises and dips, giving it a better chance at arriving at the bottom. Luckily, local and global minima tend to be roughly equivalent."
  },
  {
    "objectID": "ds_lifecycle/Model_Development.html",
    "href": "ds_lifecycle/Model_Development.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "When considering what model to use, it’s important to consider not only the model’s performance, measured by metrics such as accuracy, F1 score, and log loss, but also its other properties, such as how much data, compute, and time it needs to train, what’s its inference latency, and interpretability.\nAvoid the state-of-the-art trap\n\nResearchers often only evaluate models in academic settings, which means that a model being state of the art often means that it performs better than existing models on some static datasets. It doesn’t mean that this model will be fast enough or cheap enough for you to implement. It doesn’t even mean that this model will perform better than other models on your data.\n\nStart with the simplest models\n\nSimplest models are not always the same as models with the least effort. For example, pretrained BERT models are complex, but they require little effort to get started with, especially if you use a ready-made implementation like the one in Hugging Face’s Transformer. In this case, it’s not a bad idea to use the complex solution, given that the community around this solution is well developed enough to help you get through any problems you might encounter. However, you might still want to experiment with simpler solutions to ensure that pretrained BERT is indeed better than those simpler solutions for your problem. Pretrained BERT might be low effort to start with, but it can be quite high effort to improve upon. Whereas if you start with a simpler model, there’ll be a lot of room for you to improve upon your model.\n\nAvoid human biases in selecting models\nEvaluate good performance now versus good performance later\n\nWhile evaluating models, you might want to take into account their potential for improvements in the near future, and how easy/difficult it is to achieve those improvements.\nA situation that I’ve encountered is when a team evaluates a simple neural network against a collaborative filtering model for making recommendations. When evaluating both models offline, the collaborative filtering model outperformed. However, the simple neural network can update itself with each incoming example, whereas the collaborative filtering has to look at all the data to update its underlying matrix. The team decided to deploy both the collaborative filtering model and the simple neural network. They used the collaborative filtering model to make predictions for users, and continually trained the simple neural network in production with new, incoming data. After two weeks, the simple neural network was able to outperform the collaborative filtering model.\n\nEvaluate trade-offs\nUnderstand the model’s assumptions\n\n\n\n\n\n\n\nBagging, shortened from bootstrap aggregating, is designed to improve both the training stability and accuracy of ML algorithms. It reduces variance and helps to avoid overfitting.\n\n\n\n\n\n\nBoosting is a family of iterative ensemble algorithms that convert weak learners to strong ones. Each learner in this ensemble is trained on the same set of samples, but the samples are weighted differently among iterations. As a result, future weak learners focus more on the examples that previous weak learners misclassified.\n\n\n\n\n\n\n\n\n\n\n\nKaggle Ensembling guide\n\n\n\nStart simple and gradually add more components\nOverfit a single batch - If it’s for image recognition, overfit on 10 images and see if you can get the accuracy to be 100%, or if it’s for machine translation, overfit on 100 sentence pairs and see if you can get to a BLEU score of near 100. If it can’t overfit a small amount of data, there might be something wrong with your implementation.\nSet a random seed\n\n\n\n\n\nGradient-checkpointing\n\n\n\n\nEach worker has its own copy of the whole model and does all the computation necessary for its copy of the model\nSynchronous SGD and Asynchronous SGD - The way in which gradients are combined in parallel training\n\n\n\n\n\n\n Model parallelism is when different components of your model are trained on different machines\n\n\n\n\n\n\n\n\n\n\n\nAutoML: Methods, Systems, Challenges\n\n\n\n\nwhat if we replace the functions that specify the update rule with a neural network? How much to update the model’s weights will be calculated by this neural network. This approach results in learned optimizers, as opposed to hand-designed optimizers."
  },
  {
    "objectID": "ds_lifecycle/Sampling.html",
    "href": "ds_lifecycle/Sampling.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Two families of sampling\n\nnonprobability sampling\nrandom sampling (probability based sampling)\n\nNonprobability sampling\n\nconvenience sampling - selection based on availability\nsnowball sampling - Future samples selected based on exisitng samples\nJudgment sampling\nQuota sampling - You select samples based on quotas for certain slices of data without any randomization\n\nRandom sampling\n\nsimple random sampling - All samples in the population equal probability of being selected\nStratified sampling - to sample 1% of data that has two classes, A and B, you can sample 1% of class A and 1% of class B. Challenging in case of multilabel tasks\n\nWeighted sampling\n\nIn weighted sampling, each sample is given a weight, which determines the probability of it being selected.\n\n\n# Choose two items from the list such that 1, 2, 3, 4 each has\n# 20% chance of being selected, while 100 and 1000 each have only 10% chance.\nimport random\nrandom.choices(population=[1, 2, 3, 4, 100, 1000],\n               weights=[0.2, 0.2, 0.2, 0.2, 0.1, 0.1],\n               k=2)\n# This is equivalent to the following\nrandom.choices(population=[1, 1, 2, 2, 3, 3, 4, 4, 100, 1000],\n               k=2)\n\nReservoir sampling - useful when have to deal with streaming data"
  },
  {
    "objectID": "ds_lifecycle/MLOPS.html",
    "href": "ds_lifecycle/MLOPS.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "MlOps\nData science specific orchestrators\n\nAirflow\nArgo\nPrefect\nDagster\n\nScheduler is different from Orchestrator Scheduler is high level - If schedulers are concerned with when to run jobs and what resources are needed to run those jobs, orchestrators are concerned with where to get those resources. Schedulers deal with job-type abstractions such as DAGs, priority queues, user-level quotas (i.e., the maximum number of instances a user can use at a given time), etc. Orchestrators deal with lower-level abstractions like machines, instances, clusters, service-level grouping, replication, etc. If the orchestrator notices that there are more jobs than the pool of available instances, it can increase the number of instances in the available instance pool."
  },
  {
    "objectID": "ds_lifecycle/Model_Deployment.html",
    "href": "ds_lifecycle/Model_Deployment.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Model Deployment\n\nThree main modes of prediction\n\n\n\n\n\nUnifying Batch and streaming pipeline\n\n\n\n\nModel Compression\n\nLow rank Factorization\n\nReplace high-dimensional tensors with lower-dimensional tensors Ex - compact convolutional filters\n\n\nKnowledge distillation\n\nA small model (student) is trained to mimic a larger model or ensemble of models. The advantage of this approach is that it can work regardless of the architectural differences between the teacher and the student networks.\n\nPruning\n\nIn neural networks it means either removing the nodes there by altering the architecture. Or keeping the architecture same and driving the non-important parameters to zero.\n\nQuantization\n\nQuantization reduces a model’s size by using fewer bits to represent its parameters.\n\n\n\nCompiling for Edge devices\n\n\n\nModel Optimization\n\nThere are two ways to optimize your ML models: locally and globally. Locally is when you optimize an operator or a set of operators of your model. Globally is when you optimize the entire computation graph end to end.\nLocal optimization techniques\n\nVectorization\nParallelization\nLoop tiling\nOperator fusion"
  },
  {
    "objectID": "ds_lifecycle/Monitoring_ML_Models_in_production.html",
    "href": "ds_lifecycle/Monitoring_ML_Models_in_production.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Dependency failure\nDeployment failure\nHardware failures\nDowntime or crashing\n\n\n\n\n\nData distribution shifts in production\nEdge cases\nDegenerate feedback loops\n\nwe can detect degenerate feedback loops by measuring the popularity diversity of a system’s output in recommenders. Other metrics - Aggregate diversity, average coverage of long-tail items can help measure the diversity of outputs of a recommender system\nIn 2021, Chia et al. went a step further and proposed the measurement of hit rate against popularity. They first divided items into buckets based on their popularity—e.g., bucket 1 consists of items that have been interacted with less than 100 times, bucket 2 consists of items that have been interacted with more than 100 times but less than 1,000 times, etc. Then they measured the prediction accuracy of a recommender system for each of these buckets. If a recommender system is much better at recommending popular items than recommending less popular items, it likely suffers from popularity bias. Once your system is in production and you notice that its predictions become more homogeneous over time, it likely suffers from degenerate feedback loops.\n\nCorrecting degenerate feedback loops\n\nIntroducing randomization in the predictions can reduce homogeneity\nEach new video is randomly assigned an initial pool of traffic (which can be up to hundreds of impressions). This pool of traffic is used to evaluate each video’s unbiased quality to determine whether it should be moved to a bigger pool of traffic or be marked as irrelevant.\nTo exclude positional bias, we may need to encode the position information using positional features.\nDuring inference, you want to predict whether a user will click on a song regardless of where the song is recommended, so you might want to set the 1st Position feature to be False. Then you look at the model’s predictions for various songs for each user and can choose the order in which to show each song."
  },
  {
    "objectID": "ds_lifecycle/Monitoring_ML_Models_in_production.html#data-distribution-shifts",
    "href": "ds_lifecycle/Monitoring_ML_Models_in_production.html#data-distribution-shifts",
    "title": "My Datascience Journey",
    "section": "Data distribution shifts",
    "text": "Data distribution shifts\n\nConcept Drift\nCovariate shift\nLabel shift\n\nP(X) - probability density of the input P(Y) - probability density of the output\n\n\n\nCovariate Shift\n\n\nDuring model development, covariate shifts can happen due to biases during the data selection process\nCovariate shifts can also happen because the training data is artifically altered to make it easier for the model to learn, Ex - oversampling or undersampling the imbalanced datasets\nIn production, this can happen because of major changes in environment or in the way application is used\n\n\n\nLabel Shift\n\nRemember that covariate shift is when the input distribution changes. When the input distribution changes, the output distribution also changes, resulting in both covariate shift and label shift happening at the same time. Because label shift is closely related to covariate shift, methods for detecting and adapting models to label shifts are similar to covariate shift adaptation methods. We’ll discuss them more later in this chapter.\n\n\n\nConcept Drift\n\nConsider you’re in charge of a model that predicts the price of a house based on its features. Before COVID-19, a three-bedroom apartment in San Francisco could cost $2,000,000. However, at the beginning of COVID-19, many people left San Francisco, so the same apartment would cost only $1,500,000. So even though the distribution of house features remains the same, the conditional distribution of the price of a house given its features has changed.\nConcept drift is seasonal\n\n\n\nDetecting Data Distribution Shifts\n\nAre the model’s performance degrading?\nWhen ground truth labels are unavailable or too delayed, we can monitor other distributions of interest - P(X)\nMonitor P(X), P(Y), P(X|y), P(y|X) when Y is available\n\n\nStatistical Methods\n\nCompute the stats of the values of feature during inference and compare them to the metrics computed during training - Min, Max, Mean, Median, Variance, Quantiles, Skewness, Kurtosis etc\nUse two-sample hypothesis test to test if the difference between two populations is statistically significant.\n\nAlibi Detect is an open-source package for drift detection.\n\n\n\n\nMonitoring ML Specific Metrics\n\nAccuracy related metrics\nPredictions\n\nMonitor predictions for distribution shifts\nPrediction distribution shifts are also a proxy for input distribution shifts\nMonitor predictions for anything odd happening, such as predicting an unusual number of False in a row\n\nMonitoring features\n\nFeature validation: ensuring that features follow an expected schema\nGreat Expectations and Deequ are open-source packages to do data validation\n\n\n\n\nMonitoring Raw inputs"
  },
  {
    "objectID": "ds_lifecycle/Monitoring_ML_Models_in_production.html#resources",
    "href": "ds_lifecycle/Monitoring_ML_Models_in_production.html#resources",
    "title": "My Datascience Journey",
    "section": "Resources",
    "text": "Resources\nDataset shift in Machine learning by Quinonero-Candela"
  },
  {
    "objectID": "ds_lifecycle/ML_Algorithms.html",
    "href": "ds_lifecycle/ML_Algorithms.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Ordered logistic regression in which the outcomes are ordered values\nMulti-class classification is also called multinomial or sotmax regression.\n\nLogistic Slides\nyoutube video\n\nLogit (log of odds) function takes input values in the range of 0 to 1 and transforms them into values over the entire real-number range\nUnder the logistic model, we assume there is a linear relationship between the weighted inputs and the log-odds      \n\n\n\n\n\n\n\nHandle both categorical and continuous data\nLeaves that contain mixtures of classifications are called Impure\nGini Impurity, Entropy and Information Gain are used to measure the impurity of the trees\n\n\nThe lower the Gini Impurity of a variable, the better it is at prediction\nIn case of continuous variables, we sort the numbers and take the average of the two consequtive numbers. The average will be the threshold which will be used for building the tree. The average number with low gini impurity will be preferred\n\n\n\n\n\n\nK-medoids use actual data points as centroids rather than mean positions in a given cluster. The medoids are points that minimize the distance to all other points in their cluster. This variation is more interpretable because the centroids are always data points\nFuzzy C-Means Clustering enables the data points to participate in multiple clusters to varying degrees. It replaces hard cluster assignments with degrees of membership depending on distance from the centroids"
  },
  {
    "objectID": "notes.html",
    "href": "notes.html",
    "title": "Notes",
    "section": "",
    "text": "My Notes on Data Science, Machine learning and Artificial Intelligence, I am learning from different books, videos, courses and Notebooks. These pages are summaries, explanations, important points and replication of various source material from which I am learning."
  },
  {
    "objectID": "deep_learning/02_training_tips.html",
    "href": "deep_learning/02_training_tips.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Tips to Train Your Model\n\nFor images, it’s a good idea to normalize the pixel values to the range of -1 and 1. It works better than normalizing the pixel values to the range of 0 and 1."
  },
  {
    "objectID": "deep_learning/01_intro.html",
    "href": "deep_learning/01_intro.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Dot Product\n\nDot product offers a neat way to represent the weighted sum model output\nThe dot product is defined only if the vectors have the same dimensions\nSometimes the dot product is also referred to as inner products\nThe product of a matrix (feature matrix) and column vector (weights) is another vector\n\n\n\n\nMatrix Vector Multiplication\n\n\n\n\n\nMatrix Multiplication\n\n\n\nSquared magnitude or length or L2-Norm of a vector: dot product of the vector with itself. (dot product of the squared difference between target and prediction)\nDot-product between a pair of vectors can be used as a measure of similarity between them. Similar vectors have larger dot product and dissimilar vectors have near zero dot products.\nA component of a vector along another vector is yielded by the dot product. If the vectors point in more or less the same direction their dot products are higher compared to when the vectors are perpendicular to each other. If the vectors point in opposite direction their dot product will be negative\nThe dot product can be expressed using the cosine of the angle between the vectors\n\n\n\n\nExpressing dot product using cosine\n\n\n\nThe dot product between two vectors is also proportional to the lengths of the vectors. If we want the agreement score to be neutral to the vector length, we can use a normalized dot product - between unit length vectors along the same direction. Normalized dot product (cosine similarity) is used for document similarity (we want similarity score to be independent of document length)\n\n\n\n\nNormalized dot product\n\n\n\nTwo vectors are orthogonal if their dot product is zero"
  },
  {
    "objectID": "study.html",
    "href": "study.html",
    "title": "Study",
    "section": "",
    "text": "A Study undertook to eradicate poverty using Data science and Machine learning"
  },
  {
    "objectID": "ml_optimization_algorithms/gradient_descent.html",
    "href": "ml_optimization_algorithms/gradient_descent.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Gradient Descent\n\n\n\nDifferent initializations will cause gradient descent to converge to different local minimizers"
  },
  {
    "objectID": "python_packages/create_python_package.html",
    "href": "python_packages/create_python_package.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Github repo for the Python Package\nLink to Python Package\nDocumentation for the Python Package\n\n\n\n\nchapter 3 of the book python packages"
  },
  {
    "objectID": "Interview_resources/resources.html",
    "href": "Interview_resources/resources.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Best Interveiw Resources\n\nThe Data Science Interview Book\nThe Illustrated Machine Learning"
  },
  {
    "objectID": "research/unknown.html",
    "href": "research/unknown.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "What is warmup ratio\nsoftmax temperature\nWhat is dice loss\n\n\nResearch papers to read and summarize\nGoogle MusicLM []\nSWIN DETR ConvNeXt DiNAT MSDeformAttn Dilated Neighborbood Attention"
  }
]