[
  {
    "objectID": "unsupervised_learning/01_pca.html",
    "href": "unsupervised_learning/01_pca.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Calcualate the covariance matrix. Covariance matrix will have variance of the features along the diagonal. Other positions will consist of covariance between a pair of features. The resulting covariance matrix will be symmetrical.\nWe need to find the eigenvectors and eigenvalues for the covariance matrix.\nEigenvectors are those vectors whose direction will remain the same after linear transformation. (When we multiply a vector with covariance matrix, the direction of the vector should remain the same)\nBecause the covariance matrix is symmetrical, the resulting vectors will be orthogonal to each other.\nEigenvalues are the strech factors for the eigenvectors. The higher the strech factor, the greater their importance in dimension reduction.\nThe goal of PCA is to find a new set of orthogonal variables that capture the maximum variation in the data and then reduce the dimensionality of the data using these principal components.\nThe rank of the original data matrix and the rank of the transformed matrix may not be the same. The rank of the transformed matrix is equal to the number of non-zero eigenvalues. PCA does not change the rank of the original data matrix, but it does reduce the dimensionality by keeping only the most important principal components that capture the maximum variance in the data.\n\n\n\n\n\nIf the data is correlated then we can use that information to compress the data. Correlated features imply the presence of redundant information. Having redundant information is suboptimal\nReshape the data where variables are uncorrelated and ordered according to importance will be more expressive\nPrincipal components maximize variance. PCA can be thought of as an iterative process that finds directions along which the variance of projected data is maximal.\n\n\n\n\n\nUnderstanding PCA"
  },
  {
    "objectID": "Interesting_packages/01_pandas_coding_assistant.html",
    "href": "Interesting_packages/01_pandas_coding_assistant.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Coding Assistant for Pandas"
  },
  {
    "objectID": "computer_vision/object_detection/training_custom_models_with_yolov.html",
    "href": "computer_vision/object_detection/training_custom_models_with_yolov.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "yolov7 complete guide\nTrain yolov7 on custom dataset\nyolov7 github repo\nGoogle colab notebook tutorial to train yolov7 on custom dataset\nRoboflow notebooks for computer vision\nTorchvision Object detection finetuning tutorial\n\n\n\n\nThis model can do classification, detection and segmentation\nUsing YOLOv8 to train custom model\nultralytics docs\n\n\n\n\n\n\n\nObject Detection github resources"
  },
  {
    "objectID": "computer_vision/object_detection/anchor_less_object_detection.html",
    "href": "computer_vision/object_detection/anchor_less_object_detection.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Anchor less object detection\n\nNanoDet and CornerNet are anchor free object detection models"
  },
  {
    "objectID": "computer_vision/00_Interesting_links_to_read.html",
    "href": "computer_vision/00_Interesting_links_to_read.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Receptive Field\nskip connections\nDice loss - segmentation\ncomputer vision\nGenerative Learning\nGAN\nMedical\nNLP\nReinforcement learning\nUnsupervised learning\n\n\n\n\nhttps://arxiv.org/abs/1609.01388\nhttps://github.com/yahoo/hecate\nhttps://huggingface.co/spaces/Gradio-Blocks/Create_GIFs_from_Video"
  },
  {
    "objectID": "computer_vision/applications/surveillance_with_dl.html",
    "href": "computer_vision/applications/surveillance_with_dl.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Video Feed\nQuality is often lowered to maximize storage Scalalbe system should be able to intrepret low quality images Training should happen on low quality images\n\n\nProcessing power\nProcessing on a centralized server Processing on the edge - TensorRT"
  },
  {
    "objectID": "computer_vision/image_classification/classification.html",
    "href": "computer_vision/image_classification/classification.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "classification with pytorch - Functions to use"
  },
  {
    "objectID": "python/pandas/02_pandas_grouping.html",
    "href": "python/pandas/02_pandas_grouping.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "# Grouping in Pandas\n\n\nimport pandas as pd\n\n\ndata = pd.read_csv(\"/home/thulasiram/personal/my_data_science_journey/personal/ads_pii.csv\")\n\n\ndata.employee_id.nunique()\n\n3711\n\n\n\ndata.head(3)\n\n\n\n\n\n  \n    \n      \n      employee_id\n      manager\n      driver\n      question\n      still_in_company\n      answer\n    \n  \n  \n    \n      0\n      922249\n      Mary\n      My Team\n      Do you have a best friend at work?\n      1\n      Yes\n    \n    \n      1\n      554242\n      Anna\n      My Team\n      Do you feel that your team contributes to your...\n      1\n      Yes, absolutely!\n    \n    \n      2\n      679867\n      Emma\n      My Team\n      Do you have a best friend at work?\n      1\n      Yes\n    \n  \n\n\n\n\n\ndata_deduplicated = data.drop_duplicates(subset=['employee_id','manager'],keep='last')\n\n\ndata_deduplicated.manager.nunique()\n\n815\n\n\n\ndata_deduplicated[data_deduplicated['still_in_company'] == 0].manager.value_counts()\n\nMissouri    17\nAntonia      9\nSophia       8\nRegina       8\nRhoda        8\n            ..\nAnnetta      1\nJean         1\nSalome       1\nDrusilla     1\nPolly        1\nName: manager, Length: 531, dtype: int64\n\n\n\ngrouped = data_deduplicated.groupby(['manager','still_in_company'])['employee_id'].agg(['count'])\n\n\ngrouped\n\n\n\n\n\n  \n    \n      \n      \n      count\n    \n    \n      manager\n      still_in_company\n      \n    \n  \n  \n    \n      Abbie\n      0\n      3\n    \n    \n      1\n      6\n    \n    \n      Abby\n      0\n      3\n    \n    \n      1\n      2\n    \n    \n      Abigail\n      1\n      2\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      Zona\n      1\n      2\n    \n    \n      Zora\n      0\n      2\n    \n    \n      1\n      1\n    \n    \n      Zula\n      0\n      2\n    \n    \n      1\n      4\n    \n  \n\n1218 rows × 1 columns\n\n\n\n\ngrouped_unstack = grouped.unstack(-1).reset_index()\n\n\ngrouped_unstack\n\n\n\n\n\n  \n    \n      \n      manager\n      count\n    \n    \n      still_in_company\n      \n      0\n      1\n    \n  \n  \n    \n      0\n      Abbie\n      3.0\n      6.0\n    \n    \n      1\n      Abby\n      3.0\n      2.0\n    \n    \n      2\n      Abigail\n      NaN\n      2.0\n    \n    \n      3\n      Ada\n      1.0\n      1.0\n    \n    \n      4\n      Adah\n      NaN\n      6.0\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      810\n      Zoe\n      1.0\n      7.0\n    \n    \n      811\n      Zola\n      NaN\n      1.0\n    \n    \n      812\n      Zona\n      NaN\n      2.0\n    \n    \n      813\n      Zora\n      2.0\n      1.0\n    \n    \n      814\n      Zula\n      2.0\n      4.0\n    \n  \n\n815 rows × 3 columns\n\n\n\n\ngrouped_unstack.columns = ['manager','in_company','left_company']\n\n\ngrouped_unstack.fillna(0,inplace=True)\ngrouped_unstack['total'] = grouped_unstack['left_company'] + grouped_unstack['in_company']\ngrouped_unstack['proportion'] = round(grouped_unstack['left_company'] / grouped_unstack['total']*100,2)\n\n\ngrouped_unstack\n\n\n\n\n\n  \n    \n      \n      manager\n      in_company\n      left_company\n      total\n      proportion\n    \n  \n  \n    \n      0\n      Abbie\n      3.0\n      6.0\n      9.0\n      66.67\n    \n    \n      1\n      Abby\n      3.0\n      2.0\n      5.0\n      40.00\n    \n    \n      2\n      Abigail\n      0.0\n      2.0\n      2.0\n      100.00\n    \n    \n      3\n      Ada\n      1.0\n      1.0\n      2.0\n      50.00\n    \n    \n      4\n      Adah\n      0.0\n      6.0\n      6.0\n      100.00\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      810\n      Zoe\n      1.0\n      7.0\n      8.0\n      87.50\n    \n    \n      811\n      Zola\n      0.0\n      1.0\n      1.0\n      100.00\n    \n    \n      812\n      Zona\n      0.0\n      2.0\n      2.0\n      100.00\n    \n    \n      813\n      Zora\n      2.0\n      1.0\n      3.0\n      33.33\n    \n    \n      814\n      Zula\n      2.0\n      4.0\n      6.0\n      66.67\n    \n  \n\n815 rows × 5 columns\n\n\n\n\ngrouped_unstack.sort_values(by='proportion',ascending=False,inplace=True)\n\n\ngrouped_unstack.head(10)\n\n\n\n\n\n  \n    \n      \n      manager\n      in_company\n      left_company\n      total\n      proportion\n    \n  \n  \n    \n      407\n      Jettie\n      0.0\n      2.0\n      2.0\n      100.0\n    \n    \n      101\n      Betty\n      0.0\n      7.0\n      7.0\n      100.0\n    \n    \n      730\n      Sadie\n      0.0\n      3.0\n      3.0\n      100.0\n    \n    \n      99\n      Betsy\n      0.0\n      6.0\n      6.0\n      100.0\n    \n    \n      724\n      Rosina\n      0.0\n      1.0\n      1.0\n      100.0\n    \n    \n      725\n      Rowena\n      0.0\n      1.0\n      1.0\n      100.0\n    \n    \n      100\n      Bettie\n      0.0\n      4.0\n      4.0\n      100.0\n    \n    \n      133\n      Celia\n      0.0\n      2.0\n      2.0\n      100.0\n    \n    \n      131\n      Celestia\n      0.0\n      1.0\n      1.0\n      100.0\n    \n    \n      59\n      Anner\n      0.0\n      2.0\n      2.0\n      100.0\n    \n  \n\n\n\n\n\ndata_deduplicated.groupby(['manager'])['still_in_company'].value_counts()\n\nmanager  still_in_company\nAbbie    1                   6\n         0                   3\nAbby     0                   3\n         1                   2\nAbigail  1                   2\n                            ..\nZona     1                   2\nZora     0                   2\n         1                   1\nZula     1                   4\n         0                   2\nName: still_in_company, Length: 1218, dtype: int64\n\n\n\ndata_deduplicated.groupby(['manager','still_in_company'])['still_in_company'].count()\n\nmanager  still_in_company\nAbbie    0                   3\n         1                   6\nAbby     0                   3\n         1                   2\nAbigail  1                   2\n                            ..\nZona     1                   2\nZora     0                   2\n         1                   1\nZula     0                   2\n         1                   4\nName: still_in_company, Length: 1218, dtype: int64"
  },
  {
    "objectID": "python/pandas/04_useful_pandas_tips.html",
    "href": "python/pandas/04_useful_pandas_tips.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Useful Pandas Methods\n\nimport pandas as pd\n\n\ndata = {\n    \"city\": [\"New York\", \"Paris\", \"London\", \"San Francisco\", \"Tokyo\"],\n    \"temp_c\": [15.0, 13.5, 12.0, 17.0, 25.0]\n}\nrow_labels = [0,1,2,3,4]\n\n\ndf = pd.DataFrame(data,index=row_labels)\n\n\ndf.assign()\n\nTo create a new column for a given dataframe\nThe column names are keywords\nIf the values are callable, they are computed on the df and assigned to the new column\nIf the values are not callable, they are simply assigned\n\n\n# using a callable\ndf = df.assign(temp_f = lambda x: x.temp_c * 9 / 5 + 32)\n\n\ndf\n\n\n\n\n\n  \n    \n      \n      city\n      temp_c\n      temp_f\n    \n  \n  \n    \n      0\n      New York\n      15.0\n      59.0\n    \n    \n      1\n      Paris\n      13.5\n      56.3\n    \n    \n      2\n      London\n      12.0\n      53.6\n    \n    \n      3\n      San Francisco\n      17.0\n      62.6\n    \n    \n      4\n      Tokyo\n      25.0\n      77.0\n    \n  \n\n\n\n\n\n# using a series directly\ndf = df.assign(temp_fe = df['temp_c'] * 9 / 5 + 32)\n\n\ndf\n\n\n\n\n\n  \n    \n      \n      city\n      temp_c\n      temp_f\n      temp_fe\n    \n  \n  \n    \n      0\n      New York\n      15.0\n      59.0\n      59.0\n    \n    \n      1\n      Paris\n      13.5\n      56.3\n      56.3\n    \n    \n      2\n      London\n      12.0\n      53.6\n      53.6\n    \n    \n      3\n      San Francisco\n      17.0\n      62.6\n      62.6\n    \n    \n      4\n      Tokyo\n      25.0\n      77.0\n      77.0"
  },
  {
    "objectID": "spatial_data_processing/osm_processing copy.html",
    "href": "spatial_data_processing/osm_processing copy.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "OpenStreetMap\n\nIt is an crowd-sourced dataset\nIt contains data about streets, buildings, services, landuse etc.\nOSMnx is a package used to retrieve, construct, analyze and visualize street networks from OpenStreetMap and also retrieve data about points of interest such as restaurants, schools and lots of different kind of services.\nIt is also easy to conduct network routing based on walking, cycling or driving by combining OSMnx functionalities with a package called NetworkX\n\n\nimport osmnx as ox\nimport matplotlib.pyplot as plt\n\n\n#place_name = \"Togo, Africa\"\nplace_name = {18.5786832,73.7666697}\n\n\ngraph = ox.graph_from_point(place_name,dist=750,dist_type='bbox',network_type=\"drive\")\n\nEmptyOverpassResponse: There are no data elements in the response JSON\n\n\n\ntype(graph)\n\nnetworkx.classes.multidigraph.MultiDiGraph\n\n\n\nfig, ax = ox.plot_graph(graph)\n\n\n\n\n\nnodes, edges = ox.graph_to_gdfs(graph)\n\n\nnodes.head()\n\n\n\n\n\n  \n    \n      \n      y\n      x\n      street_count\n      geometry\n    \n    \n      osmid\n      \n      \n      \n      \n    \n  \n  \n    \n      652724178\n      18.574935\n      73.763832\n      4\n      POINT (73.76383 18.57493)\n    \n    \n      652724182\n      18.574981\n      73.764610\n      3\n      POINT (73.76461 18.57498)\n    \n    \n      763423062\n      18.571967\n      73.764768\n      3\n      POINT (73.76477 18.57197)\n    \n    \n      871491336\n      18.574828\n      73.770821\n      4\n      POINT (73.77082 18.57483)\n    \n    \n      1377773005\n      18.574932\n      73.763664\n      3\n      POINT (73.76366 18.57493)\n    \n  \n\n\n\n\n\nedges.head()\n\n\n\n\n\n  \n    \n      \n      \n      \n      osmid\n      oneway\n      highway\n      reversed\n      length\n      name\n      geometry\n      access\n      lanes\n      ref\n      maxspeed\n    \n    \n      u\n      v\n      key\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      652724178\n      7984103956\n      0\n      669050753\n      False\n      primary\n      False\n      13.448\n      NaN\n      LINESTRING (73.76383 18.57493, 73.76387 18.57482)\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      6262990166\n      0\n      669050753\n      False\n      primary\n      True\n      60.618\n      NaN\n      LINESTRING (73.76383 18.57493, 73.76364 18.57545)\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      6305085563\n      0\n      73533877\n      True\n      secondary\n      False\n      24.596\n      Moze College Road\n      LINESTRING (73.76383 18.57493, 73.76402 18.574...\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      652724182\n      4676484316\n      0\n      73533877\n      True\n      secondary\n      False\n      69.380\n      Moze College Road\n      LINESTRING (73.76461 18.57498, 73.76493 18.575...\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      7983557257\n      0\n      [250171874, 223437750]\n      False\n      residential\n      [False, True]\n      306.812\n      Echinus Court Road\n      LINESTRING (73.76461 18.57498, 73.76460 18.575...\n      yes\n      NaN\n      NaN\n      NaN\n    \n  \n\n\n\n\n\narea = ox.geocode_to_gdf(place_name)\n\nValueError: each query must be a dict or a string\n\n\n\ntype(area)\n\nNameError: name 'area' is not defined\n\n\n\narea\n\n\n\n\n\n  \n    \n      \n      geometry\n      bbox_north\n      bbox_south\n      bbox_east\n      bbox_west\n      place_id\n      osm_type\n      osm_id\n      lat\n      lon\n      display_name\n      class\n      type\n      importance\n    \n  \n  \n    \n      0\n      POLYGON ((74.91434 26.83563, 74.91534 26.83465...\n      27.860562\n      26.440461\n      76.285428\n      74.914344\n      298175590\n      relation\n      1950062\n      27.150677\n      75.747016\n      Jaipur, Rajasthan, India\n      boundary\n      administrative\n      0.671968\n    \n  \n\n\n\n\n\narea.plot()\n\n<AxesSubplot: >\n\n\n\n\n\n\ntags = {\"building\":True}\n\n\nbuildings = ox.geometries_from_place(place_name,tags)\n\nTypeError: query must be dict, string, or list of strings\n\n\n\nlen(buildings)\n\n32708\n\n\n\nbuildings.head()\n\n\n\n\n\n  \n    \n      \n      \n      nodes\n      building\n      geometry\n      area\n      barrier\n      currency:INR\n      layer\n      name\n      payment:cash\n      payment:fasttag\n      ...\n      name:tg\n      name:fr\n      motor_vehicle\n      architect\n      historic:civilization\n      outdoor_seating\n      location\n      parking\n      changing_table\n      toilets:disposal\n    \n    \n      element_type\n      osmid\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      way\n      383032803\n      [3862350688, 3862350689, 3862350690, 386235069...\n      residential\n      POLYGON ((75.82021 26.78322, 75.82020 26.78289...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      383032804\n      [3862350692, 3862350693, 3862350694, 386235069...\n      residential\n      POLYGON ((75.82045 26.78350, 75.82042 26.78332...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      383032805\n      [3862350696, 3862350697, 3862350698, 386235069...\n      residential\n      POLYGON ((75.82068 26.78322, 75.82084 26.78320...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      383032806\n      [3862350700, 3862350701, 3862350702, 386235070...\n      residential\n      POLYGON ((75.82069 26.78321, 75.82085 26.78319...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      383032807\n      [3862350704, 3862350705, 3862350706, 386235070...\n      residential\n      POLYGON ((75.82068 26.78297, 75.82079 26.78295...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n  \n\n5 rows × 132 columns\n\n\n\n\nbuildings.shape\n\n(32708, 132)\n\n\n\n# List key-value pairs for tags\ntags = {\"amenity\":\"restaurant\"}\n\n\n# Retrieve restaurants\nrestaurants = ox.geometries_from_place(place_name, tags)\n\n# How many restaurants do we have?\nlen(restaurants)\n\n127\n\n\n\nfig, ax = plt.subplots(figsize=(12, 8))\n\n# Plot the footprint\narea.plot(ax=ax, facecolor=\"black\")\n\n# Plot street edges\nedges.plot(ax=ax, linewidth=1, edgecolor=\"dimgray\",alpha=0.9)\n\n# Plot buildings\nbuildings.plot(ax=ax, facecolor=\"yellow\", markersize=20)\n\n# Plot restaurants\nrestaurants.plot(ax=ax, color=\"red\", markersize=20)\nplt.tight_layout()"
  },
  {
    "objectID": "spatial_data_processing/spatial_modelling.html",
    "href": "spatial_data_processing/spatial_modelling.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "import warnings\nimport keplergl\nimport numpy as np\nimport osmnx as ox\nimport pandas as pd\nimport geopandas as gpd\nimport plotly.express as px\nfrom skgstat import Variogram\nimport matplotlib.pyplot as plt\nfrom shapely.geometry import Point\nfrom pykrige.ok import OrdinaryKriging\nfrom scipy.interpolate import NearestNDInterpolator\nfrom tobler.area_weighted import area_interpolate\n# Custom functions\nfrom scripts.utils import pixel2poly\n# Plotting defaults\nplt.style.use('ggplot')\npx.defaults.height = 400; px.defaults.width = 620\nplt.rcParams.update({'font.size': 16, 'axes.labelweight': 'bold', 'figure.figsize': (6, 6), 'axes.grid': False})"
  },
  {
    "objectID": "spatial_data_processing/gee_timelapse.html",
    "href": "spatial_data_processing/gee_timelapse.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Steps to create a Landsat timelapse:\n\nPan and zoom to your area of interest, or click the globe icon at the upper left corner to search for a location.\nUse the drawing tool to draw a rectangle anywhere on the map.\nAdjust the parameters (e.g., start year, end year, title) if needed.\nClick the Create timelapse button to create a timelapse.\nOnce the timelapse has been added to the map, click the hyperlink at the end if you want to download the GIF.\n\n\nimport os\nimport ee\nimport geemap\nimport ipywidgets as widgets\n\n\nMap = geemap.Map()\nMap.add_basemap('HYBRID')\nMap\n\n\n\n\nTraitError: The 'value' trait of a Label instance expected a unicode string, not the int 10.\n\n\nTraitError: The 'value' trait of a Label instance expected a unicode string, not the int 1984.\n\n\nTraitError: The 'value' trait of a Label instance expected a unicode string, not the int 2020.\n\n\nTraitError: The 'value' trait of a Label instance expected a unicode string, not the int 5.\n\n\nTraitError: The 'value' trait of a Label instance expected a unicode string, not the int 10.\n\n\nTraitError: The 'value' trait of a Label instance expected a unicode string, not the int 30.\n\n\nTraitError: The 'value' trait of a Label instance expected a unicode string, not the int 0.\n\n\n\n\n\nGenerating URL...\nDownloading GIF image from https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/videoThumbnails/8557d6ed3c0cdb0ffa673666569b4260-0e20ead6bdef0b1d973c2259323cb508:getPixels\nPlease wait ...\nAn error occurred while downloading.\nUser memory limit exceeded.\nThe input gif file does not exist.\nThe input gif file does not exist.\nAdding GIF to the map ...\nThe provided file does not exist.\nThe timelapse has been added to the map.\n\n\nGenerating URL...\nDownloading GIF image from https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/videoThumbnails/8557d6ed3c0cdb0ffa673666569b4260-0ed6f7095071ae8dea998b8fcb895851:getPixels\nPlease wait ...\nAn error occurred while downloading.\nUser memory limit exceeded.\nThe input gif file does not exist.\nThe input gif file does not exist.\nAdding GIF to the map ...\nThe provided file does not exist.\nThe timelapse has been added to the map.\n\n\nGenerating URL...\nDownloading GIF image from https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/videoThumbnails/8557d6ed3c0cdb0ffa673666569b4260-be14a4d9b69711ca1550db81d8fda1a8:getPixels\nPlease wait ...\nAn error occurred while downloading.\nUser memory limit exceeded.\nThe input gif file does not exist.\nThe input gif file does not exist.\nAdding GIF to the map ...\nThe provided file does not exist.\nThe timelapse has been added to the map."
  },
  {
    "objectID": "3D_deep_learning/07_gan_based_image_synthesis.html",
    "href": "3D_deep_learning/07_gan_based_image_synthesis.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Compositional 3D Aware Image Synthesis"
  },
  {
    "objectID": "3D_deep_learning/06_Differentiable_volumetric_rendering.html",
    "href": "3D_deep_learning/06_Differentiable_volumetric_rendering.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Volumetric rendering is a collection of techniques used to generate a 2D view of discrete 3D data.The projections generated from this method are without any explicit conversion to a geometric representation. Volumetric rendering is typically used when generating surfaces is difficult or can lead to errors. It can also be used when the content (and not just the geometry and surface) of the volume is important. It is typically used for data visualization.\n\n\n\nVolumetric Rendering\n\n\n\n\nTo determine the RGB values at each pixel, a ray is generated from the projection center going through each image pixel of the cameras. We need to check the probability of occupancy or opacity and colors along this ray to determine RGB values for the pixel. Note there are infinitely many points on each such ray. Thus, we need to have a sampling scheme to select a certain number of points along this ray. This sampling operation is called ray sampling.\n\n\n\nwe have densities and colors defined on the nodes of the volume but not on the points on the rays. Thus, we need to have a way to convert densities and colors of volumes to points on rays. This operation is called volume sampling.\nThe points defined in the ray sampling step might not fall exactly on a point. The nodes of the volume grids and points on rays typically have different spatial locations. We need to use an interpolation scheme to interpolate the densities and colors at points of rays from the densities and colors at volumes.\n\n\n\nFrom the densities and colors of the rays, we need to determine the RGB values of each pixel. In this process, we need to compute how many incident lights can arrive at each point along the ray and how many lights are reflected to the image pixel. We call this process ray marching.\n\n\n\nWhile standard volumetric rendering is used to render 2D projections of 3D data, differentiable volume rendering is used to do the opposite: construct 3D data from 2D images.\n\n\nWe will have ground-truth images. From the intial 3D model, 2D images are rendered and compared to the ground truth images. As this process is differentiable, we run an optimization algorithm to get the final resulting volumetric model. This model can be used to render images from new angles.\nwe represent the shape and texture of the object as a parametric function. This function can be used to generate 2D projections. But, given 2D projections (this is typically multiple views of the 3D scene), we can optimize the parameters of these implicit shape and texture functions so that its projections are the multi-view 2D images. This optimization is possible since the rendering process is completely differentiable, and the implicit functions used are also differentiable."
  },
  {
    "objectID": "pytorch/04_intialization.html",
    "href": "pytorch/04_intialization.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Initialization\n\n\nProperties for Initializing a neural network\n\nThe variance of the input should be propagated through the model to the last layer, so that we have a similar standard deviation for the output neurons\nIf the variance vanish in deeper layers then it is difficult to optimize\nGradient distribution with equal variance across layers. If this is not the case then we will have difficulties in selecting the learning rate"
  },
  {
    "objectID": "causal_inference/05_potential_outcomes.html",
    "href": "causal_inference/05_potential_outcomes.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Intro - Potential Outcomes"
  },
  {
    "objectID": "blogs/ideas.html",
    "href": "blogs/ideas.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "How stable diffusion works\nHow to train stable diffusion on your own data\nWhat is textual inversion fine-tuning for stable diffusion\nComputer vision use case for the construction industry\nDeep learning architectures for computer vision\nTransformers for computer vision\nTransformers Vs Convolutions for computer vision\nSegmentation architectures for computer vision\nImage classification architectures for computer vision\nObject detection architectures for computer vision\nGenerative models for computer vision\nHow reverse image search works\nHow computer vision is used in Insurance sector"
  },
  {
    "objectID": "blogs/ideas.html#nlp",
    "href": "blogs/ideas.html#nlp",
    "title": "My Datascience Journey",
    "section": "NLP",
    "text": "NLP\n\nHow Question Answering NLP systems work\nHow NLP is used for Summarization\nHOW Named Entity Recognition Models are trained\nHow Keyword Extraction models are trained using NLP\nPopular Transformer Architectures for NLP\nHow to use NLP for cleaning social media data\nHow open domain chatbots like chatgpt are trained\nHow Rasa trains their models for intent detection and predicting next action\nHow to create your own LLM powered applications"
  },
  {
    "objectID": "blogs/ideas.html#recommendation-systems",
    "href": "blogs/ideas.html#recommendation-systems",
    "title": "My Datascience Journey",
    "section": "Recommendation Systems",
    "text": "Recommendation Systems\n\nHow Deep Knowledge-aware Networks (DKN) is used for News Recommendation System\nHow Tiktok recommenders work\nHow Neural Collobarative Filtering (NCF) works\nAlternative Least Square (ALS) for recommender systems\nAutoencoders for Recommender sytems\nBuilding Graph Based Recommender systems\nPopular architectures for recommender systems\nWhat is Matrix Factorization? How is it used in recommendation systems?\nwhat are session based recommendation systems and how do they work?"
  },
  {
    "objectID": "blogs/ideas.html#data-engineering",
    "href": "blogs/ideas.html#data-engineering",
    "title": "My Datascience Journey",
    "section": "Data Engineering",
    "text": "Data Engineering\n\nData engineering life cycle\nHow to design good data architecture\nwhat is a data mesh\nData pipeline patterns\nHow to orchestrate your data pipelines\nCreating datapiplines in Jupyter\nwhat is Fugue - Distributed compute engine"
  },
  {
    "objectID": "blogs/ideas.html#python",
    "href": "blogs/ideas.html#python",
    "title": "My Datascience Journey",
    "section": "Python",
    "text": "Python\n\nPandas 2.0 release is expected soon. Blog post on the advantages of Pandas 2.0\nUsing Python dictionaries effectively\nUsing Decorators for Data Science\nGenerators and Iterators"
  },
  {
    "objectID": "blogs/ideas.html#data-science-machine-learning-and-deep-learning-concepts-demystified",
    "href": "blogs/ideas.html#data-science-machine-learning-and-deep-learning-concepts-demystified",
    "title": "My Datascience Journey",
    "section": "Data Science, Machine Learning and Deep Learning Concepts Demystified",
    "text": "Data Science, Machine Learning and Deep Learning Concepts Demystified\nExplaining the Jargon which we come across in our day to day life. For example:- * What is Reinforcement Learning with Human Feedback * What is label smoothing * What is KL divergence"
  },
  {
    "objectID": "blogs/ideas.html#content-publishing",
    "href": "blogs/ideas.html#content-publishing",
    "title": "My Datascience Journey",
    "section": "Content Publishing",
    "text": "Content Publishing\n\nHow to create your Data Science Portfolio\nHow to create your personal website without any coding skills\nHow to write and maintain your own Data Science blog"
  },
  {
    "objectID": "blogs/rlhf.html",
    "href": "blogs/rlhf.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Loss functions for ethical, safe?\nTo encode these values into the model\n\n\n\nRHFL for decision making\n\n\nscaler reward\n\n\n\n\nIt came in 2017\n\npolicy gradient methods\n\n\n\nPPO"
  },
  {
    "objectID": "personal/data_engineering.html",
    "href": "personal/data_engineering.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Courses\n\nlearndataengineering by Andreas Kertz"
  },
  {
    "objectID": "personal/to_do.html",
    "href": "personal/to_do.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Fast.ai deep learning from scratch course\nUniversity of Amsterdam Notebooks and course\nStanford transformers videos\nSabestien Rakschika - Deep learning videos from lightning ai\nStatistical Rethinking - Reading the book\nNeural Networks from Scratch book\nNetwork science book\nNetworks online book\nLearn data engineering course\nAlgoexpert.io completing the code puzzles\nSeries of blog post on a topic which you are learning\nComputer vision - Segmentation complete understanding\nDiffusion in Computer vision\nGenerative AI\nLinear Algebra Tivadar danka\nLinear algebra udemy course\nDocker and Kubernetes udemy course\nProbability and statistics in-depth understanding\n\n\n\n\nFluent python"
  },
  {
    "objectID": "personal/blog_ideas.html",
    "href": "personal/blog_ideas.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "contextual bandits or Multi-arm bandits\nDifferenct attention mechanisms and how do they work\nMachine learning used in industry - Scaled robotics, Weave (using ML to decarbonize) etc\nSegment anything\n\nhttps://segment-anything.com/\nhttps://github.com/facebookresearch/segment-anything\nhttps://arxiv.org/abs/2304.02643\n\n\n\ncallable in python\n\n\nnamespace in python"
  },
  {
    "objectID": "scaled/01_test.html",
    "href": "scaled/01_test.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "currentBestTeam = \"\"\n\n\nscores = {currentBestTeam:0}\n\n\nscores\n\n{'': 0}\n\n\n\nscores[currentBestTeam]\n\n0\n\n\n\ncurrentBestTeam = 'python'\n\n\ncurrentBestTeam\n\n'python'\n\n\n\nimport pandas as pd\n\n\ndef isValidSubsequence(array, sequence):\n    # Write your code here.\n    j = 0\n    for i in range(len(array)):\n        if array[i] == sequence[j]:\n            j+=1\n        if j == len(sequence):\n            return True\n    return False\n\n\narray = [6,1,22,25,-1,8,10]\nsequence = [1,6,-1,10]\n\n\nisValidSubsequence(array, sequence)\n\nFalse"
  },
  {
    "objectID": "scaled/blog_post.html",
    "href": "scaled/blog_post.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Many of us are familiar with common applications of Data Science and Machine learning, such as churn prediction, recommendation engines, and customer segmentation.\nSometimes you will come across a use case that astonishes you and make you wonder, “Is it possible with DS and ML?” I came across such a use case recently and I was instantly hooked.\nPredicting Poverty using ML\nA lot of developing countries cannot afford regular surveys which estimate household income or consumption. When COVID hit and people were unable to find work, how can a government provide support to the needy when survey data is not available? How to decide to who support should be provided?\nHave you heard of using night lights as a proxy for regional development to provide social support?\nDo you know how mobile phone interactions are used to identify people living in extreme poverty?\nHow classification and object detection on satellite images are used to identify extreme poverty?\nDo you know how advanced techniques like Reinforcement learning are used in poverty detection?\nI also didn’t know about these things a few months back. I created a website to collect all the relevant information in one place. The website includes summaries of research papers, available datasets, information on geospatial data processing, and satellite imagery.\nI plan to expand the website as a comprehensive resource for using data science and machine learning to combat poverty.\nIf you are interested in this topic, feel free to check out my website and let me know if you have any information to contribute that could aid in poverty eradication using Data Science and Machine Learning."
  },
  {
    "objectID": "unsupervised_learning/04_tbd.html",
    "href": "unsupervised_learning/04_tbd.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "TBD"
  },
  {
    "objectID": "data_manipulation/01_numpy.html",
    "href": "data_manipulation/01_numpy.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "import numpy as np\n\n\n# Generate random numbers\nrng = np.random.default_rng()\n\n\n# Uniform inclusive of 0 and exclusive of 10\nrng.integers(0,10,3)\n\narray([5, 0, 8])\n\n\n\n# Uniform inclusive of 0 and  10\nrng.integers(0,10,3, endpoint=True)\n\narray([ 9,  3, 10])\n\n\n\n# Uniform, x belongs [0,1)\nrng.random(3)\n\narray([0.72175294, 0.0886603 , 0.80159783])\n\n\n\n# Uniform inclusive of 0 and exclusive of 10\nrng.uniform(1,10,3)\n\narray([5.44827948, 4.7933101 , 2.88206522])\n\n\n\n# Mean 0 and std deviation 1\nrng.standard_normal(3)\n\narray([ 0.5878078 ,  0.67988484, -0.83850319])\n\n\n\n# Mean 5 and std. dev 3\nrng.normal(5,2,3)\n\narray([5.54222614, 7.24807366, 5.68900357])\n\n\n\na = np.array([1,2,3,4,5,6,7,6,5,4,3,2,1])\n\n\nnp.where(a > 5)\n\n(array([5, 6, 7]),)\n\n\n\nnp.nonzero(a>5)\n\n(array([5, 6, 7]),)\n\n\n\nnp.where(a >= 5,1,0)\n\narray([0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0])\n\n\n\nnp.clip(a,2,5)\n\narray([2, 2, 3, 4, 5, 5, 5, 5, 5, 4, 3, 2, 2])\n\n\n\nb = np.array([1,2,3,4,5])\n\n\nb[::-1]\n\narray([5, 4, 3, 2, 1])"
  },
  {
    "objectID": "Interpretable_ml/interpret_ai/notebooks/02_gamchanger.html",
    "href": "Interpretable_ml/interpret_ai/notebooks/02_gamchanger.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "from json import load\nimport gamchanger as gc\nimport pandas as pd\nfrom interpret.glassbox import ExplainableBoostingClassifier\n\n\nsamples = load(open('/home/thulasiram/personal/going_deep_and_wide/Interpretable_ml/interpret_ai/interpret_ai/data/sample.json','r'))\nebm_model = load(open('/home/thulasiram/personal/going_deep_and_wide/Interpretable_ml/interpret_ai/interpret_ai/data/model.json','r'))\n\n\nsamples.keys()\n\ndict_keys(['featureNames', 'featureTypes', 'samples', 'labels'])\n\n\n\nX_test = pd.DataFrame(samples['samples'],columns=samples['featureNames'])\n\n\ny_test = pd.Series(samples['labels'])\n\n\nebm = ExplainableBoostingClassifier(random_state=123)\n\n\ngc.visualize(ebm=ebm,model_data=ebm_model,sample_data=samples)\n\n\n        \n        \n    \n\n\n\ngc_dict = load(open('/home/thulasiram/personal/going_deep_and_wide/Interpretable_ml/interpret_ai/interpret_ai/data/modified_model.gamchanger','r'))\n\n\nnew_ebm = gc.get_edited_model(ebm=ebm,gamchanger_export=gc_dict)\n\nTypeError: 'NoneType' object is not iterable\n\n\n\ngc."
  },
  {
    "objectID": "Interpretable_ml/interpret_ai/notebooks/03_rulefit.html",
    "href": "Interpretable_ml/interpret_ai/notebooks/03_rulefit.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom imodels import FIGSClassifier\nfrom sklearn.datasets import load_breast_cancer\nimport os\n\n\nraw_data = load_breast_cancer()\n\n\nX = raw_data.data\ny = raw_data.target\nfeatures = raw_data.feature_names\ntarget_names = raw_data.target_names\n\n\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.20,random_state=123)\n\n\nfigs = FIGSClassifier(max_rules=4)\n\n\nfigs.fit(X_train,y_train,feature_names=features)\n\n\n> ------------------------------\n> FIGS-Fast Interpretable Greedy-Tree Sums:\n>    Predictions are made by summing the \"Val\" reached by traversing each tree\n> ------------------------------\nworst radius <= 16.795 (Tree #0 root)\n    worst concave points <= 0.136 (split)\n        Val: 0.985 (leaf)\n        worst texture <= 25.670 (split)\n            Val: 0.812 (leaf)\n            Val: 0.091 (leaf)\n    mean concavity <= 0.072 (split)\n        Val: 0.500 (leaf)\n        Val: 0.007 (leaf)\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.FIGSClassifier> ------------------------------\n> FIGS-Fast Interpretable Greedy-Tree Sums:\n>    Predictions are made by summing the \"Val\" reached by traversing each tree\n> ------------------------------\nworst radius <= 16.795 (Tree #0 root)\n    worst concave points <= 0.136 (split)\n        Val: 0.985 (leaf)\n        worst texture <= 25.670 (split)\n            Val: 0.812 (leaf)\n            Val: 0.091 (leaf)\n    mean concavity <= 0.072 (split)\n        Val: 0.500 (leaf)\n        Val: 0.007 (leaf)\n\n\n\n\nprint(figs)\n\n> ------------------------------\n> FIGS-Fast Interpretable Greedy-Tree Sums:\n>   Predictions are made by summing the \"Val\" reached by traversing each tree\n> ------------------------------\nworst radius <= 16.795 (Tree #0 root)\n    worst concave points <= 0.136 (split)\n        Val: 0.985 (leaf)\n        worst texture <= 25.670 (split)\n            Val: 0.812 (leaf)\n            Val: 0.091 (leaf)\n    mean concavity <= 0.072 (split)\n        Val: 0.500 (leaf)\n        Val: 0.007 (leaf)\n\n\n\n\nprint(figs.print_tree(X_train, y_train))\n\n------------\nworst radius <= 16.795 284/455 (62.42%)\n    worst concave points <= 0.136 275/302 (91.06%)\n        ΔRisk = 0.98 260/264 (98.48%)\n        worst texture <= 25.670 15/38 (39.47%)\n            ΔRisk = 0.81 13/16 (81.25%)\n            ΔRisk = 0.09 2/22 (9.09%)\n    mean concavity <= 0.072 9/153 (5.88%)\n        ΔRisk = 0.50 8/16 (50.0%)\n        ΔRisk = 0.01 1/137 (0.73%)\n\n\n\n\nfigs.plot(fig_size=8)\n\n\n\n\n\nfigs.complexity_\n\n4\n\n\n\nfigs.get_params()\n\n{'max_features': None,\n 'max_rules': 4,\n 'min_impurity_decrease': 0.0,\n 'random_state': None}\n\n\n\nfigs.feature_names_\n\narray(['mean radius', 'mean texture', 'mean perimeter', 'mean area',\n       'mean smoothness', 'mean compactness', 'mean concavity',\n       'mean concave points', 'mean symmetry', 'mean fractal dimension',\n       'radius error', 'texture error', 'perimeter error', 'area error',\n       'smoothness error', 'compactness error', 'concavity error',\n       'concave points error', 'symmetry error',\n       'fractal dimension error', 'worst radius', 'worst texture',\n       'worst perimeter', 'worst area', 'worst smoothness',\n       'worst compactness', 'worst concavity', 'worst concave points',\n       'worst symmetry', 'worst fractal dimension'], dtype='<U23')"
  },
  {
    "objectID": "anomaly_detection/anomaly_detection_using_graphs.html",
    "href": "anomaly_detection/anomaly_detection_using_graphs.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Structural anomalies - They consider graph structural information. Abnormal nodes have different connection patterns.\nGlobal anomalies - They only consider the node attributes. Nodes with attributes significantly different from other nodes.\nCommunity anomalies - Considers both node attributes and graph structural information.\n\n\n\n\n\n\n\n\n\n\nUsing statistical features associated with each node, such as in/out degree, to detect anomalous nodes.\nProhibitive cost for assessing the most significant features and do not effectively capture the structural information.\nNetwork representation methods such as Deepwalk, Node2Vec and LINE is used to generate node representations. By pairing the conventional anomaly detection techniques such as density or distance based techniques, anomalous nodes can be identified with regard to their distinguishable locations in the embedding space.\n\n\n\n\n\nThese techniques encode the graph structure into an embedded vector space and identify anomalous nodes through further analysis.\n\n\n\n\n\n\n\n\nAutoencoder and DNN provide solid basis for learning data representations.\n\n\n\n\nAutoencoder based anomaly detection\n\n\n\n\n\n\n\n\nGCN anomaly detection\n\n\n\n\n\n\n\nIn addition to structural information and node attributes,, dynamic graphs also contain rich temporal signals. Some anomalies might appear to be normal in the graph snapshot at each time stamp, and, only when the changes in a graph’s structure are considered, do they become noticeable.\n\n\n\n\n\nResearch paper"
  },
  {
    "objectID": "nlp/multilingual_transformers.html",
    "href": "nlp/multilingual_transformers.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "The corpus used for pretraining consists of documents in many languages. A remarkable feature of this approach is that despite receiving no explicit information to differentiate among the languages, the resulting linguistic representations are able to generalize well across languages for a variety of downstream tasks.\n\n\n\nmBERT\nXLM - RoBERTa (XLM- R)\n\n\n\n\n\nUses MLM as a pretraining objective, for 100 languages\nHuge size of the training corpus (2.5 TB data)\nUses SentencePiece to tokenize the raw text directly\nVocabulary size is 250,000 tokens"
  },
  {
    "objectID": "nlp/text_generation.html",
    "href": "nlp/text_generation.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Instead of decoding the token with the highest probability at each step, beam search keeps track of the top-b most probable next tokens, where b is referred to as the number of beams or partial hypotheses. The next set of beams are chosen by considering all possible next-token extensions of the existing set and selecting the b most likely extensions. The process is repeated until we reach the maximum length or an EOS token, and the most likely sequence is selected by ranking the b beams according to their log probabilities.\n\n\n\nBeam Search\n\n\nBeam Search with n-gram penalty - A penalty is added to the beam search to avoid repetetion of the text generated. Another alternative is to avoid repetition is to use sampling."
  },
  {
    "objectID": "nlp/ner_transformer.html",
    "href": "nlp/ner_transformer.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "NER with Transformers\n\n\n\nNER as token classification task\n\n\n\n\n\nHugging Face Transformer class"
  },
  {
    "objectID": "nlp/MusicLM.html",
    "href": "nlp/MusicLM.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Generating high-fidelity music from text descriptions\nConditional music generation as a hierarchical sequence-to-sequence modeling task and it generates music at 24kHZ that remains consistent over several minutes.\nIt can be conditioned on both text and a melody - It can transform whistled and hummed melodies according to the style described in a text caption.\n\n\n\n\n\nScarcity of paired audio-text data. Text descriptions of general audio is harder. Not possible to capture with a few words the characteristics of acoustic scenes.\nAudio is structured along a temporal dimension which makes sequence-wide captions a much weaker level of annotation than an image caption\n\n\n\n\n\nQuantization\nGenerative models for audio\nConditioned audio generation\nText conditioned Image generation\nJoint embedding models for music and text (MuLan)\n\nQuantization - The goal is to provide a compact, discrete representation, which at the same time allows for high-fidelity reconstruction. (VQ-VAE)\nSoundStream is a universal neural audio codec - compress audio and reconstruct - tokenizer. SoundStream uses residual vector quantization (RVQ)\n\n\n\n\nMusicCaps - High quality music caption dataset with 5.5K examples prepared by expert musicians.\n\n\n\n\n\n\n\n\nJoint music-text model that is trained to project music and its corresponding text description to representations close to each other in an embedding space. This shared embedding space eliminates the need for captions at training time altogether, and allows training on massive audio-only corpora.\nIt is a music-text joint embedding model consisting of two embedding towers, one for each modality. The towers map the two modalities to a shared embedding space of 128 dimensions using contrastive learning.\n\n\n\n\naudio –> embeddings –> quantization (RVQ) –> acoustic tokens. Each second of audio is represented by 600 tokens, referred as audio tokens.\n\n\n\nAn intermediate layer of MLM module of a w2v-BERT model with 600M parameters are used. Embeddings are extracted from the 7th layer and quantize them using the centroids of a learned k-means over the embeddings. We use 1024 clusters and a sampling rate of 25 Hz, resulting in 25 semantic tokens for every second of audio.\n\n\n\n\nSoundStream, w2v-BERT, MuLan are pretrained independently and frozen, such that they provide the discrete audio and text representations for the sequence-to-sequence modeling.\n\n\n\nTo achieve text-conditioned music generation, hierarchical modeling is proposed. Each stage is modeled autogressively by a separate decoder-only transformer.\n\n\n\n\nPretrained MuLan\nSoundStream and w2v-BERT trained on Free Music Archive (FMA) dataset. They are trained on a dataset amounting to 280K hours of music\n\n\n\n\nMusicCaps dataset was prepared. It consists of 5.5K music clips each paired with corresponding text descriptions in English, written by ten professional musicians. For each 10-second music clip, MusicCaps provides: (1) a free-text caption consisting of four sentences on average, describing the music and (2) a list of music aspects, describing genre, mood, tempo, singer voices, instrumentation, dissonances, rhythm, etc. On average, the dataset includes eleven aspects per clip\n\n\n\nAudio quality\nAdherence to text description\n\nFrechet Audio Distance (FAD) is a reference-free audio quality metric, low scores are preferred\n\nKL Divergence (KLD)\n\nThere is many to many relationship between text descriptions and music clips compatible with them. To overcome this a proxy was adopted. A classifier trained on multi-label classification on AudioSet is used to compute class predictions for both the generated and the reference music and measure the KL divergence between probability distributions of class predictions. KLD is expected to be low.\n\nMuLan Cycle Consistency (MCC) - High preferred.\n\nAs a joint musictext embedding model, MuLan can be used to quantify the similarity between music-text pairs. We compute the MuLan embeddings from the text descriptions in MusicCaps as well as the generated music based on them, and define the MCC metric as the average cosine similarity between these embeddings.\n\nQualitative evaluation\nTraining data memorization\n\nTo study the extent to which MusicLM might memorize music segments.\n\n\n\n\n\n\n\n\nMusicLM compared with Mubert and Riffusion. MusicLM perform better than Mubert and Riffusion.\n\nwhen the transformer models are directly trained on acoustic tokens from MuLAN tokens, there is a drop n KLD and MCC. Semantic modeling facilitate the adherence to the text description.\n\n\n\n\nPotential misappropriation of creative content - conducted a thorough study of memorization - when feeding MuLan embeddings to MusicLM, the sequences of generated tokens significantly differ from the corresponding sequences in the training set.\n\n\n\n\n\nMusicLM website with Music samples generated\nMusicLM Research Paper"
  },
  {
    "objectID": "nlp/dealing_few_labels.html",
    "href": "nlp/dealing_few_labels.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Adapt a pretrained model for another task without training it.\ntext entailment\nunsupervised data augmentation (UDA) uncerainty-aware self training (UST)\nMultilabel classification can be casted as a one-versus-rest classification tasks.\n\n\n\nscikit-multilearn for working with multi label challenges"
  },
  {
    "objectID": "data_architecture/data_lake.html",
    "href": "data_architecture/data_lake.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Image Credit: Amazon AWS\n\n\nData lakes are powerful tools for storing, managing and analyzing vast amounts of data from a variety of sources. They promise to provide businesses with invaluable insights that can help drive growth and innovation. However, like any powerful tool, data lakes can also pose significant risks if not managed properly. Without careful planning and management, a data lake can quickly become a “data swamp,” filled with irrelevant, low-quality, and outdated data that can harm business operations and hinder decision-making. In this blog post, we will explore the common causes of data swamps in data lakes and provide practical tips and best practices to help you avoid the risks and ensure your data lake remains a valuable asset for your organization.\nBefore discussing data swamps, let’s take a look at what are the benefits of a data lake.\n\n\nThere are several benefits to using a data lake, including:\nCentralized data storage: Data lakes provide a centralized location for storing all data, making it easier to access and analyze.\nScalability: Data lakes can store and process petabytes of data, making them ideal for businesses that generate large amounts of data.\nFlexibility: Data lakes can store structured, semi-structured, and unstructured data, allowing businesses to store and analyze data from a variety of sources.\nCost-effectiveness: Data lakes are typically more cost-effective, as they typically store raw data in object stores like s3 or blob storage.\nImproved data analysis: Data lakes enable businesses to apply a wide range of analytics techniques to their data, including machine learning, artificial intelligence, and natural language processing. This enables businesses to gain insights and make more informed decisions.\nOverall, data lakes provide businesses with a powerful tool for storing, processing, and analyzing large amounts of data, enabling better decision-making and business outcomes.\n\n\n\nData lakes are like Gardens. What makes a Garden either attractive or unattractive to us?\nA garden is a thing of beauty, a place to escape the hustle and bustle of everyday life and connect with nature. However, just like any other beautiful thing, a garden requires constant maintenance to keep it looking its best. Without regular care, a garden can quickly become overgrown, unkempt, and unsightly. Weeds will sprout up where they’re not wanted, flowers will wither and die, and before long, the once-beautiful garden will be unrecognizable. But with a little bit of attention and care, a garden can thrive, with vibrant flowers, lush greenery, and a peaceful ambience that can soothe the soul. Data lakes need care and maintenance just like a beautiful garden.\nHere are some reasons why a data lake becomes a data swamp:-\n\nLack of data governance: Without proper data governance policies and procedures, a data lake can become cluttered with irrelevant or inaccurate data, making it difficult to find and use the data that is needed.\nPoor data quality: If data is ingested into the data lake without being properly validated or cleaned, it can lead to poor data quality.\nLack of metadata management: Metadata is data that describes the data, and it is essential for understanding and using the data in the data lake. Without proper metadata management, it can be difficult to understand what data is stored in the data lake and how it should be used.\nLack of data access controls: If the data lake is not properly secured, unauthorized users may be able to access and modify data, which can lead to inaccurate or irrelevant data being stored in the data lake.\nLack of privacy controls: If Personally Identifiable Information (PII) data is not handled properly, it can lead to data leakage and loss of user privacy. This can severely damage the reputation of the organization.\n\n\n\n\nImage Credit: Reddit\n\n\n\n\n\nSimply put, taking regular care and maintenance can be a great way to prevent a data lake from becoming a data swamp.\nHere are some ways to maintain a data lake:-\n\nEstablishing proper data governance policies to ensure that data is properly managed throughout its lifecycle.\nEnsure data quality during the processing of the raw data. Take necessary action to ensure raw data is clean.\nImplement Metadata management and data discovery, your users should be able to find the required data in the data lake and also know how it was generated and what various attributes mean.\nEnsure data access controls, such that only authorized users can access and modify the data.\nEnsure user privacy is protected, never store PII data in the data lake. Ensure PII data is either masked or any appropriate privacy controls are in place. If there is no business case for using PII data then better not to store it in the first place.\n\nThese are easier said than done. It needs a lot of time, effort and coordination with different teams to ensure data lakes are crystal clear and beneficial to the organisation.\nData lakes are not the only solution to handle large amounts of an organisation’s data. There are other alternatives to data lakes like Enterprise Data Warehouse, Data Mart, Data Virtualization, Data Fabric, Data Hub and Data Mesh. Every organization should evaluate the requirements, strengths and weaknesses of each framework and choose the best solution. Let us discuss these frameworks in some of our future blog posts.\nTo Summarize, data lakes are capable to handle a variety of data sources in huge volumes. They are flexible and scalable. When they are planned, built and maintained properly, they can be very beneficial for any organization. If not they will become data swamps. A few ways to prevent a data lake from becoming a data swamp are ensuring data quality, governance, metadata management, data discovery, data access control, and protecting user privacy.\nHope you learned something about data lakes today and let us meet in our next blog post."
  },
  {
    "objectID": "transformers/Transformer_architectures.html",
    "href": "transformers/Transformer_architectures.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "The changes to the transformer architectures can be split into two broad categories:-\n\nChanges to the internal arrangement of transformer block\nChanges to the layers that a transformer block is made of\n\n\n\n\nDecreasing memory footprint and compute\nAdding connections between transformer blocks\nAdaptive computation time (Ex - Early stopping)\nRecurrence or hierarchial structure\nNeural Architecture Search\n\n\n\n\nModifications to the transformer block\n\n\n\n\n\n\nchanges to the four important parts of a transformer\n\nPositional Encodings\nMulti-Head Attention\nResidual connection with layer normalization\nPosition-wise Feed Forward network\n\n\nThe most important being changes to the multi-head attention.\n\n\n\n\nModifications to the Multi-Head Attention sublayer\n\n\n\n\n\nModifications to the positional encodings sublayer"
  },
  {
    "objectID": "transformers/rough_work.html",
    "href": "transformers/rough_work.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\nd_embed = 512       # embedding size for the attention modules\nnum_heads = 8       # Number of attention heads\nnum_batches = 1     # number of batches (1 makes it easier to see what is going on)\nvocab = 50000       # vocab size\nmax_len = 5000      # Max length of TODO what exactly?\nn_layers = 1        # number of attention layers (not used but would be an expected hyper-parameter)\nd_ff = 2048         # hidden state size in the feed forward layers\nepsilon = 1e-6      # epsilon to use when we need a small non-zero number\n\n\n\nx = torch.tensor([[1, 2, 3]]) # input will be 3 tokens\ny = torch.tensor([[1, 2, 3]]) # target will be same as the input for many applications\nx_mask = torch.tensor([[1, 0, 1]]) # Mask the 2nd input token\ny_mask = torch.tensor([[1, 0, 1]]) # Mask the 2nd target token\nprint(\"x\", x.size())\nprint(\"y\", y.size())\n\nx torch.Size([1, 3])\ny torch.Size([1, 3])\n\n\n\n\n# Make the embedding module. It understands that each token should result in a separate embedding.\nemb = nn.Embedding(vocab, d_embed)\nx = emb(x)\n# Scale the embedding\nx = x * math.sqrt(d_embed)\nprint(x.size())\n\ntorch.Size([1, 3, 512])\n\n\n\npe = torch.zeros(max_len,d_embed, requires_grad=False)\n\n\npe.size()\n\ntorch.Size([5000, 512])\n\n\n\nposition = torch.arange(0, max_len).unsqueeze(1)\nprint(position.size())\n\ntorch.Size([5000, 1])\n\n\n\n# Start with an empty tensor\npe = torch.zeros(max_len, d_embed, requires_grad=False)\n# array containing index values 0...max_len\nposition = torch.arange(0, max_len).unsqueeze(1)\ndivisor = torch.exp(torch.arange(0, d_embed, 2) * -(math.log(10000.0) / d_embed))\n# Make overlapping sine and cosine wave inside positional embedding tensor\npe[:, 0::2] = torch.sin(position * divisor)\npe[:, 1::2] = torch.cos(position * divisor)\npe = pe.unsqueeze(0)\n# Add the position embedding to the main embedding\nx = x + pe[:, :x.size(1)]\nprint(x.size())\n\ntorch.Size([1, 3, 512])\n\n\n\nmean = x.mean(-1, keepdim=True)\nstd = x.std(-1, keepdim=True)\nW1 = nn.Parameter(torch.ones(d_embed))\nb1 = nn.Parameter(torch.zeros(d_embed))\nx = W1 * (x - mean) / (std + epsilon) + b1\nprint(x.size())\n\ntorch.Size([1, 3, 512])\n\n\n\n# Make three versions of x, for the query, key, and value\n# We don't need to clone because these will immediately go through linear layers, making new tensors\nk = x # key\nq = x # query\nv = x # value\n# Make three linear layers\n# This is where the network learns to make scores\nlinear_k = nn.Linear(d_embed, d_embed)\nlinear_q = nn.Linear(d_embed, d_embed)\nlinear_v = nn.Linear(d_embed, d_embed)\n# We are going to fold the embedding dimensions and treat each fold as an attention head\nd_k = d_embed // num_heads\n# Pass q, k, v through their linear layers\nq = linear_q(q)\nk = linear_k(k)\nv = linear_v(v)\n# Do the fold, treating each h dimenssions as a head\n# Put the head in the second position\nq = q.view(num_batches, -1, num_heads, d_k).transpose(1, 2)\nk = k.view(num_batches, -1, num_heads, d_k).transpose(1, 2)\nv = v.view(num_batches, -1, num_heads, d_k).transpose(1, 2)\nprint(\"q\", q.size())\nprint(\"x\", k.size())\nprint(\"v\", v.size())\n\nq torch.Size([1, 8, 3, 64])\nx torch.Size([1, 8, 3, 64])\nv torch.Size([1, 8, 3, 64])\n\n\n\nq.transpose(-2, -1).size()\n\ntorch.Size([1, 8, 64, 3])\n\n\n\nd_k = q.size(-1)\n# Compute the raw scores by multiplying k and q (and normalize)\nscores = torch.matmul(k, q.transpose(-2, -1)) / math.sqrt(d_k)\nprint(\"scores\", scores.size())\n# Mask out the scores\nscores = scores.masked_fill(x_mask == 0, -epsilon)\nattn = F.softmax(scores, dim = -1)\nprint(\"attention\", attn.size())\n# Apply the scores to v\nx = torch.matmul(attn, v)\nprint(\"x\", x.size())    \n\nscores torch.Size([1, 8, 3, 3])\nattention torch.Size([1, 8, 3, 3])\nx torch.Size([1, 8, 3, 64])\n\n\n\nF.softmax(scores, dim=-1).size()\n\ntorch.Size([1, 8, 3, 3])\n\n\n\nF.softmax(scores, dim=-1)\n\ntensor([[[[0.2252, 0.3892, 0.3856],\n          [0.4831, 0.2371, 0.2798],\n          [0.3575, 0.3727, 0.2698]],\n\n         [[0.5478, 0.2035, 0.2487],\n          [0.3136, 0.3352, 0.3512],\n          [0.3864, 0.3723, 0.2413]],\n\n         [[0.3558, 0.3108, 0.3334],\n          [0.3114, 0.3117, 0.3768],\n          [0.4919, 0.3081, 0.2000]],\n\n         [[0.2825, 0.3268, 0.3907],\n          [0.2674, 0.4068, 0.3257],\n          [0.4279, 0.3212, 0.2510]],\n\n         [[0.2240, 0.4012, 0.3748],\n          [0.5012, 0.2689, 0.2299],\n          [0.3869, 0.3520, 0.2611]],\n\n         [[0.2569, 0.3529, 0.3902],\n          [0.2282, 0.2568, 0.5150],\n          [0.2551, 0.3774, 0.3676]],\n\n         [[0.4467, 0.2480, 0.3053],\n          [0.3810, 0.3086, 0.3104],\n          [0.3773, 0.3159, 0.3068]],\n\n         [[0.2699, 0.4120, 0.3181],\n          [0.3626, 0.3377, 0.2997],\n          [0.2675, 0.3964, 0.3361]]]], grad_fn=<SoftmaxBackward0>)"
  },
  {
    "objectID": "transformers/transformers_new.html",
    "href": "transformers/transformers_new.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Self-attention is permutation-invariant. It is an operation on sets.\n\n\n\n\nSinusoidal positional encoding\nLearned positional encoding\nRelative position encoding\nRotary position encoding\n\n\n\n\nLilian Weng Blog post"
  },
  {
    "objectID": "transformers/data_eff_img_transformer.html",
    "href": "transformers/data_eff_img_transformer.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "This uses a teacher-student strategy specific to transformers. It relies on a distillation token ensuring that the student learns from the teacher through attention.\n\n\nResearch paper"
  },
  {
    "objectID": "transformers/vision_transformer_architectures.html",
    "href": "transformers/vision_transformer_architectures.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "pyramid vision transformer\n\n\n\nPyramid vision transformer\n\n\nTo overcome the quadratic complexity of the attention mechanism, Pyramid Vision Transformers (PVTs) employed a variant of self-attention called Spatial-Reduction Attention (SRA), characterized by a spatial reduction of both keys and values. By applying SRA, the spatial dimensions of the features slowly decrease throughout the model.\n\n\nSWIN Transformer"
  },
  {
    "objectID": "transformers/transformers_from_scratch.html",
    "href": "transformers/transformers_from_scratch.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "The trick to use one-hot vectors to pull out a particular row of a matrix is at the core of how transformers work.\n\n\n\nMatrix multiplication with one-hot vectors\n\n\n\n\n\n\n\n\nMarkov model\n\n\nMarkov chains can be expressed conveniently in matrix form.\n\n\n\nMarkov chain represented as a matrix form\n\n\nUsing one-hot vector to pull out the relevant row and shows the probability distribution of what the next word will be.\n\n\n\nUsing one-hot vector ro pull out the transition probabilities associated with given word\n\n\n\n\n\nThe mask has the effect of hiding a lot of the transition matrix (which is not relevant for the word combinations)\n\n\n\nMasking\n\n\n\n\n\nMasking hiding transition matrix\n\n\n\n\n\n\n\n\n\nAttention\n\n\nThe highlighted piece in the above formula Q represents the feature of interest nad the matrix K represents the collection of masks (which words are important for the given query of interest)\n\n\n\nMatrix multiplication of query with masks\n\n\n\n\n\nThe following happens in Feed Forward Network * Feature creation matrix multiplication * Transition matrix multiplication * ReLU nonlinearity\n\n\n\nFFN\n\n\n\n\n\nThe area in the architecture the above operations happen\n\n\n\n\nEmbedding can be learned during training\n\n\n\nInput Embedding\n\n\n\n\n\n\n\n\nWorking of positional Embedding\n\n\n\n\n\n\n\n\nConverting the embedding to original vocabulary\n\n\nTo get the softmax of the value x in a vector, divide the exponential of x, e^x, by the sum of the exponentials of all the values in the vector.\n\n\n\nDeembedding\n\n\n\n\n\n\n\n\nMatrix Dimensions\n\n\n\n\n\nMatrix Dimensions for Multihead attention\n\n\n\n\n\nScaled Dot-product attention\n\n\n\n\n\n\nThey help keep the gradient smooth\nThe second purpose is specific to Transformers - Preserving the original input sequence. Even with a lot of attention heads, there’s no guarantee that a word will attend to its own position. It’s possible for the attention filter to forget entirely about the most recent word in favor of watching all of the earlier words that might be relevant. A skip connection takes the original word and manually adds it back into the signal, so that there’s no way it can be dropped or forgotten.\n\n\n\n\nskip connections\n\n\n\n\n\nThe values of the matrix are shifted to have a mean of zero and scaled to have standard deviation of one\n\n\n\nCross-attention works just like self-attention with the exception that the key matrix K and value matrix V are based on the output of the final encoder layer, rather than the output of the previous decoder layer. The query matrix Q is still calculated from the results of the previous decoder layer. This is the channel by which information from the source sequence makes its way into the target sequence and steers its creation in the right direction. It’s interesting to note that the same embedded source sequence is provided to every layer of the decoder, supporting the notion that successive layers provide redundancy and are all cooperating to perform the same task.\n\n\n\ncross attention\n\n\n\n\n\n\nBrandon Rohrer blog post"
  },
  {
    "objectID": "transformers/attention.html",
    "href": "transformers/attention.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Attention\nThe context vector was a bottleneck for the RNN models. It made it challenging for models to process long sentences. In RNN models a single hidden state was passed between the encoder and the decoder.\nIn attention models the encoder passes a lot more data to the decoder.Instead of passing the last hidden state of the encoding stage, the encoder passes all the hidden states to the decoder.\nThe decoder will look at the set of hidden states it received. It will give each hidden state a score. Multiply each hidden state by its softmaxed score. This scoring is done at each time step on the decoder side.\nThis is how the deocder works:-\n\nThe attention decoder RNN takes in the embedding of the  token, and an initial decoder hidden state.\nThe RNN processes its inputs, producing an output and a new hidden state vector (h4). The output is discarded. 3.Attention Step: We use the encoder hidden states and the h4 vector to calculate a context vector (C4) for this time step.\nWe concatenate h4 and C4 into one vector.\nWe pass this vector through a feedforward neural network (one trained jointly with the model).\nThe output of the feedforward neural networks indicates the output word of this time step.\nRepeat for the next time steps\n\n\nReference\nJay Alammar Blog post"
  },
  {
    "objectID": "transformers/vision_transformer.html",
    "href": "transformers/vision_transformer.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Transformers cannot process grid-structured data. It needs sequences. We need to convert non-sequential signal to a sequence.\n\n\n\nSplit an image into patches\nFlatten the patches\nProduce lower-dimensional linear embeddings from the flattened patches\nAdd positional embeddings\nFeed the sequence as an input to standard transformer encoder\nPretrain the model with image labels\nFinetune on the downstream dataset for image classification\n\nThe architecure is the same as the original Attention is all you need paper. The number of blocks is changed.\n\n\n\nProposed Architecture for Vision Transformers\n\n\nThere is no decoder in the architecture.\n\n\n[CLS] embedding begins as a “blank slate” for each sentence in BERT. The final output from [CLS] embedding is used as the input into a classification head during pretraining.Using a “blank slate” token as the sole input to a classification head pushes the transformer to learn to encode a “general representation” of the entire sentence into that embedding. ViT applies the same logic by adding a learnable embedding.\n\n\n\nLearnable Embedding\n\n\n\n\n\nSpecifically, if ViT is trained on datasets with more than 14M (at least :P) images it can approach or beat state-of-the-art CNNs.\nViT is pretrained on the large dataset and then fine-tuned to small ones. The only modification is to discard the prediction head (MLP head) and attach a new D KD×K linear layer, where K is the number of classes of the small dataset.\nEven though many positional embedding schemes were applied, no significant difference was found. Hence, after the low-dimensional linear projection, a trainable position embedding is added to the patch representations.\nDeep learning is all about scale. Indeed, scale is a key component in pushing the state-of-the-art. In this study7 by Zhai et al. from Google Brain Research, the authors train a slightly modified ViT model with 2 billion parameters, which attains 90.45% top-1 accuracy on ImageNet7. The generalization of this over-parametrized beast is tested on few-shot learning: it reaches 84.86% top-1 accuracy on ImageNet with only 10 examples per class.\n\n\n\n\nViTs are robust against data corruptions, image occlusions and adversarial attacks\n\n\n\n\nNeed More data In CNNs the model knows how to focus, we tell them on how much to focus. In case of transformers, the model does not know how to focus. It pays attention to all the patches and learns where to focus during the training process. Due to this they need huge amounts of data.\nOverfitting to small datasets Due to their flexibility they are notorious for overfitting on small datasets. A lot of data augmentation would be required to make it work on small datasets.\n\n\n\n\nAI Summer blog Pinecone blog"
  },
  {
    "objectID": "transformers/how_to_train_ViT.html",
    "href": "transformers/how_to_train_ViT.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Without the translational equivariance of CNNs, ViT models are generally found to perform best in settings with large amounts of training data or to require strong AugReg schemes to avoid overfitting\nCarefully selected regularization and augmentations roughly correspond to a 10x increase in training data size\n\n\n\n\nRegularization used - Dropout to intermediate activations of ViT, stochastic depth regularization\nData augmentations - Mixup, RandAugment\nWeight decay\n\n\n\n\n\nFor most practical purposes, transferring a pre-trained model is both more cost-efficient and leads to better results\n\n\n\n\nOne approach is to run downstream adaptation for all available pre-trained models and then select the best performing model, based on validation score on the downstream task. (expensive)\nSelect a single pre-trained model based on the upstream validation accuracy and then only use this model for adaptation (cheaper)\nCheaper strategy works equally well as the more expensive strategy in the majority of scenarios\n\n\n\n\n\nResearch Paper"
  },
  {
    "objectID": "transformers/code_transformers.html",
    "href": "transformers/code_transformers.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "This notebook is to practice transformer code implementation. The reference for this Mark Riedl Github repo. This notebook is a replication for practice purpose.\n\n\n\nTransformer Architecture\n\n\n\n\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n\n\n\nd_embed = 512\nnum_heads = 8\nnum_batches = 1\nvocab = 50_000\nmax_len = 5000\nn_layers = 1\nd_ff = 2048\nepsilon = 1e-6\n\n\n\n\n\nx = torch.tensor([[1,2,3]]) # Input is size batch_size x sequence_length\ny = torch.tensor([[1,2,3]]) \nx_mask = torch.tensor([[1,0,1]])\ny_mask = torch.tensor([[1,0,1]])\nprint(\"x\",x.size())\nprint(\"y\",y.size())\n\nx torch.Size([1, 3])\ny torch.Size([1, 3])\n\n\n\n\n\n\n\n\nemb = nn.Embedding(vocab, d_embed)\n# We are extracting the embeddings for the tokens from the vocabulary\n# The dimensions after this operation will be batch_size x sequence_length x d_embed\nx = emb(x) \n# scale the embedding by sqrt(d_model) to make them bigger\nx = x * math.sqrt(d_embed)\nprint(x.size())\n\ntorch.Size([1, 3, 512])\n\n\n\n\n\n# start with empty tensor\npe = torch.zeros(max_len, d_embed, requires_grad=False)\n# array containing index values 0 to max_len\nposition = torch.arange(0,max_len).unsqueeze(1)\ndivisor = torch.exp(torch.arange(0,d_embed,2)) * -(math.log(10000.0)/d_embed)\n# Make overlapping sine and cosine wave inside positional embedding tensor\npe[:,0::2] = torch.sin(position * divisor)\npe[:,1::2] = torch.cos(position * divisor)\npe = pe.unsqueeze(0)\n# Add the positional embedding to the main embedding\nx = x + pe[:,:x.size(1)]\nprint(x.size())\n\ntorch.Size([1, 3, 512])\n\n\n\n\n\n\n\n\n\nx_residual = x.clone()\nprint(x.size())\n\ntorch.Size([1, 3, 512])\n\n\n\n\n\n\n# Centering all the values relative to mean\n# W and b are hyperparameters which needs tuning\nmean = x.mean(-1,keepdim=True)\nstd = x.std(-1,keepdim=True)\nW1 = nn.Parameter(torch.ones(d_embed))\nb1 = nn.Parameter(torch.zeros(d_embed))\nx = W1 * (x - mean) / (std + epsilon) + b1\nprint(x.size())\n\ntorch.Size([1, 3, 512])\n\n\n\n\n\nSelf-attention is a process of generating scores that indicate how each token is to every other token. So we would expect a seq_length x seg_length matrix of values between 0 and 1, each indicating the importance of the i-th token to the j-th token.\nThe input to self-attention is batch_size x sequence_length x embedding_size matrix.\nSelf-attention copies the input x , three tiles and calls them query(q), key(k) and values(v). Each of these matrices go through a linear layer. The marix learns to make scores in the linear layersa. It makes each matrix different. If the networks comes up with the right, different, matrices, it will get good attention scores.\nWe designate chunks of each token embedding to different heads.\nThe q and k tensors are multiplied together. This creates a batch_size x num_heads x sequence_length x sequence_length matrix. Ignoring batching and heads, one can interpret this matrix as containing the raw scores where each cell computes how related the i-th token is to the j-th token (i is the row and j is the column).\nNext we pass this matrix through a softmax layer. The secret to softmax is that it can act like an argmax—it can pick the best match. Softmax squishes all values along a particular dimenion into 0…1. But what it is really doing is trying to force one particular cell to have a number close to 1 and all the rest close to 0. If we multiply this softmaxed score matrix to the v matrix, we are in essence asking (for each head), which column is best for each row. Recall that rows and columns correspond to tokens. So we are asking, which token goes best with every other token. Again, if the earlier linear layers get their parameters right, this multiplication will make good choices and loss will improve.\nAt this point we can think of the softmaxed scores multiplied against v as tryinng to zero out everything but the most relevant token embedding (several because of multiple heads). The result, which we will store back in x for consistency is mainly the most-attended token embedding (several because of multiple heads) plus a little bit of every other embedded token sprinkled in because we can’t do an actual argmax—the best we can do is get everything irrelevant to be close to zero so it doesn’t impact anything else.\nThis multiplication of the scores against the v matrix is what we refer to as self-attention. It is essentially a dot-product with an underlying learned scoring function. It basically tells us where we should look for good information. The Decoder will use this later.\n\n# Make three versions of x for the query, key and values\nk = x\nq = x\nv = x\n# Make three linear layers\n# This is where the network learns to make scores\nlinear_k = nn.Linear(d_embed, d_embed)\nlinear_q = nn.Linear(d_embed, d_embed)\nlinear_v = nn.Linear(d_embed, d_embed)\n# We are going to fold the embedding dimensions and treat each fold as an attention head\nd_k = d_embed // num_heads\n# Pass q, k, v through their linear layers\nq = linear_q(q)\nk = linear_k(k)\nv = linear_v(v)\n# Do the fold, treating each h dimensions as a head\n# Put the head in the second position\nq = q.view(num_batches, -1, num_heads, d_k).transpose(1,2)\nk = k.view(num_batches, -1, num_heads, d_k).transpose(1,2)\nv = v.view(num_batches, -1, num_heads, d_k).transpose(1,2)\nprint(\"q\",q.size())\nprint(\"k\",k.size())\nprint(\"v\",v.size())\n\nq torch.Size([1, 8, 3, 64])\nk torch.Size([1, 8, 3, 64])\nv torch.Size([1, 8, 3, 64])\n\n\nTo produce the attention scores we multiply q and k (and normalize). We need to apply the mask so masked tokens don’t attend to themselves. Apply softmax to emulate argmax (good stuff close to 1 irrelevant stuff close to 0). You won’t see this happen if you look at attn because the linear layers aren’t trained yet. The attention scores are finally applied to v.\n\nd_k = q.size(-1)\n# compute the scores by multiplying k and q (and normalize)\nscores = torch.matmul(k,q.transpose(-2,-1)) / math.sqrt(d_k)\n# Mask out the scores\nscores = scores.masked_fill(x_mask == 0, -epsilon)\n# Softmax the scores, ideally creating one score close to 1 and the rest close to 0 \n# (Note: this won't happen if you look at the numbers because the linear layers haven't \n# learned anything yet.)\nattn = F.softmax(scores,dim = -1)\nprint(\"attention\",attn.size())\n# Apply the scores to v\nx = torch.matmul(attn,v)\nprint(\"x\",x.size())\n\nattention torch.Size([1, 8, 3, 3])\nx torch.Size([1, 8, 3, 64])\n\n\n\n# Recombine the multiple attention heads (unfold)\nx = x.transpose(1,2).contiguous().view(num_batches, -1, num_heads * (d_embed // num_heads))\nprint(\"x\",x.size())\n\nx torch.Size([1, 3, 512])\n\n\n\n\n\n\nff = nn.Linear(d_embed, d_embed)\nx = ff(x)\nprint(x.size())\n\ntorch.Size([1, 3, 512])\n\n\n\n\n\n\n# Adding the residual - This is changing the original embedding values for each token by some delta up or down\nx = x_residual + x\nprint(x.size())\n\ntorch.Size([1, 3, 512])\n\n\n\n\n\nThe output of this layer is a stack of hidden states, one for each token. The decoder will be able to look back and attend to the hidden state that will be most useful for decoding by looking just at this stack. To move the matrix toward a hidden state we expand the embeddings, giving the network some capacity, and then collapse it down again to force it to make trade-offs.\n\n\n\nx_residual = x.clone()\nprint(x.size())\n\ntorch.Size([1, 3, 512])\n\n\n\n\n\n\nmean = x.mean(-1,keepdim=True)\nstd = x.std(-1,keepdim=True)\nW2 = nn.Parameter(torch.ones(d_embed))\nb2 = nn.Parameter(torch.zeros(d_embed))\nx = W2 * (x - mean) / (std + epsilon) + b2\nprint(x.size())\n\ntorch.Size([1, 3, 512])\n\n\n\n\n\n\n# The embeddings is grown and compressed again.  This is part of process of transforming the outputs of the self-attention module into a hidden state encoding.\nlinear_expand = nn.Linear(d_embed, d_ff)\nlinear_compress = nn.Linear(d_ff, d_embed)\nx = linear_compress(F.relu(linear_expand(x)))\nprint(x.size())\n\ntorch.Size([1, 3, 512])\n\n\n\n##### 1.1.2.4 Add residual block back\nx = x_residual + x\nprint(x.size())\n\ntorch.Size([1, 3, 512])\n\n\n\n\n\n\n\n\n# After repeating the self-attention and feed forward sub-layers for N times, we apply one last layer normalization\nmean = x.mean(-1, keepdim=True)\nstd = x.std(-1, keepdim=True)\nWn = nn.Parameter(torch.ones(d_embed))\nbn = nn.Parameter(torch.zeros(d_embed))\nx = Wn * (x - mean) / (std + epsilon) + bn\nprint(x.size())\n\ntorch.Size([1, 3, 512])\n\n\nAt this point, we should have a matrix, stored in x that we can interpret as a stack of hidden states. The Decoder will attempt to attend to this stack and pick out (via softmax emulating argmax) the hidden state that is most helpful in guessing the work that goes in the masked position.\n\n# The output is the hidden state\nhidden = x\nprint(hidden.size())\n\ntorch.Size([1, 3, 512])\n\n\n\n\n\n\n\n\n\nemb_d = nn.Embedding(vocab, d_embed)\ny = emb_d(y) * math.sqrt(d_embed)\nprint(y.size())\n\ntorch.Size([1, 3, 512])\n\n\n\npe = torch.zeros(max_len,d_embed, requires_grad = False)\nposition = torch.arange(0, max_len).unsqueeze(1)\ndivisor = torch.exp(torch.arange(0, d_embed, 2) * -(math.log(10000.0) / d_embed))\npe[:,0::2] = torch.sin(position * divisor)\npe[:,1::2] = torch.cos(position * divisor)\npe = pe.unsqueeze(0)\ny = y + pe[:, :y.size(1)]\nprint(y.size())\n\ntorch.Size([1, 3, 512])\n\n\n\n\n\n\n\n\n\n\ny_residual = y.clone()\nprint(y.size())\n\ntorch.Size([1, 3, 512])\n\n\n\n\n\n\nmean = y.mean(-1, keepdim=True)\nstd = y.std(-1, keepdim=True)\nW1_d = nn.Parameter(torch.ones(d_embed))\nb1_d = nn.Parameter(torch.zeros(d_embed))\ny = W1_d * (y - mean) / (std + epsilon) + b1_d\nprint(y.size())\n\ntorch.Size([1, 3, 512])\n\n\n\n\n\n\nk = y\nq = y\nv = y\nlinear_q_self = nn.Linear(d_embed, d_embed)\nlinear_k_self = nn.Linear(d_embed, d_embed)\nlinear_v_self = nn.Linear(d_embed, d_embed)\nd_k = d_embed // num_heads\nq = linear_q_self(q)\nk = linear_k_self(k)\nv = linear_k_self(v)\nq = q.view(num_batches, -1, num_heads, d_k).transpose(1,2)\nk = k.view(num_batches, -1, num_heads, d_k).transpose(1,2)\nv = v.view(num_batches, -1, num_heads, d_k).transpose(1,2)\nprint(\"q\",q.size())\nprint(\"k\",k.size())\nprint(\"v\",v.size())\n\nq torch.Size([1, 8, 3, 64])\nk torch.Size([1, 8, 3, 64])\nv torch.Size([1, 8, 3, 64])\n\n\n\nd_k = q.size(-1)\nscores = torch.matmul(k,q.transpose(-2,-1)) / math.sqrt(d_k)\nscores = scores.masked_fill(y_mask == 0, -epsilon)\nattn = F.softmax(scores, dim=-1)\nprint(\"attention\",attn.size())\ny = torch.matmul(attn, v)\nprint(\"y\",y.size())\n\nattention torch.Size([1, 8, 3, 3])\ny torch.Size([1, 8, 3, 64])\n\n\n\n# Assemble heads\ny = y.transpose(1,2).contiguous().view(num_batches,-1,num_heads * (d_embed // num_heads))\nprint(y.size())\n\ntorch.Size([1, 3, 512])\n\n\n\n\n\n\nff_d1 = nn.Linear(d_embed, d_embed)\ny = ff_d1(y)\nprint(y.size())\n\ntorch.Size([1, 3, 512])\n\n\n\n##### 2.2.1.5 Add Residual back \ny = y_residual + y\nprint(y.size())\n\ntorch.Size([1, 3, 512])\n\n\n\n\n\n\n\n\n\n\ny_residual = y.clone()\nprint(y.size())\n\ntorch.Size([1, 3, 512])\n\n\n\n\n\n\nmean = y.mean(-1, keepdim = True)\nstd = y.std(-1, keepdim = True)\nW2_d = nn.Parameter(torch.ones(d_embed))\nb2_d = nn.Parameter(torch.ones(d_embed))\ny = W2_d * (y - mean) / (std + epsilon) + b2_d\nprint(y.size())\n\ntorch.Size([1, 3, 512])\n\n\n\n\n\nSource attention works just like self-attention, except we compute the scores using keys and values from the encoder and apply it to the query from the decoder. That is, based on what the encoder thinks we should attend to, what part of the decoder sequence should we actually attend to.\n\nq = y\nk = x # we are using x\nv = x # we are using x\nlinear_q_source = nn.Linear(d_embed,d_embed)\nlinear_k_source = nn.Linear(d_embed,d_embed)\nlinear_v_source = nn.Linear(d_embed,d_embed)\nd_k = d_embed // num_heads\nq = linear_q(q)\nk = linear_k(k)\nv = linear_v(v)\nq = q.view(num_batches, -1, num_heads, d_k).transpose(1,2)\nv = v.view(num_batches, -1, num_heads, d_k).transpose(1,2)\nk = k.view(num_batches, -1, num_heads, d_k).transpose(1,2)\nprint(\"q\",q.size())\nprint(\"k\",k.size())\nprint(\"v\",v.size())\n\nq torch.Size([1, 8, 3, 64])\nk torch.Size([1, 8, 3, 64])\nv torch.Size([1, 8, 3, 64])\n\n\n\nd_k = q.size(-1)\nscores = torch.matmul(k,q.transpose(-2,-1)) / math.sqrt(d_k)\nattn = F.softmax(scores, dim=-1)\ny = torch.matmul(attn,v)\nprint(\"y\",y.size())\n\ny torch.Size([1, 8, 3, 64])\n\n\n\n# Assemble heads\ny = y.transpose(1,2).contiguous().view(num_batches, -1, num_heads * (d_embed // num_heads))\nprint(y.size())\n\ntorch.Size([1, 3, 512])\n\n\n\n\n\n\nff_d2 = nn.Linear(d_embed,d_embed)\ny = ff_d2(y)\nprint(y.size())\n\ntorch.Size([1, 3, 512])\n\n\n\n\n\ny = y_residual + y\nprint(y.size())\n\ntorch.Size([1, 3, 512])\n\n\n\n\n\n\n\n\n\n\ny_residual = y.clone()\nprint(y.size())\n\ntorch.Size([1, 3, 512])\n\n\n\n\n\n\nmean = y.mean(-1, keepdim=True)\nstd = y.std(-1, keepdim=True)\nW3_d = nn.Parameter(torch.ones(d_embed))\nb3_d = nn.Parameter(torch.zeros(d_embed))\ny = W3_d * (y - mean) / (std + epsilon) + b3_d\nprint(y.size())\n\ntorch.Size([1, 3, 512])\n\n\n\n\n\n\nlinear_expand_d = nn.Linear(d_embed, d_ff)\nlinear_compress_d = nn.Linear(d_ff, d_embed)\ny = linear_compress_d(F.relu(linear_expand_d(y)))\nprint(y.size())\n\ntorch.Size([1, 3, 512])\n\n\n\n\n\n\ny = y_residual + y\nprint(y.size())\n\ntorch.Size([1, 3, 512])\n\n\n\n\n\n\n\nmean = y.mean(-1, keepdim=True)\nstd = y.std(-1, keepdim=True)\nWn_d = nn.Parameter(torch.ones(d_embed))\nbn_d = nn.Parameter(torch.zeros(d_embed))\ny = Wn_d * (y - mean) / (std + epsilon) + bn_d\nprint(y.size())\n\ntorch.Size([1, 3, 512])\n\n\n\n\n\n\nThis next module sits on top of the decoder and expands the decoder output into a log probability distribution over the vocabulary for each token position. This is done for all tokens, though the only ones that will matter for loss computation are the ones that are masked.\n\nlinear_scores = nn.Linear(d_embed, vocab)\nprobs = F.log_softmax(linear_scores(y), dim=-1)\nprint(probs.size())\n\ntorch.Size([1, 3, 50000])"
  },
  {
    "objectID": "transformers/object_detection_transformers.html",
    "href": "transformers/object_detection_transformers.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Object Detection with DETR\n\nNon-maximum supression and anchor generation are not required\nThey don’t require customized layers\n\n\n\n\nDETR\n\n\n\nDETR predicts all objects at once and is trained end-to-end with a set loss function which performs bipartite matching between predicted and ground-truth objects.\nPerforms better on larger objects\nA segmentation head trained on top of a pre-trained DETR works on Panoptic segmentation\nNo autogression in decoder\nUse transformers with parallel decoding\nDirectly predicting the set of detections with absolute box prediction\n\n\n\n\nDETR Architecture"
  },
  {
    "objectID": "transformers/gaps.html",
    "href": "transformers/gaps.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Considerations and pitfalls of attention for your own data Advantages and disadvantages of ViT for Images ViT Vs CNN Classification on Videos Validation for video classification other classification metrics Scene change detection in videos Continual learning - Update model Closing the presentation with summary"
  },
  {
    "objectID": "transformers/transformers.html",
    "href": "transformers/transformers.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "The Transformer is a model which uses attention to boost the speed of the training. The transformer lends itself to parallelization.\nThe transformers will consist of encoders and decoders.\nAn encoder consists two sub-layers self-attention and feed forward neural network. A decoder along with these two sub-layers will also consist of an attention layer between them to focus on relevant parts of the input sentence.\n\n\n\nComponents of Encoder and Decoder\n\n\n\n\nEmbedding happens in the bottom most encoder. The word in each position flows through its own path in the self-attention layer. The feed-forward layer does not have those dependencies and various paths can be executed in parallel.\n\n\n\nProcessing by an Encoder\n\n\n\n\n\n\n\nself attention\n\n\n\n\n\nSelf attention at a glance\n\n\n\n\n\nSteps for calculating self-attention\n\n\n\n\n\nself attention in matrix form\n\n\n\n\n\nSelf attention in a single formula\n\n\n\n\n\nIt expands the model’s ability to focus on different positions.\nIt gives the attention layer multiple “representation subspaces”\n\n\n\nMulti-headed attention\n\n\nIn the original paper, eight attention heads were used. This way we will end up with eight different Z matrices. The feed-forward layer is not expecting eight matrices – it’s expecting a single matrix (a vector for each word). So we need a way to condense these eight down into a single matrix.\n\n\n\nConcatenating Attention Heads\n\n\n\n\n\nMultiple attention heads focusing on different representations\n\n\n\n\n\nThis will account for the order of the words in the input sequence.\n\n\n\npositional encoding\n\n\n\n\n\nEach sub-layer in each encoder has a residual connection around it, and is follwed by a layer-normalization step\n\n\n\nResidual connection and layer normalization\n\n\n\n\n\n2 layer encoder decoder architecture\n\n\n\n\n\n\nThe encoder start by processing the input sequence. The output of the top encoder is then transformed into a set of attention vectors K and V. These are to be used by each decoder in its “encoder-decoder attention” layer which helps the decoder focus on appropriate places in the input sequence.\n\n\n\nDecoder\n\n\nThe following steps repeat the process until a special symbol is reached indicating the transformer decoder has completed its output. The output of each step is fed to the bottom decoder in the next time step, and the decoders bubble up their decoding results just like the encoders did. And just like we did with the encoder inputs, we embed and add positional encoding to those decoder inputs to indicate the position of each word.\nIn the decoder, the self-attention layer is only allowed to attend to earlier positions in the output sequence. This is done by masking future positions (setting them to -inf) before the softmax step in the self-attention calculation.\nThe “Encoder-Decoder Attention” layer works just like multiheaded self-attention, except it creates its Queries matrix from the layer below it, and takes the Keys and Values matrix from the output of the encoder stack.\n\n\n\nSteps in Decoding\n\n\n\n\n\nThe Linear layer is a simple fully connected neural network that projects the vector produced by the stack of decoders, into a much, much larger vector called a logits vector.\nLet’s assume that our model knows 10,000 unique English words (our model’s “output vocabulary”) that it’s learned from its training dataset. This would make the logits vector 10,000 cells wide – each cell corresponding to the score of a unique word. That is how we interpret the output of the model followed by the Linear layer.\nThe softmax layer then turns those scores into probabilities (all positive, all add up to 1.0). The cell with the highest probability is chosen, and the word associated with it is produced as the output for this time step.\n\n\n\nworking of final and softmax layer\n\n\n\n\n\nThe illustrated Transformer"
  },
  {
    "objectID": "transformers/Attention_for_vision.html",
    "href": "transformers/Attention_for_vision.html",
    "title": "Computer Vision Using Transformers",
    "section": "",
    "text": "Core concepts of Attention is all you need\nAdapting Attention to vision\nUse case Implementation"
  },
  {
    "objectID": "transformers/Attention_for_vision.html#attention-is-all-you-need",
    "href": "transformers/Attention_for_vision.html#attention-is-all-you-need",
    "title": "Computer Vision Using Transformers",
    "section": "Attention is all you need",
    "text": "Attention is all you need"
  },
  {
    "objectID": "transformers/Attention_for_vision.html#why-a-new-architecture-is-required",
    "href": "transformers/Attention_for_vision.html#why-a-new-architecture-is-required",
    "title": "Computer Vision Using Transformers",
    "section": "Why a new architecture is required?",
    "text": "Why a new architecture is required?\n\n\nRNNs process words sequentially\nRNN cannot consider long sequence lengths"
  },
  {
    "objectID": "transformers/Attention_for_vision.html#attention-transformer-architecture",
    "href": "transformers/Attention_for_vision.html#attention-transformer-architecture",
    "title": "Computer Vision Using Transformers",
    "section": "Attention Transformer Architecture",
    "text": "Attention Transformer Architecture"
  },
  {
    "objectID": "transformers/Attention_for_vision.html#blocks-of-the-architecture",
    "href": "transformers/Attention_for_vision.html#blocks-of-the-architecture",
    "title": "Computer Vision Using Transformers",
    "section": "Blocks of the Architecture",
    "text": "Blocks of the Architecture\n\n\nEmbedding layer\n\nReduce the dimension of word tokens\nProjection to latent space\n\nPositional Encoding\n\nTo track the relative position of the words"
  },
  {
    "objectID": "transformers/Attention_for_vision.html#self-attention",
    "href": "transformers/Attention_for_vision.html#self-attention",
    "title": "Computer Vision Using Transformers",
    "section": "Self Attention",
    "text": "Self Attention"
  },
  {
    "objectID": "transformers/Attention_for_vision.html#self-attention-1",
    "href": "transformers/Attention_for_vision.html#self-attention-1",
    "title": "Computer Vision Using Transformers",
    "section": "Self Attention",
    "text": "Self Attention"
  },
  {
    "objectID": "transformers/Attention_for_vision.html#self-attention-2",
    "href": "transformers/Attention_for_vision.html#self-attention-2",
    "title": "Computer Vision Using Transformers",
    "section": "Self Attention",
    "text": "Self Attention"
  },
  {
    "objectID": "transformers/Attention_for_vision.html#why-multi-head-attention",
    "href": "transformers/Attention_for_vision.html#why-multi-head-attention",
    "title": "Computer Vision Using Transformers",
    "section": "Why Multi-Head Attention",
    "text": "Why Multi-Head Attention\n\n\nIt expands the models ability to focus on different positions\nIt gives the attention layer multiple “representation subspaces”"
  },
  {
    "objectID": "transformers/Attention_for_vision.html#importance-of-attention",
    "href": "transformers/Attention_for_vision.html#importance-of-attention",
    "title": "Computer Vision Using Transformers",
    "section": "Importance of Attention",
    "text": "Importance of Attention\n\n\nEncoder providing a context to the decoder query by providing keys and values\nEach position in the encoder can attend to all positions in the previous layer of encoder\nEach position in decoder attending to all positions in the decoder"
  },
  {
    "objectID": "transformers/Attention_for_vision.html#skip-connections",
    "href": "transformers/Attention_for_vision.html#skip-connections",
    "title": "Computer Vision Using Transformers",
    "section": "Skip connections",
    "text": "Skip connections\n\n\nSkip connection help a word to pay attention to its own position\nKeep the gradients smooth"
  },
  {
    "objectID": "transformers/Attention_for_vision.html#steps-in-decoder",
    "href": "transformers/Attention_for_vision.html#steps-in-decoder",
    "title": "Computer Vision Using Transformers",
    "section": "Steps in Decoder",
    "text": "Steps in Decoder"
  },
  {
    "objectID": "transformers/Attention_for_vision.html#adapting-attention-to-vision",
    "href": "transformers/Attention_for_vision.html#adapting-attention-to-vision",
    "title": "Computer Vision Using Transformers",
    "section": "Adapting Attention to Vision",
    "text": "Adapting Attention to Vision"
  },
  {
    "objectID": "transformers/Attention_for_vision.html#vision-transformer",
    "href": "transformers/Attention_for_vision.html#vision-transformer",
    "title": "Computer Vision Using Transformers",
    "section": "Vision Transformer",
    "text": "Vision Transformer\n\n\n\nViT Architecture"
  },
  {
    "objectID": "transformers/Attention_for_vision.html#vit-vs-cnn",
    "href": "transformers/Attention_for_vision.html#vit-vs-cnn",
    "title": "Computer Vision Using Transformers",
    "section": "ViT Vs CNN",
    "text": "ViT Vs CNN\n\n\nInductive Bias and Locality Vs Global"
  },
  {
    "objectID": "transformers/Attention_for_vision.html#vit-vs-cnn-1",
    "href": "transformers/Attention_for_vision.html#vit-vs-cnn-1",
    "title": "Computer Vision Using Transformers",
    "section": "ViT Vs CNN",
    "text": "ViT Vs CNN\n\n\nInductive Bias and Locality Vs Global\nFlexibility\nCNN works with less amount of data than ViT\nSpecifically, if ViT is trained on datasets with more than 14M (at least) images it can approach or beat state-of-the-art CNNs."
  },
  {
    "objectID": "transformers/Attention_for_vision.html#vit-vs-cnn-2",
    "href": "transformers/Attention_for_vision.html#vit-vs-cnn-2",
    "title": "Computer Vision Using Transformers",
    "section": "ViT Vs CNN",
    "text": "ViT Vs CNN\n\n\nTransformer models are more memory efficient than ResNet models\nViT are prone to over-fitting due to their flexibility\nTransformers can learn meaningful information even in the lowest layers"
  },
  {
    "objectID": "transformers/Attention_for_vision.html#vit-vs-cnn-3",
    "href": "transformers/Attention_for_vision.html#vit-vs-cnn-3",
    "title": "Computer Vision Using Transformers",
    "section": "ViT Vs CNN",
    "text": "ViT Vs CNN\n\n\nViT reaches 84.86% top-1 accuracy on ImageNet with only 10 examples per class.\nViT is suitable for Transfer learning\nViTs are robust against data corruptions, image occlusions and adversarial attacks"
  },
  {
    "objectID": "transformers/Attention_for_vision.html#hybrid-architectures",
    "href": "transformers/Attention_for_vision.html#hybrid-architectures",
    "title": "Computer Vision Using Transformers",
    "section": "Hybrid Architectures",
    "text": "Hybrid Architectures\n\n\nCNN is used to extract features\nThe extracted features are used by the transformers"
  },
  {
    "objectID": "transformers/Attention_for_vision.html#use-case-discussion",
    "href": "transformers/Attention_for_vision.html#use-case-discussion",
    "title": "Computer Vision Using Transformers",
    "section": "Use Case Discussion",
    "text": "Use Case Discussion"
  },
  {
    "objectID": "transformers/Attention_for_vision.html#problem-statement",
    "href": "transformers/Attention_for_vision.html#problem-statement",
    "title": "Computer Vision Using Transformers",
    "section": "Problem Statement",
    "text": "Problem Statement\n\n\nMonitoring when the children are in danger of leaving the front yard\nPredict when childrean are about to leave the yard to trigger the alarm\nWe have videos of children playing sports"
  },
  {
    "objectID": "transformers/Attention_for_vision.html#considerations",
    "href": "transformers/Attention_for_vision.html#considerations",
    "title": "Computer Vision Using Transformers",
    "section": "Considerations",
    "text": "Considerations\n\n\nCollect and train the model on low quality images\nAmount of data available\nTraining time available\nImportance of Interpretability\nDeployment requirements\n\nlatency\nModel size\nInference cost"
  },
  {
    "objectID": "transformers/Attention_for_vision.html#approach",
    "href": "transformers/Attention_for_vision.html#approach",
    "title": "Computer Vision Using Transformers",
    "section": "Approach",
    "text": "Approach\n\n\nObject Detection\nTrain a ML model on the object detection output"
  },
  {
    "objectID": "transformers/Attention_for_vision.html#object-detection-with-detr",
    "href": "transformers/Attention_for_vision.html#object-detection-with-detr",
    "title": "Computer Vision Using Transformers",
    "section": "Object Detection with DETR",
    "text": "Object Detection with DETR\n\nDEtection TRansformer(DETR)"
  },
  {
    "objectID": "transformers/Attention_for_vision.html#object-detection-with-detr-1",
    "href": "transformers/Attention_for_vision.html#object-detection-with-detr-1",
    "title": "Computer Vision Using Transformers",
    "section": "Object Detection with DETR",
    "text": "Object Detection with DETR"
  },
  {
    "objectID": "transformers/Attention_for_vision.html#object-detection-with-detr-2",
    "href": "transformers/Attention_for_vision.html#object-detection-with-detr-2",
    "title": "Computer Vision Using Transformers",
    "section": "Object Detection with DETR",
    "text": "Object Detection with DETR"
  },
  {
    "objectID": "transformers/Attention_for_vision.html#object-detection-with-detr-3",
    "href": "transformers/Attention_for_vision.html#object-detection-with-detr-3",
    "title": "Computer Vision Using Transformers",
    "section": "Object Detection with DETR",
    "text": "Object Detection with DETR"
  },
  {
    "objectID": "transformers/Attention_for_vision.html#why-detr",
    "href": "transformers/Attention_for_vision.html#why-detr",
    "title": "Computer Vision Using Transformers",
    "section": "Why DETR",
    "text": "Why DETR\n\n\nHand-crafted anchors not required\nThey don’t require customized layers\nPredict all objects at once\nPost-processing not required for predicting bounding boxes\nAttention maps can be used for Interpretation"
  },
  {
    "objectID": "transformers/Attention_for_vision.html#why-detr-1",
    "href": "transformers/Attention_for_vision.html#why-detr-1",
    "title": "Computer Vision Using Transformers",
    "section": "Why DETR",
    "text": "Why DETR"
  },
  {
    "objectID": "transformers/Attention_for_vision.html#why-detr-2",
    "href": "transformers/Attention_for_vision.html#why-detr-2",
    "title": "Computer Vision Using Transformers",
    "section": "Why DETR",
    "text": "Why DETR\n\n\nWhat if object detection model doesn’t work?\nA segmentation head can be trained on top of a pre-trained DETR"
  },
  {
    "objectID": "transformers/Attention_for_vision.html#why-detr-3",
    "href": "transformers/Attention_for_vision.html#why-detr-3",
    "title": "Computer Vision Using Transformers",
    "section": "Why DETR",
    "text": "Why DETR"
  },
  {
    "objectID": "transformers/Attention_for_vision.html#why-detr-4",
    "href": "transformers/Attention_for_vision.html#why-detr-4",
    "title": "Computer Vision Using Transformers",
    "section": "Why DETR",
    "text": "Why DETR"
  },
  {
    "objectID": "transformers/Attention_for_vision.html#why-detr-5",
    "href": "transformers/Attention_for_vision.html#why-detr-5",
    "title": "Computer Vision Using Transformers",
    "section": "Why DETR",
    "text": "Why DETR\n\n\nWe can get the FPS for processing videos\nPre-trained Pytorch models and code available"
  },
  {
    "objectID": "transformers/Attention_for_vision.html#data-collection",
    "href": "transformers/Attention_for_vision.html#data-collection",
    "title": "Computer Vision Using Transformers",
    "section": "Data Collection",
    "text": "Data Collection\n\n\nBrainstorming how data needs to be annotated\nStandardizing the definitions\nCollecting Diversified data - Different yards, balls, walls, seasons etc\nLabeling the data - Quality Vs Quantity\nDiscussing ambigious cases with labelers and keep improving"
  },
  {
    "objectID": "transformers/Attention_for_vision.html#modelling",
    "href": "transformers/Attention_for_vision.html#modelling",
    "title": "Computer Vision Using Transformers",
    "section": "Modelling",
    "text": "Modelling\n\nUsing a pre-trained model is both more cost-efficient and leads to better results"
  },
  {
    "objectID": "transformers/Attention_for_vision.html#how-to-select-a-pre-trained-model",
    "href": "transformers/Attention_for_vision.html#how-to-select-a-pre-trained-model",
    "title": "Computer Vision Using Transformers",
    "section": "How to select a pre-trained model",
    "text": "How to select a pre-trained model\n\n\nSpot check all the available pre-trained models (expensive)\nSelect a single pre-trained model based on\n\nAmount of data used for training\nVaried upstream data\nBest upstream validation performance\n\n\nCheaper strategy works equally well as the more expensive strategy in the majority of scenarios\nHow to train your ViT - Research paper"
  },
  {
    "objectID": "transformers/Attention_for_vision.html#validation",
    "href": "transformers/Attention_for_vision.html#validation",
    "title": "Computer Vision Using Transformers",
    "section": "Validation",
    "text": "Validation\n\n\nFalse alarms are better than not raising alarm when necessary\nRecall is more important in this case\nToo many false alarms will reduce the customer satisfaction\nImprove Recall while maintaining Precision at an acceptable level"
  },
  {
    "objectID": "transformers/Attention_for_vision.html#validation-1",
    "href": "transformers/Attention_for_vision.html#validation-1",
    "title": "Computer Vision Using Transformers",
    "section": "Validation",
    "text": "Validation\n\n\nFPS\nmAP for object detection\nRecall, Precision and F1 for the classification"
  },
  {
    "objectID": "transformers/Attention_for_vision.html#deployment",
    "href": "transformers/Attention_for_vision.html#deployment",
    "title": "Computer Vision Using Transformers",
    "section": "Deployment",
    "text": "Deployment\n\n\nFrame sampling instead of predicting on all frames?\nDeployment using platforms like Ray for effective GPU utilization\nDeployment at Edge to meet latency requirements - TensorRT\nUse AB testing framework to deploy new models"
  },
  {
    "objectID": "transformers/Attention_for_vision.html#continual-learning",
    "href": "transformers/Attention_for_vision.html#continual-learning",
    "title": "Computer Vision Using Transformers",
    "section": "Continual Learning",
    "text": "Continual Learning\n\n\nContinual learning suits deep learning models\nIncentivize customers to label the data in real time\nUpdate the model parameters in real time\nContinously monitor the model performance"
  },
  {
    "objectID": "transformers/Attention_for_vision.html#tool-suggestion-for-monitoring",
    "href": "transformers/Attention_for_vision.html#tool-suggestion-for-monitoring",
    "title": "Computer Vision Using Transformers",
    "section": "Tool Suggestion for Monitoring",
    "text": "Tool Suggestion for Monitoring\n\n\nFiftyone"
  },
  {
    "objectID": "transformers/Attention_for_vision.html#summary",
    "href": "transformers/Attention_for_vision.html#summary",
    "title": "Computer Vision Using Transformers",
    "section": "Summary",
    "text": "Summary\n\n\nStart simple\nSmall improvements on regular basis\nLookout for new discovery in the field\nGet Feedback, Iterate, Improve\nKeep the cycle going"
  },
  {
    "objectID": "transformers/attention_is_all_you_need.html",
    "href": "transformers/attention_is_all_you_need.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Sequential nature of RNN precludes parallelization within training examples which becomes critical at longer sequence lengths.\nTransformer allows for significantly more parallelization\n\n\n\n\nTransformer Architecture\n\n\n\n\n\nEncoder * stack of 6 identical layers * To faciliate residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension 512.\nDecoder * Stack of 6 indentical layers\n\n\n\n\n8 attention heads are used\n\n\n\n\nAttention and Multi-head Attention\n\n\n\nFor large values of dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients.To counteract this effect scaling of dot product is done.\n\nMulti-head attention allows the model to jointly attend to information from different representation subspaces at different positions.\n\n\n\n\nIn “encoder-decoder attention” layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence.\nEach position in the encoder can attend to all positions in the previous layer of the encoder.\nSimilarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position.\n\n\n\n\n\nThe input and output dimensions are 512\nThe inner layer has dimensionality of 2048\n\n\n\n\nPositional Embeddings\n\n\nSelf attention could yield more interpretable models"
  },
  {
    "objectID": "diffusion_models/Introduction.html",
    "href": "diffusion_models/Introduction.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Unet - Input is Somewhat a noisy image, and it output the noise  \n\n\n\n\n \n\n\n\n\nPaperspace Gradient\nLambda labs\nJarvis labs\n\n\n\n\nJeremy Howard video"
  },
  {
    "objectID": "probability/Probability.html",
    "href": "probability/Probability.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Probability\n\nprobability = preferred events / All events\nUnion is similar to ‘OR’ condition\nIntersection is similar to ‘AND’ condition\nProbability of getting event A or B is\n\nP(A U B) = P(A) + P(B) - P(A and B)\n\nWe are subtracting P (A and B) because some elements will be common to both A and B and we will be double counting them. So we subtract P (A and B) to eleminate the double count\n\nWe can use Venn diagram to draw intersections, mutually exclusive probabilities etc. Venn diagrams can’t be used for conditional probabillity or to show dependence\nWe can use tree diagrams to visualize conditional probability and dependence\nProbability of A given B - P(A | B) \nIf two probabilities are independent then:\n\nP(A|B) = P(A) because A and B are independent and A does not depend on occurance of B, we can use this expression to check if two events are independent\nP( A and B) = P(A|B)* P(B) as P(A|B) = P(A)\nP(A and B) = P(A) * P(B) - when two events are independent then we need to multiply their probabilities\nIf two probabilities are independent then they are not mutually exclusive. If probabilities are mutually exclusive then they are not independent\n\nMutually exclusive events - Add the probabilities - P(A U B) = P(A) + P(B) because P(A and B) will be zero\n\n\n\nBayes Thereom\n\nWhen we are provided information about a conditional probability eg P(A|B) and if we need to find the conditional probability P(B|A) then we should use Bayes theorem"
  },
  {
    "objectID": "ds_lifecycle/Feature_Engineering.html",
    "href": "ds_lifecycle/Feature_Engineering.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Deep learning is also called feature learning, many features can be automatically learned and extracted\n\n\n\n\nNot all types of missing values are equal\nMissing Not At Random (MNAR) - The values are missing for reasons related to the values themselves - Ex - Income not available for high earners\nMissing At Random (MAR) - The reason for missing is not due to the value itself, but due to another observed variable - Ex - Age missing for a gender value\nMissing completely at Random (MCAR) - No pattern in when the value is missing.\nThere is no perfect way to handle missing values. With deletion, you risk losing important information or accentuating biases. With imputation, you risk injecting your own bias into and adding noise to your data, or worse, data leakage\n\n\n\n\n\nA range of [-1,1] work better than the range [0,1]\nuse standardization if the variable might follow normal distribution\nIf there is skew in the distribution, log transformation may work\nScaling is a common source of data leakage\nScaling often requires global statistics. If the new data has changed significantly compared to the training, these statistics won’t be very useful. Therefore, it’s important to retrain your model often to account for these changes.\n\n\n\n\n\nThis will rarely help\nTurning a continuous feature into a discrete feature\nHistograms can help to choose the values for the categories\n\n\n\n\n\nCategories change in production - The model will crash if it encounters a brand not seen in training\nIf you are using a category called ‘UNKNOWN’ in production, this category should also be a part of training\nPeople who haven’t worked with data in production tend to assume that categories are static, which means the categories don’t change over time. This is true for many categories. For example, age brackets and income brackets are unlikely to change, and you know exactly how many categories there are in advance. Handling these categories is straightforward. You can just give each category a number and you’re done.\nHowever, in production, categories change. Imagine you’re building a recommender system to predict what products users might want to buy from Amazon. One of the features you want to use is the product brand. When looking at Amazon’s historical data, you realize that there are a lot of brands. Even back in 2019, there were already over two million brands on Amazon. The number of brands is overwhelming, but you think: “I can still handle this.” You encode each brand as a number, so now you have two million numbers, from 0 to 1,999,999, corresponding to two million brands. Your model does spectacularly on the historical test set, and you get approval to test it on 1% of today’s traffic. In production, your model crashes because it encounters a brand it hasn’t seen before and therefore can’t encode. New brands join Amazon all the time. To address this, you create a category UNKNOWN with the value of 2,000,000 to catch all the brands your model hasn’t seen during training. Your model doesn’t crash anymore, but your sellers complain that their new brands are not getting any traffic. It’s because your model didn’t see the category UNKNOWN in the train set, so it just doesn’t recommend any product of the UNKNOWN brand. You fix this by encoding only the top 99% most popular brands and encode the bottom 1% brand as UNKNOWN. This way, at least your model knows how to deal with UNKNOWN brands. Your model seems to work fine for about one hour, then the click-through rate on product recommendations plummets. Over the last hour, 20 new brands joined your site; some of them are new luxury brands, some of them are sketchy knockoff brands, some of them are established brands. However, your model treats them all the same way it treats unpopular brands in the training data. This isn’t an extreme example that only happens if you work at Amazon. This problem happens quite a lot. For example, if you want to predict whether a comment is spam, you might want to use the account that posted this comment as a feature, and new accounts are being created all the time. The same goes for new product types, new website domains, new restaurants, new companies, new IP addresses, and so on. If you work with any of them, you’ll have to deal with this problem. One solution to this problem is the hashing trick, popularized by the package Vowpal Wabbit developed at Microsoft.One solution to this problem is the hashing trick, popularized by the package Vowpal Wabbit developed at Microsoft. The gist of this trick is that you use a hash function to generate a hashed value of each category. The hashed value will become the index of that category. Because you can specify the hash space, you can fix the number of encoded values for a feature in advance, without having to know how many categories there will be. For example, if you choose a hash space of 18 bits, which corresponds to 218 = 262,144 possible hashed values, all the categories, even the ones that your model has never seen before, will be encoded by an index between 0 and 262,143.One problem with hashed functions is collision: two categories being assigned the same index. However, with many hash functions, the collisions are random; new brands can share an index with any of the existing brands instead of always sharing an index with unpopular brands, which is what happens when we use the preceding UNKNOWN category. The impact of colliding hashed features is, fortunately, not that bad. In research done by Booking.com, even for 50% colliding features, the performance loss is less than 0.5%\n\n\n\n\n\n4 categorical encoding concepts\nCategorical features for high cardinality\nCategorical encoding in Python\nKaggle Tutorial\n\n\n\n\nFeature crossing is the technique to combine two or more features to generate new features. This technique is useful to model the nonlinear relationships between features. Because feature crossing helps model nonlinear relationships between variables, it’s essential for models that can’t learn or are bad at learning nonlinear relationships, such as linear regression, logistic regression, and tree-based models.\nIt will blow-up the number of features and make the model overfit the data due to high features\n\n\n\n\n\nData leakage refers to the phenomenon when a form of the label “leaks” into the set of features used for making predictions, and this same information is not available during inference.\nCauses\n\nSplitting time-correlated data randomly instead of by time. To prevent future information from leaking into the training process and allowing models to cheat during evaluation, split your data by time, instead of splitting randomly, whenever possible\nScaling before splitting - This will leak mean and variance of the test samples into the training process, allowing a model to adjust its predictions for the test samples. To avoid this type of leakage, always split your data first before scaling, then use the statistics from the train split to scale all the splits.\nFilling in missing data with statistics from the test split - This type of leakage is similar to the type of leakage caused by scaling, and it can be prevented by using only statistics from the train split to fill in missing values in all the splits.\nPoor handling of data duplication before splitting - If you have duplicates or near-duplicates in your data, failing to remove them before splitting your data might cause the same samples to appear in both train and validation/test splits. If you oversample your data, do it after splitting\n\n\n\n\n\n\nA group of examples have strongly correlated labels but are divided into different splits. For example, a patient might have two lung CT scans that are a week apart, which likely have the same labels on whether they contain signs of lung cancer, but one of them is in the train split and the second is in the test split.\n\n\n\n\n\nMeasure the predictive power of each feature or a set of features with respect to the target variable (label). If a feature has unusually high correlation, investigate how this feature is generated and whether the correlation makes sense.\n\n\n\n\n\nSHAP\nBuilt-In feature importance functions of XGBoost\nInterpretML is a great open-source package that leverages feature importance to help understand how a model makes predictions\n\n\n\n\n\nSplit data by time into train/valid/test splits instead of doing it randomly.\n\nIf you oversample your data, do it after splitting.\n\nScale and normalize your data after splitting to avoid data leakage.\n\nUse statistics from only the train split, instead of the entire data, to scale your features and handle missing values.\n\nUnderstand how your data is generated, collected, and processed. Involve domain experts if possible.\n\nKeep track of your data’s lineage.\n\nUnderstand feature importance to your model.\n\nUse features that generalize well.\n\nRemove no longer useful features from your models"
  },
  {
    "objectID": "ds_lifecycle/Resources.html",
    "href": "ds_lifecycle/Resources.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Resources\n\nEnjoy algorithms\nGenerate Images from text\ncode generation models\nDall-e2 explained\nopen source data labeling platform\nIntroduction to ML interviews book\nA Recipe for training neural networks\nModel calibration\nKaggle solutions\nXGBoost resources\nHyperparameter tuning using Milano\nChip Huyen’s Designing ML System book github repo\nGoogle’s rules of ML\nBias of Mortgage\nMachine learning systems design slide deck #mustcheck\nAI Ethics videos by Timnit Gebru\nAI Ethics videos\nAI Incident Database\nH2o Admissible machine learning\nGoogle responsible AI practices\nIBM AI fairness 360\nAI Now\nStitchfix model deployment slide deck\nKubernetes explained\nTrustworthy ML Tutorials\nMadewithml\nPython packaging and dependency management"
  },
  {
    "objectID": "ds_lifecycle/Model_Evaluation.html",
    "href": "ds_lifecycle/Model_Evaluation.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Random baseline\nSimple heuristic\nZero rule baseline - Baseline model always predicts the most common class\nHuman baseline\nExisting solutions\n\n\n\n\nPerturbation tests\n\nThe production data will be noisy compared to training data. So we can make small changes to your test splits to see how these changes affect your model’s performance.\n\nInvariance tests\n\nCertain changes to the sensitive inputs shouldn’t lead to changes in the output. changes to race information shouldn’t affect the mortgage outcome. Similarly, changes to applicants’ names shouldn’t affect their resume screening results nor should someone’s gender affect how much they should be paid. If these happen, there are biases in your model, which might render it unusable no matter how good its performance is.\n\nDirectional expectation tests\n\nCertain changes to the inputs should, however, cause predictable changes in outputs. For example, when developing a model to predict housing prices, keeping all the features the same but increasing the lot size shouldn’t decrease the predicted price, and decreasing the square footage shouldn’t increase it. If the outputs change in the opposite expected direction, your model might not be learning the right thing, and you need to investigate it further before deploying it.\n\nModel calibration\n\nChapter 6. Model Development and Offline Evaluation In Chapter 4, we discussed how to create training data for your model, and in Chapter 5, we discussed how to engineer features from that training data. With the initial set of features, we’ll move to the ML algorithm part of ML systems. For me, this has always been the most fun step, as it allows me to play around with different algorithms and techniques, even the latest ones. This is also the first step where I can see all the hard work I’ve put into data and feature engineering transformed into a system whose outputs (predictions) I can use to evaluate the success of my effort.\n\n\nTo build an ML model, we first need to select the ML model to build. There are so many ML algorithms out there, with more actively being developed. This chapter starts with six tips for selecting the best algorithms for your task.\nThe section that follows discusses different aspects of model development, such as debugging, experiment tracking and versioning, distributed training, and AutoML.\nModel development is an iterative process. After each iteration, you’ll want to compare your model’s performance against its performance in previous iterations and evaluate how suitable this iteration is for production. The last section of this chapter is dedicated to how to evaluate your model before deploying it to production, covering a range of evaluation techniques including perturbation tests, invariance tests, model calibration, and slide-based evaluation.\nI expect that most readers already have an understanding of common ML algorithms such as linear models, decision trees, k-nearest neighbors, and different types of neural networks. This chapter will discuss techniques surrounding these algorithms but won’t go into details of how they work. Because this chapter deals with ML algorithms, it requires a lot more ML knowledge than other chapters. If you’re not familiar with them, I recommend taking an online course or reading a book on ML algorithms before reading this chapter. Readers wanting a quick refresh on basic ML concepts might find helpful the section “Basic ML Reviews” in the book’s GitHub repository.\nModel Development and Training In this section, we’ll discuss necessary aspects to help you develop and train your model, including how to evaluate different ML models for your problem, creating ensembles of models, experiment tracking and versioning, and distributed training, which is necessary for the scale at which models today are usually trained at. We’ll end this section with the more advanced topic of AutoML—using ML to automatically choose a model best for your problem.\nEvaluating ML Models There are many possible solutions to any given problem. Given a task that can leverage ML in its solution, you might wonder what ML algorithm you should use for it. For example, should you start with logistic regression, an algorithm that you’re already familiar with? Or should you try out a new fancy model that is supposed to be the new state of the art for your problem? A more senior colleague mentioned that gradient-boosted trees have always worked for her for this task in the past—should you listen to her advice?\nIf you had unlimited time and compute power, the rational thing to do would be to try all possible solutions and see what is best for you. However, time and compute power are limited resources, and you have to be strategic about what models you select.\nWhen talking about ML algorithms, many people think in terms of classical ML algorithms versus neural networks. There are a lot of interests and media coverage for neural networks, especially deep learning, which is understandable given that most of the AI progress in the last decade happened due to neural networks getting bigger and deeper.\nThese interests and coverage might give off the impression that deep learning is replacing classical ML algorithms. However, even though deep learning is finding more use cases in production, classical ML algorithms are not going away. Many recommender systems still rely on collaborative filtering and matrix factorization. Tree-based algorithms, including gradient-boosted trees, still power many classification tasks with strict latency requirements.\nEven in applications where neural networks are deployed, classic ML algorithms are still being used in tandem. For example, neural networks and decision trees might be used together in an ensemble. A k-means clustering model might be used to extract features to input into a neural network. Vice versa, a pretrained neural network (like BERT or GPT-3) might be used to generate embeddings to input into a logistic regression model.\nWhen selecting a model for your problem, you don’t choose from every possible model out there, but usually focus on a set of models suitable for your problem. For example, if your boss tells you to build a system to detect toxic tweets, you know that this is a text classification problem—given a piece of text, classify whether it’s toxic or not—and common models for text classification include naive Bayes, logistic regression, recurrent neural networks, and transformer-based models such as BERT, GPT, and their variants.\nIf your client wants you to build a system to detect fraudulent transactions, you know that this is the classic abnormality detection problem—fraudulent transactions are abnormalities that you want to detect—and common algorithms for this problem are many, including k-nearest neighbors, isolation forest, clustering, and neural networks.\nKnowledge of common ML tasks and the typical approaches to solve them is essential in this process.\nDifferent types of algorithms require different numbers of labels as well as different amounts of compute power. Some take longer to train than others, whereas some take longer to make predictions. Non-neural network algorithms tend to be more explainable (e.g., what features contributed the most to an email being classified as spam) than neural networks.\nWhen considering what model to use, it’s important to consider not only the model’s performance, measured by metrics such as accuracy, F1 score, and log loss, but also its other properties, such as how much data, compute, and time it needs to train, what’s its inference latency, and interpretability. For example, a simple logistic regression model might have lower accuracy than a complex neural network, but it requires less labeled data to start, it’s much faster to train, it’s much easier to deploy, and it’s also much easier to explain why it’s making certain predictions.\nComparing ML algorithms is out of the scope of this book. No matter how good a comparison is, it will be outdated as soon as new algorithms come out. Back in 2016, LSTM-RNNs were all the rage and the backbone of the architecture seq2seq (Sequence-to-Sequence) that powered many NLP tasks from machine translation to text summarization to text classification. However, just two years later, recurrent architectures were largely replaced by transformer architectures for NLP tasks.\nTo understand different algorithms, the best way is to equip yourself with basic ML knowledge and run experiments with the algorithms you’re interested in. To keep up to date with so many new ML techniques and models, I find it helpful to monitor trends at major ML conferences such as NeurIPS, ICLR, and ICML, as well as following researchers whose work has a high signal-to-noise ratio on Twitter.\nSix tips for model selection Without getting into specifics of different algorithms, here are six tips that might help you decide what ML algorithms to work on next.\nAvoid the state-of-the-art trap While helping companies as well as recent graduates get started in ML, I usually have to spend a nontrivial amount of time steering them away from jumping straight into state-of-the-art models. I can see why people want state-of-the-art models. Many believe that these models would be the best solutions for their problems—why try an old solution if you believe that a newer and superior solution exists? Many business leaders also want to use state-of-the-art models because they want to make their businesses appear cutting edge. Developers might also be more excited getting their hands on new models than getting stuck into the same old things over and over again.\nResearchers often only evaluate models in academic settings, which means that a model being state of the art often means that it performs better than existing models on some static datasets. It doesn’t mean that this model will be fast enough or cheap enough for you to implement. It doesn’t even mean that this model will perform better than other models on your data.\nWhile it’s essential to stay up to date with new technologies and beneficial to evaluate them for your business, the most important thing to do when solving a problem is finding solutions that can solve that problem. If there’s a solution that can solve your problem that is much cheaper and simpler than state-of-the-art models, use the simpler solution.\nStart with the simplest models Zen of Python states that “simple is better than complex,” and this principle is applicable to ML as well. Simplicity serves three purposes. First, simpler models are easier to deploy, and deploying your model early allows you to validate that your prediction pipeline is consistent with your training pipeline. Second, starting with something simple and adding more complex components step-by-step makes it easier to understand your model and debug it. Third, the simplest model serves as a baseline to which you can compare your more complex models.\nSimplest models are not always the same as models with the least effort. For example, pretrained BERT models are complex, but they require little effort to get started with, especially if you use a ready-made implementation like the one in Hugging Face’s Transformer. In this case, it’s not a bad idea to use the complex solution, given that the community around this solution is well developed enough to help you get through any problems you might encounter. However, you might still want to experiment with simpler solutions to ensure that pretrained BERT is indeed better than those simpler solutions for your problem. Pretrained BERT might be low effort to start with, but it can be quite high effort to improve upon. Whereas if you start with a simpler model, there’ll be a lot of room for you to improve upon your model.\nAvoid human biases in selecting models Imagine an engineer on your team is assigned the task of evaluating which model is better for your problem: a gradient-boosted tree or a pretrained BERT model. After two weeks, this engineer announces that the best BERT model outperforms the best gradient-boosted tree by 5%. Your team decides to go with the pretrained BERT model.\nA few months later, however, a seasoned engineer joins your team. She decides to look into gradient-boosted trees again and finds out that this time, the best gradient-boosted tree outperforms the pretrained BERT model you currently have in production. What happened?\nThere are a lot of human biases in evaluating models. Part of the process of evaluating an ML architecture is to experiment with different features and different sets of hyperparameters to find the best model of that architecture. If an engineer is more excited about an architecture, they will likely spend a lot more time experimenting with it, which might result in better-performing models for that architecture.\nWhen comparing different architectures, it’s important to compare them under comparable setups. If you run 100 experiments for an architecture, it’s not fair to only run a couple of experiments for the architecture you’re evaluating it against. You might need to run 100 experiments for the other architecture too.\nBecause the performance of a model architecture depends heavily on the context it’s evaluated in—e.g., the task, the training data, the test data, the hyperparameters, etc.—it’s extremely difficult to make claims that a model architecture is better than another architecture. The claim might be true in a context, but unlikely true for all possible contexts.\nEvaluate good performance now versus good performance later The best model now does not always mean the best model two months from now. For example, a tree-based model might work better now because you don’t have a ton of data yet, but two months from now, you might be able to double your amount of training data, and your neural network might perform much better.1\nA simple way to estimate how your model’s performance might change with more data is to use learning curves. A learning curve of a model is a plot of its performance—e.g., training loss, training accuracy, validation accuracy—against the number of training samples it uses. The learning curve won’t help you estimate exactly how much performance gain you can get from having more training data, but it can give you a sense of whether you can expect any performance gain at all from more training data.\nA situation that I’ve encountered is when a team evaluates a simple neural network against a collaborative filtering model for making recommendations. When evaluating both models offline, the collaborative filtering model outperformed. However, the simple neural network can update itself with each incoming example, whereas the collaborative filtering has to look at all the data to update its underlying matrix. The team decided to deploy both the collaborative filtering model and the simple neural network. They used the collaborative filtering model to make predictions for users, and continually trained the simple neural network in production with new, incoming data. After two weeks, the simple neural network was able to outperform the collaborative filtering model.\nWhile evaluating models, you might want to take into account their potential for improvements in the near future, and how easy/difficult it is to achieve those improvements.\nEvaluate trade-offs There are many trade-offs you have to make when selecting models. Understanding what’s more important in the performance of your ML system will help you choose the most suitable model.\nOne classic example of trade-off is the false positives and false negatives trade-off. Reducing the number of false positives might increase the number of false negatives, and vice versa. In a task where false positives are more dangerous than false negatives, such as fingerprint unlocking (unauthorized people shouldn’t be classified as authorized and given access), you might prefer a model that makes fewer false positives. Similarly, in a task where false negatives are more dangerous than false positives, such as COVID-19 screening (patients with COVID-19 shouldn’t be classified as no COVID-19), you might prefer a model that makes fewer false negatives.\nAnother example of trade-off is compute requirement and accuracy—a more complex model might deliver higher accuracy but might require a more powerful machine, such as a GPU instead of a CPU, to generate predictions with acceptable inference latency. Many people also care about the interpretability and performance trade-off. A more complex model can give a better performance, but its results are less interpretable.\nUnderstand your model’s assumptions The statistician George Box said in 1976 that “all models are wrong, but some are useful.” The real world is intractably complex, and models can only approximate using assumptions. Every single model comes with its own assumptions. Understanding what assumptions a model makes and whether our data satisfies those assumptions can help you evaluate which model works best for your use case.\nFollowing are some of the common assumptions. It’s not meant to be an exhaustive list, but just a demonstration:\nPrediction assumption Every model that aims to predict an output Y from an input X makes the assumption that it’s possible to predict Y based on X.\nIID Neural networks assume that the examples are independent and identically distributed, which means that all the examples are independently drawn from the same joint distribution.\nSmoothness Every supervised machine learning method assumes that there’s a set of functions that can transform inputs into outputs such that similar inputs are transformed into similar outputs. If an input X produces an output Y, then an input close to X would produce an output proportionally close to Y.\nTractability Let X be the input and Z be the latent representation of X. Every generative model makes the assumption that it’s tractable to compute the probability P(Z|X).\nBoundaries A linear classifier assumes that decision boundaries are linear.\nConditional independence A naive Bayes classifier assumes that the attribute values are independent of each other given the class.\nNormally distributed Many statistical methods assume that data is normally distributed.\nEnsembles When considering an ML solution to your problem, you might want to start with a system that contains just one model (the process of selecting one model for your problem was discussed earlier in the chapter). After developing one single model, you might think about how to continue improving its performance. One method that has consistently given a performance boost is to use an ensemble of multiple models instead of just an individual model to make predictions. Each model in the ensemble is called a base learner. For example, for the task of predicting whether an email is SPAM or NOT SPAM, you might have three different models. The final prediction for each email is the majority vote of all three models. So if at least two base learners output SPAM, the email will be classified as SPAM.\nTwenty out of 22 winning solutions on Kaggle competitions in 2021, as of August 2021, use ensembles.2 As of January 2022, 20 top solutions on SQuAD 2.0, the Stanford Question Answering Dataset, are ensembles, as shown in Figure 6-2.\nEnsembling methods are less favored in production because ensembles are more complex to deploy and harder to maintain. However, they are still common for tasks where a small performance boost can lead to a huge financial gain, such as predicting click-through rate for ads.\nWe’ll go over an example to give you the intuition of why ensembling works. Imagine you have three email spam classifiers, each with an accuracy of 70%. Assuming that each classifier has an equal probability of making a correct prediction for each email, and that these three classifiers are not correlated, we’ll show that by taking the majority vote of these three classifiers, we can get an accuracy of 78.4%.\nFor each email, each classifier has a 70% chance of being correct. The ensemble will be correct if at least two classifiers are correct. Table 6-1 shows the probabilities of different possible outcomes of the ensemble given an email. This ensemble will have an accuracy of 0.343 + 0.441 = 0.784, or 78.4%.\nOutputs of three models Probability Ensemble’s output All three are correct 0.7 * 0.7 * 0.7 = 0.343 Correct Only two are correct (0.7 * 0.7 * 0.3) * 3 = 0.441 Correct Only one is correct (0.3 * 0.3 * 0.7) * 3 = 0.189 Wrong None are correct 0.3 * 0.3 * 0.3 = 0.027 Wrong This calculation only holds if the classifiers in an ensemble are uncorrelated. If all classifiers are perfectly correlated—all three of them make the same prediction for every email—the ensemble will have the same accuracy as each individual classifier. When creating an ensemble, the less correlation there is among base learners, the better the ensemble will be. Therefore, it’s common to choose very different types of models for an ensemble. For example, you might create an ensemble that consists of one transformer model, one recurrent neural network, and one gradient-boosted tree.\nThere are three ways to create an ensemble: bagging, boosting, and stacking. In addition to helping boost performance, according to several survey papers, ensemble methods such as boosting and bagging, together with resampling, have shown to help with imbalanced datasets.3 We’ll go over each of these three methods, starting with bagging.\nBagging Bagging, shortened from bootstrap aggregating, is designed to improve both the training stability and accuracy of ML algorithms.4 It reduces variance and helps to avoid overfitting.\nGiven a dataset, instead of training one classifier on the entire dataset, you sample with replacement to create different datasets, called bootstraps, and train a classification or regression model on each of these bootstraps. Sampling with replacement ensures that each bootstrap is created independently from its peers. Figure 6-3 shows an illustration of bagging.\nFigure 6-3. Bagging illustration. Source: Adapted from an image by Sirakorn If the problem is classification, the final prediction is decided by the majority vote of all models. For example, if 10 classifiers vote SPAM and 6 models vote NOT SPAM, the final prediction is SPAM.\nIf the problem is regression, the final prediction is the average of all models’ predictions.\nBagging generally improves unstable methods, such as neural networks, classification and regression trees, and subset selection in linear regression. However, it can mildly degrade the performance of stable methods such as k-nearest neighbors.5\nA random forest is an example of bagging. A random forest is a collection of decision trees constructed by both bagging and feature randomness, where each tree can pick only from a random subset of features to use.\nBoosting Boosting is a family of iterative ensemble algorithms that convert weak learners to strong ones. Each learner in this ensemble is trained on the same set of samples, but the samples are weighted differently among iterations. As a result, future weak learners focus more on the examples that previous weak learners misclassified. Figure 6-4 shows an illustration of boosting, which involves the steps that follow.\nFigure 6-4. Boosting illustration. Source: Adapted from an image by Sirakorn You start by training the first weak classifier on the original dataset.\nSamples are reweighted based on how well the first classifier classifies them, e.g., misclassified samples are given higher weight.\nTrain the second classifier on this reweighted dataset. Your ensemble now consists of the first and the second classifiers.\nSamples are weighted based on how well the ensemble classifies them.\nTrain the third classifier on this reweighted dataset. Add the third classifier to the ensemble.\nRepeat for as many iterations as needed.\nForm the final strong classifier as a weighted combination of the existing classifiers—classifiers with smaller training errors have higher weights.\nAn example of a boosting algorithm is a gradient boosting machine (GBM), which produces a prediction model typically from weak decision trees. It builds the model in a stage-wise fashion like other boosting methods do, and it generalizes them by allowing optimization of an arbitrary differentiable loss function.\nXGBoost, a variant of GBM, used to be the algorithm of choice for many winning teams of ML competitions.6 It’s been used in a wide range of tasks from classification, ranking, to the discovery of the Higgs Boson.7 However, many teams have been opting for LightGBM, a distributed gradient boosting framework that allows parallel learning, which generally allows faster training on large datasets.\nStacking Stacking means that you train base learners from the training data then create a meta-learner that combines the outputs of the base learners to output final predictions, as shown in Figure 6-5. The meta-learner can be as simple as a heuristic: you take the majority vote (for classification tasks) or the average vote (for regression tasks) from all base learners. It can be another model, such as a logistic regression model or a linear regression model.\nFigure 6-5. A visualization of a stacked ensemble from three base learners For more great advice on how to create an ensemble, refer to the awesome ensemble guide by one of Kaggle’s legendary teams, MLWave.\nExperiment Tracking and Versioning During the model development process, you often have to experiment with many architectures and many different models to choose the best one for your problem. Some models might seem similar to each other and differ in only one hyperparameter—such as one model using a learning rate of 0.003 and another model using a learning rate of 0.002—and yet their performances are dramatically different. It’s important to keep track of all the definitions needed to re-create an experiment and its relevant artifacts. An artifact is a file generated during an experiment—examples of artifacts can be files that show the loss curve, evaluation loss graph, logs, or intermediate results of a model throughout a training process. This enables you to compare different experiments and choose the one best suited for your needs. Comparing different experiments can also help you understand how small changes affect your model’s performance, which, in turn, gives you more visibility into how your model works.\nThe process of tracking the progress and results of an experiment is called experiment tracking. The process of logging all the details of an experiment for the purpose of possibly recreating it later or comparing it with other experiments is called versioning. These two go hand in hand with each other. Many tools originally set out to be experiment tracking tools, such as MLflow and Weights & Biases, have grown to incorporate versioning. Many tools originally set out to be versioning tools, such as DVC, have also incorporated experiment tracking.\nExperiment tracking A large part of training an ML model is babysitting the learning processes. Many problems can arise during the training process, including loss not decreasing, overfitting, underfitting, fluctuating weight values, dead neurons, and running out of memory. It’s important to track what’s going on during training not only to detect and address these issues but also to evaluate whether your model is learning anything useful.\nWhen I just started getting into ML, all I was told to track was loss and speed. Fast-forward several years, and people are tracking so many things that their experiment tracking boards look both beautiful and terrifying at the same time. Following is just a short list of things you might want to consider tracking for each experiment during its training process:\nThe loss curve corresponding to the train split and each of the eval splits.\nThe model performance metrics that you care about on all nontest splits, such as accuracy, F1, perplexity.\nThe log of corresponding sample, prediction, and ground truth label. This comes in handy for ad hoc analytics and sanity check.\nThe speed of your model, evaluated by the number of steps per second or, if your data is text, the number of tokens processed per second.\nSystem performance metrics such as memory usage and CPU/GPU utilization. They’re important to identify bottlenecks and avoid wasting system resources.\nThe values over time of any parameter and hyperparameter whose changes can affect your model’s performance, such as the learning rate if you use a learning rate schedule; gradient norms (both globally and per layer), especially if you’re clipping your gradient norms; and weight norm, especially if you’re doing weight decay.\nIn theory, it’s not a bad idea to track everything you can. Most of the time, you probably don’t need to look at most of them. But when something does happen, one or more of them might give you clues to understand and/or debug your model. In general, tracking gives you observability into the state of your model.8 However, in practice, due to the limitations of tooling today, it can be overwhelming to track too many things, and tracking less important things can distract you from tracking what is really important.\nExperiment tracking enables comparison across experiments. By observing how a certain change in a component affects the model’s performance, you gain some understanding into what that component does.\nA simple way to track your experiments is to automatically make copies of all the code files needed for an experiment and log all outputs with their timestamps.9 Using third-party experiment tracking tools, however, can give you nice dashboards and allow you to share your experiments with your coworkers.\nVersioning Imagine this scenario. You and your team spent the last few weeks tweaking your model, and one of the runs finally showed promising results. You wanted to use it for more extensive tests, so you tried to replicate it using the set of hyperparameters you’d noted down somewhere, only to find out that the results weren’t quite the same. You remembered that you’d made some changes to the code between that run and the next, so you tried your best to undo the changes from memory because your reckless past self had decided that the change was too minimal to be committed. But you still couldn’t replicate the promising result because there are just too many possible ways to make changes.\nThis problem could have been avoided if you versioned your ML experiments. ML systems are part code, part data, so you need to not only version your code but your data as well. Code versioning has more or less become a standard in the industry. However, at this point, data versioning is like flossing. Everyone agrees it’s a good thing to do, but few do it.\nThere are a few reasons why data versioning is challenging. One reason is that because data is often much larger than code, we can’t use the same strategy that people usually use to version code to version data.\nFor example, code versioning is done by keeping track of all the changes made to a codebase. A change is known as a diff, short for difference. Each change is measured by line-by-line comparison. A line of code is usually short enough for line-by-line comparison to make sense. However, a line of your data, especially if it’s stored in a binary format, can be indefinitely long. Saying that this line of 1,000,000 characters is different from the other line of 1,000,000 characters isn’t going to be that helpful.\nCode versioning tools allow users to revert to a previous version of the codebase by keeping copies of all the old files. However, a dataset used might be so large that duplicating it multiple times might be unfeasible.\nCode versioning tools allow for multiple people to work on the same codebase at the same time by duplicating the codebase on each person’s local machine. However, a dataset might not fit into a local machine.\nSecond, there’s still confusion in what exactly constitutes a diff when we version data. Would diffs mean changes in the content of any file in your data repository, only when a file is removed or added, or when the checksum of the whole repository has changed?\nAs of 2021, data versioning tools like DVC only register a diff if the checksum of the total directory has changed and if a file is removed or added.\nAnother confusion is in how to resolve merge conflicts: if developer 1 uses data version X to train model A and developer 2 uses data version Y to train model B, it doesn’t make sense to merge data versions X and Y to create Z, since there’s no model corresponding with Z.\nThird, if you use user data to train your model, regulations like General Data Protection Regulation (GDPR) might make versioning this data complicated. For example, regulations might mandate that you delete user data if requested, making it legally impossible to recover older versions of your data.\nAggressive experiment tracking and versioning helps with reproducibility, but it doesn’t ensure reproducibility. The frameworks and hardware you use might introduce nondeterminism to your experiment results,10 making it impossible to replicate the result of an experiment without knowing everything about the environment your experiment runs in.\nThe way we have to run so many experiments right now to find the best possible model is the result of us treating ML as a black box. Because we can’t predict which configuration will work best, we have to experiment with multiple configurations. However, I hope that as the field progresses, we’ll gain more understanding into different models and can reason about what model will work best instead of running hundreds or thousands of experiments.\nDEBUGGING ML MODELS Debugging is an inherent part of developing any piece of software. ML models aren’t an exception. Debugging is never fun, and debugging ML models can be especially frustrating for the following three reasons.\nFirst, ML models fail silently, a topic we’ll cover in depth in Chapter 8. The code compiles. The loss decreases as it should. The correct functions are called. The predictions are made, but the predictions are wrong. The developers don’t notice the errors. And worse, users don’t either and use the predictions as if the application was functioning as it should.\nSecond, even when you think you’ve found the bug, it can be frustratingly slow to validate whether the bug has been fixed. When debugging a traditional software program, you might be able to make changes to the buggy code and see the result immediately. However, when making changes to an ML model, you might have to retrain the model and wait until it converges to see whether the bug is fixed, which can take hours. In some cases, you can’t even be sure whether the bugs are fixed until the model is deployed to the users.\nThird, debugging ML models is hard because of their cross-functional complexity. There are many components in an ML system: data, labels, features, ML algorithms, code, infrastructure, etc. These different components might be owned by different teams. For example, data is managed by data engineers, labels by subject matter experts, ML algorithms by data scientists, and infrastructure by ML engineers or the ML platform team. When an error occurs, it could be because of any of these components or a combination of them, making it hard to know where to look or who should be looking into it.\nHere are some of the things that might cause an ML model to fail:\nTheoretical constraints As discussed previously, each model comes with its own assumptions about the data and the features it uses. A model might fail because the data it learns from doesn’t conform to its assumptions. For example, you use a linear model for the data whose decision boundaries aren’t linear.\nPoor implementation of model The model might be a good fit for the data, but the bugs are in the implementation of the model. For example, if you use PyTorch, you might have forgotten to stop gradient updates during evaluation when you should. The more components a model has, the more things that can go wrong, and the harder it is to figure out which goes wrong. However, with models being increasingly commoditized and more and more companies using off-the-shelf models, this is becoming less of a problem.\nPoor choice of hyperparameters With the same model, one set of hyperparameters can give you the state-of-the-art result but another set of hyperparameters might cause the model to never converge. The model is a great fit for your data, and its implementation is correct, but a poor set of hyperparameters might render your model useless.\nData problems There are many things that could go wrong in data collection and preprocessing that might cause your models to perform poorly, such as data samples and labels being incorrectly paired, noisy labels, features normalized using outdated statistics, and more.\nPoor choice of features There might be many possible features for your models to learn from. Too many features might cause your models to overfit to the training data or cause data leakage. Too few features might lack predictive power to allow your models to make good predictions.\nDebugging should be both preventive and curative. You should have healthy practices to minimize the opportunities for bugs to proliferate as well as a procedure for detecting, locating, and fixing bugs. Having the discipline to follow both the best practices and the debugging procedure is crucial in developing, implementing, and deploying ML models.\nThere is, unfortunately, still no scientific approach to debugging in ML. However, there have been a number of tried-and-true debugging techniques published by experienced ML engineers and researchers. The following are three of them. Readers interested in learning more might want to check out Andrej Karpathy’s awesome post “A Recipe for Training Neural Networks”.\nStart simple and gradually add more components Start with the simplest model and then slowly add more components to see if it helps or hurts the performance. For example, if you want to build a recurrent neural network (RNN), start with just one level of RNN cell before stacking multiple together or adding more regularization. If you want to use a BERT-like model (Devlin et al. 2018), which uses both a masked language model (MLM) and next sentence prediction (NSP) loss, you might want to use only the MLM loss before adding NSP loss.\nCurrently, many people start out by cloning an open source implementation of a state-of-the-art model and plugging in their own data. On the off-chance that it works, it’s great. But if it doesn’t, it’s very hard to debug the system because the problem could have been caused by any of the many components in the model.\nOverfit a single batch After you have a simple implementation of your model, try to overfit a small amount of training data and run evaluation on the same data to make sure that it gets to the smallest possible loss. If it’s for image recognition, overfit on 10 images and see if you can get the accuracy to be 100%, or if it’s for machine translation, overfit on 100 sentence pairs and see if you can get to a BLEU score of near 100. If it can’t overfit a small amount of data, there might be something wrong with your implementation.\nSet a random seed There are so many factors that contribute to the randomness of your model: weight initialization, dropout, data shuffling, etc. Randomness makes it hard to compare results across different experiments—you have no idea if the change in performance is due to a change in the model or a different random seed. Setting a random seed ensures consistency between different runs. It also allows you to reproduce errors and other people to reproduce your results.\nDistributed Training As models are getting bigger and more resource-intensive, companies care a lot more about training at scale.11 Expertise in scalability is hard to acquire because it requires having regular access to massive compute resources. Scalability is a topic that merits a series of books. This section covers some notable issues to highlight the challenges of doing ML at scale and provide a scaffold to help you plan the resources for your project accordingly.\nIt’s common to train a model using data that doesn’t fit into memory. It’s especially common when dealing with medical data such as CT scans or genome sequences. It can also happen with text data if you work for teams that train large language models (cue OpenAI, Google, NVIDIA, Cohere).\nWhen your data doesn’t fit into memory, your algorithms for preprocessing (e.g., zero-centering, normalizing, whitening), shuffling, and batching data will need to run out of core and in parallel.12 When a sample of your data is large, e.g., one machine can handle a few samples at a time, you might only be able to work with a small batch size, which leads to instability for gradient descent-based optimization.\nIn some cases, a data sample is so large it can’t even fit into memory and you will have to use something like gradient checkpointing, a technique that leverages the memory footprint and compute trade-off to make your system do more computation with less memory. According to the authors of the open source package gradient-checkpointing, “For feed-forward models we were able to fit more than 10x larger models onto our GPU, at only a 20% increase in computation time.”13 Even when a sample fits into memory, using checkpointing can allow you to fit more samples into a batch, which might allow you to train your model faster.\nData parallelism It’s now the norm to train ML models on multiple machines. The most common parallelization method supported by modern ML frameworks is data parallelism: you split your data on multiple machines, train your model on all of them, and accumulate gradients. This gives rise to a couple of issues.\nA challenging problem is how to accurately and effectively accumulate gradients from different machines. As each machine produces its own gradient, if your model waits for all of them to finish a run—synchronous stochastic gradient descent (SGD)—stragglers will cause the entire system to slow down, wasting time and resources.14 The straggler problem grows with the number of machines, as the more workers, the more likely that at least one worker will run unusually slowly in a given iteration. However, there have been many algorithms that effectively address this problem.15\nIf your model updates the weight using the gradient from each machine separately—asynchronous SGD—gradient staleness might become a problem because the gradients from one machine have caused the weights to change before the gradients from another machine have come in.16\nThe difference between synchronous SGD and asynchronous SGD is illustrated in Figure 6-6.\nFigure 6-6. Synchronous SGD versus asynchronous SGD for data parallelism. Source: Adapted from an image by Jim Dowling17 In theory, asynchronous SGD converges but requires more steps than synchronous SGD. However, in practice, when the number of weights is large, gradient updates tend to be sparse, meaning most gradient updates only modify small fractions of the parameters, and it’s less likely that two gradient updates from different machines will modify the same weights. When gradient updates are sparse, gradient staleness becomes less of a problem and the model converges similarly for both synchronous and asynchronous SGD.18\nAnother problem is that spreading your model on multiple machines can cause your batch size to be very big. If a machine processes a batch size of 1,000, then 1,000 machines process a batch size of 1M (OpenAI’s GPT-3 175B uses a batch size of 3.2M in 2020).19 To oversimplify the calculation, if training an epoch on a machine takes 1M steps, training on 1,000 machines might take only 1,000 steps. An intuitive approach is to scale up the learning rate to account for more learning at each step, but we also can’t make the learning rate too big as it will lead to unstable convergence. In practice, increasing the batch size past a certain point yields diminishing returns.20\nLast but not least, with the same model setup, the main worker sometimes uses a lot more resources than other workers. If that’s the case, to make the most use out of all machines, you need to figure out a way to balance out the workload among them. The easiest way, but not the most effective way, is to use a smaller batch size on the main worker and a larger batch size on other workers.\nModel parallelism With data parallelism, each worker has its own copy of the whole model and does all the computation necessary for its copy of the model. Model parallelism is when different components of your model are trained on different machines, as shown in Figure 6-7. For example, machine 0 handles the computation for the first two layers while machine 1 handles the next two layers, or some machines can handle the forward pass while several others handle the backward pass.\nFigure 6-7. Data parallelism and model parallelism. Source: Adapted from an image by Jure Leskovec21 Model parallelism can be misleading because in some cases parallelism doesn’t mean that different parts of the model in different machines are executed in parallel. For example, if your model is a massive matrix and the matrix is split into two halves on two machines, then these two halves might be executed in parallel. However, if your model is a neural network and you put the first layer on machine 1 and the second layer on machine 2, and layer 2 needs outputs from layer 1 to execute, then machine 2 has to wait for machine 1 to finish first to run.\nPipeline parallelism is a clever technique to make different components of a model on different machines run more in parallel. There are multiple variants to this, but the key idea is to break the computation of each machine into multiple parts. When machine 1 finishes the first part of its computation, it passes the result onto machine 2, then continues to the second part, and so on. Machine 2 now can execute its computation on the first part while machine 1 executes its computation on the second part.\nTo make this concrete, consider you have four different machines and the first, second, third, and fourth layers are on machine 1, 2, 3, and 4 respectively. With pipeline parallelism, each mini-batch is broken into four micro-batches. Machine 1 computes the first layer on the first micro-batch, then machine 2 computes the second layer on machine 1’s results while machine 1 computes the first layer on the second micro-batch, and so on. Figure 6-8 shows what pipeline parallelism looks like on four machines; each machine runs both the forward pass and the backward pass for one component of a neural network.\nFigure 6-8. Pipeline parallelism for a neural network on four machines; each machine runs both the forward pass (F) and the backward pass (B) for one component of the neural network. Source: Adapted from an image by Huang et al.22 Model parallelism and data parallelism aren’t mutually exclusive. Many companies use both methods for better utilization of their hardware, even though the setup to use both methods can require significant engineering effort.\nAutoML There’s a joke that a good ML researcher is someone who will automate themselves out of job, designing an AI algorithm intelligent enough to design itself. It was funny until the TensorFlow Dev Summit 2018, where Jeff Dean took the stage and declared that Google intended on replacing ML expertise with 100 times more computational power, introducing AutoML to the excitement and horror of the community. Instead of paying a group of 100 ML researchers/engineers to fiddle with various models and eventually select a suboptimal one, why not use that money on compute to search for the optimal model? A screenshot from the recording of the event is shown in Figure 6-9.\nFigure 6-9. Jeff Dean unveiling Google’s AutoML at TensorFlow Dev Summit 2018 Soft AutoML: Hyperparameter tuning AutoML refers to automating the process of finding ML algorithms to solve real-world problems. One mild form, and the most popular form, of AutoML in production is hyperparameter tuning. A hyperparameter is a parameter supplied by users whose value is used to control the learning process, e.g., learning rate, batch size, number of hidden layers, number of hidden units, dropout probability, β1 and β2 in Adam optimizer, etc. Even quantization—e.g., whether to use 32 bits, 16 bits, or 8 bits to represent a number or a mixture of these representations—can be considered a hyperparameter to tune.23\nWith different sets of hyperparameters, the same model can give drastically different performances on the same dataset. Melis et al. showed in their 2018 paper “On the State of the Art of Evaluation in Neural Language Models” that weaker models with well-tuned hyperparameters can outperform stronger, fancier models. The goal of hyperparameter tuning is to find the optimal set of hyperparameters for a given model within a search space—the performance of each set evaluated on a validation set.\nDespite knowing its importance, many still ignore systematic approaches to hyperparameter tuning in favor of a manual, gut-feeling approach. The most popular is arguably graduate student descent (GSD), a technique in which a graduate student fiddles around with the hyperparameters until the model works.24\nHowever, more and more people are adopting hyperparameter tuning as part of their standard pipelines. Popular ML frameworks either come with built-in utilities or have third-party utilities for hyperparameter tuning—for example, scikit-learn with auto-sklearn,25 TensorFlow with Keras Tuner, and Ray with Tune. Popular methods for hyperparameter tuning include random search,26 grid search, and Bayesian optimization.27 The book AutoML: Methods, Systems, Challenges by the AutoML group at the University of Freiburg dedicates its first chapter (which you can read online for free) to hyperparameter optimization.\nWhen tuning hyperparameters, keep in mind that a model’s performance might be more sensitive to the change in one hyperparameter than another, and therefore sensitive hyperparameters should be more carefully tuned.\nWARNING It’s crucial to never use your test split to tune hyperparameters. Choose the best set of hyperparameters for a model based on its performance on a validation split, then report the model’s final performance on the test split. If you use your test split to tune hyperparameters, you risk overfitting your model to the test split.\nHard AutoML: Architecture search and learned optimizer Some teams take hyperparameter tuning to the next level: what if we treat other components of a model or the entire model as hyperparameters. The size of a convolution layer or whether or not to have a skip layer can be considered a hyperparameter. Instead of manually putting a pooling layer after a convolutional layer or ReLu (rectified linear unit) after linear, you give your algorithm these building blocks and let it figure out how to combine them. This area of research is known as architectural search, or neural architecture search (NAS) for neural networks, as it searches for the optimal model architecture.\nA NAS setup consists of three components:\nA search space Defines possible model architectures—i.e., building blocks to choose from and constraints on how they can be combined.\nA performance estimation strategy To evaluate the performance of a candidate architecture without having to train each candidate architecture from scratch until convergence. When we have a large number of candidate architectures, say 1,000, training all of them until convergence can be costly.\nA search strategy To explore the search space. A simple approach is random search—randomly choosing from all possible configurations—which is unpopular because it’s prohibitively expensive even for NAS. Common approaches include reinforcement learning (rewarding the choices that improve the performance estimation) and evolution (adding mutations to an architecture, choosing the best-performing ones, adding mutations to them, and so on).28\nFor NAS, the search space is discrete—the final architecture uses only one of the available options for each layer/operation,29 and you have to provide the set of building blocks. The common building blocks are various convolutions of different sizes, linear, various activations, pooling, identity, zero, etc. The set of building blocks varies based on the base architecture, e.g., convolutional neural networks or transformers.\nIn a typical ML training process, you have a model and then a learning procedure, an algorithm that helps your model find the set of parameters that minimize a given objective function for a given set of data. The most common learning procedure for neural networks today is gradient descent, which leverages an optimizer to specify how to update a model’s weights given gradient updates.30 Popular optimizers are, as you probably already know, Adam, Momentum, SGD, etc. In theory, you can include optimizers as building blocks in NAS and search for one that works best. In practice, this is difficult to do, since optimizers are sensitive to the setting of their hyperparameters, and the default hyperparameters don’t often work well across architectures.\nThis leads to an exciting research direction: what if we replace the functions that specify the update rule with a neural network? How much to update the model’s weights will be calculated by this neural network. This approach results in learned optimizers, as opposed to hand-designed optimizers.\nSince learned optimizers are neural networks, they need to be trained. You can train your learned optimizer on the same dataset you’re training the rest of your neural network on, but this requires you to train an optimizer every time you have a task.\nAnother approach is to train a learned optimizer once on a set of existing tasks—using aggregated loss on those tasks as the loss function and existing designed optimizers as the learning rule—and use it for every new task after that. For example, Metz et al. constructed a set of thousands of tasks to train learned optimizers. Their learned optimizer was able to generalize to both new datasets and domains as well as new architectures.31 And the beauty of this approach is that the learned optimizer can then be used to train a better-learned optimizer, an algorithm that improves on itself.\nWhether it’s architecture search or meta-learning learning rules, the up-front training cost is expensive enough that only a handful of companies in the world can afford to pursue them. However, it’s important for people interested in ML in production to be aware of the progress in AutoML for two reasons. First, the resulting architectures and learned optimizers can allow ML algorithms to work off-the-shelf on multiple real-world tasks, saving production time and cost, during both training and inferencing. For example, EfficientNets, a family of models produced by Google’s AutoML team, surpass state-of-the-art accuracy with up to 10x better efficiency.32 Second, they might be able to solve many real-world tasks previously impossible with existing architectures and optimizers.\nFOUR PHASES OF ML MODEL DEVELOPMENT Before we transition to model training, let’s take a look at the four phases of ML model development. Once you’ve decided to explore ML, your strategy depends on which phase of ML adoption you are in. There are four phases of adopting ML. The solutions from a phase can be used as baselines to evaluate the solutions from the next phase:\nPhase 1. Before machine learning If this is your first time trying to make this type of prediction from this type of data, start with non-ML solutions. Your first stab at the problem can be the simplest heuristics. For example, to predict what letter users are going to type next in English, you can show the top three most common English letters, “e,” “t,” and “a,” which might get your accuracy to be 30%.\nFacebook newsfeed was introduced in 2006 without any intelligent algorithms—posts were shown in chronological order, as shown in Figure 6-10.33 It wasn’t until 2011 that Facebook started displaying news updates you were most interested in at the top of the feed.\nFigure 6-10. Facebook newsfeed circa 2006. Source: Iveta Ryšavá34 According to Martin Zinkevich in his magnificent “Rules of Machine Learning: Best Practices for ML Engineering”: “If you think that machine learning will give you a 100% boost, then a heuristic will get you 50% of the way there.”35 You might even find that non-ML solutions work fine and you don’t need ML yet.\nPhase 2. Simplest machine learning models For your first ML model, you want to start with a simple algorithm, something that gives you visibility into its working to allow you to validate the usefulness of your problem framing and your data. Logistic regression, gradient-boosted trees, k-nearest neighbors can be great for that. They are also easier to implement and deploy, which allows you to quickly build out a framework from data engineering to development to deployment that you can test out and gain confidence on.\nPhase 3. Optimizing simple models Once you have your ML framework in place, you can focus on optimizing the simple ML models with different objective functions, hyperparameter search, feature engineering, more data, and ensembles.\nPhase 4. Complex models Once you’ve reached the limit of your simple models and your use case demands significant model improvement, experiment with more complex models.\nYou’ll also want to experiment to figure out how quickly your model decays in production (e.g., how often it’ll need to be retrained) so that you can build out your infrastructure to support this retraining requirement.36\nModel Offline Evaluation One common but quite difficult question I often encounter when helping companies with their ML strategies is: “How do I know that our ML models are any good?” In one case, a company deployed ML to detect intrusions to 100 surveillance drones, but they had no way of measuring how many intrusions their system failed to detect, and they couldn’t decide if one ML algorithm was better than another for their needs.\nLacking a clear understanding of how to evaluate your ML systems is not necessarily a reason for your ML project to fail, but it might make it impossible to find the best solution for your need, and make it harder to convince your managers to adopt ML. You might want to partner with the business team to develop metrics for model evaluation that are more relevant to your company’s business.37\nIdeally, the evaluation methods should be the same during both development and production. But in many cases, the ideal is impossible because during development, you have ground truth labels, but in production, you don’t.\nFor certain tasks, it’s possible to infer or approximate labels in production based on users’ feedback, as covered in the section “Natural Labels”. For example, for the recommendation task, it’s possible to infer if a recommendation is good by whether users click on it. However, there are many biases associated with this.\nFor other tasks, you might not be able to evaluate your model’s performance in production directly and might have to rely on extensive monitoring to detect changes and failures in your ML system’s performance. We’ll cover monitoring in Chapter 8.\nOnce your model is deployed, you’ll need to continue monitoring and testing your model in production. In this section, we’ll discuss methods to evaluate your model’s performance before it’s deployed. We’ll start with the baselines against which we will evaluate our models. Then we’ll cover some of the common methods to evaluate your model beyond overall accuracy metrics.\nBaselines Someone once told me that her new generative model achieved the FID score of 10.3 on ImageNet.38 I had no idea what this number meant or whether her model would be useful for my problem.\nAnother time, I helped a company implement a classification model where the positive class appears 90% of the time. An ML engineer on the team told me, all excited, that their initial model achieved an F1 score of 0.90. I asked him how it was compared to random. He had no idea. It turned out that because for his task the POSITIVE class accounts for 90% of the labels, if his model randomly outputs the positive class 90% of the time, its F1 score would also be around 0.90.39 His model might as well be making predictions at random.40\nEvaluation metrics, by themselves, mean little. When evaluating your model, it’s essential to know the baseline you’re evaluating it against. The exact baselines should vary from one use case to another, but here are the five baselines that might be useful across use cases:\nRandom baseline If our model just predicts at random, what’s the expected performance? The predictions are generated at random following a specific distribution, which can be the uniform distribution or the task’s label distribution.\nFor example, consider the task that has two labels, NEGATIVE that appears 90% of the time and POSITIVE that appears 10% of the time. Table 6-2 shows the F1 and accuracy scores of baseline models making predictions at random. However, as an exercise to see how challenging it is for most people to have an intuition for these values, try to calculate these raw numbers in your head before looking at the table.\nTable 6-2. F1 and accuracy scores of a baseline model predicting at random Random distribution Meaning F1 Accuracy Uniform random Predicting each label with equal probability (50%) 0.167 0.5 Task’s label distribution Predicting NEGATIVE 90% of the time, and POSITIVE 10% of the time 0.1 0.82 Simple heuristic Forget ML. If you just make predictions based on simple heuristics, what performance would you expect? For example, if you want to build a ranking system to rank items on a user’s newsfeed with the goal of getting that user to spend more time on the newsfeed, how much time would a user spend if you just rank all the items in reverse chronological order, showing the latest one first?\nZero rule baseline The zero rule baseline is a special case of the simple heuristic baseline when your baseline model always predicts the most common class.\nFor example, for the task of recommending the app a user is most likely to use next on their phone, the simplest model would be to recommend their most frequently used app. If this simple heuristic can predict the next app accurately 70% of the time, any model you build has to outperform it significantly to justify the added complexity.\nHuman baseline In many cases, the goal of ML is to automate what would have been otherwise done by humans, so it’s useful to know how your model performs compared to human experts. For example, if you work on a self-driving system, it’s crucial to measure your system’s progress compared to human drivers, because otherwise you might never be able to convince your users to trust this system. Even if your system isn’t meant to replace human experts and only to aid them in improving their productivity, it’s still important to know in what scenarios this system would be useful to humans.\nExisting solutions In many cases, ML systems are designed to replace existing solutions, which might be business logic with a lot of if/else statements or third-party solutions. It’s crucial to compare your new model to these existing solutions. Your ML model doesn’t always have to be better than existing solutions to be useful. A model whose performance is a little bit inferior can still be useful if it’s much easier or cheaper to use.\nWhen evaluating a model, it’s important to differentiate between “a good system” and “a useful system.” A good system isn’t necessarily useful, and a bad system isn’t necessarily useless. A self-driving vehicle might be good if it’s a significant improvement from previous self-driving systems, but it might not be useful if it doesn’t perform at least as well as human drivers. In some cases, even if an ML system drives better than an average human, people might still not trust it, which renders it not useful. On the other hand, a system that predicts what word a user will type next on their phone might be considered bad if it’s much worse than a native speaker. However, it might still be useful if its predictions can help users type faster some of the time."
  },
  {
    "objectID": "ds_lifecycle/Model_Evaluation.html#evaluation-methods-1",
    "href": "ds_lifecycle/Model_Evaluation.html#evaluation-methods-1",
    "title": "My Datascience Journey",
    "section": "Evaluation Methods",
    "text": "Evaluation Methods\nIn academic settings, when evaluating ML models, people tend to fixate on their performance metrics. However, in production, we also want our models to be robust, fair, calibrated, and overall make sense. We’ll introduce some evaluation methods that help with measuring these characteristics of a model.\nPerturbation tests A group of my students wanted to build an app to predict whether someone has COVID-19 through their cough. Their best model worked great on the training data, which consisted of two-second long cough segments collected by hospitals. However, when they deployed it to actual users, this model’s predictions were close to random.\nOne of the reasons is that actual users’ coughs contain a lot of noise compared to the coughs collected in hospitals. Users’ recordings might contain background music or nearby chatter. The microphones they use are of varying quality. They might start recording their coughs as soon as recording is enabled or wait for a fraction of a second.\nIdeally, the inputs used to develop your model should be similar to the inputs your model will have to work with in production, but it’s not possible in many cases. This is especially true when data collection is expensive or difficult and the best available data you have access to for training is still very different from your real-world data. The inputs your models have to work with in production are often noisy compared to inputs in development.41 The model that performs best on training data isn’t necessarily the model that performs best on noisy data.\nTo get a sense of how well your model might perform with noisy data, you can make small changes to your test splits to see how these changes affect your model’s performance. For the task of predicting whether someone has COVID-19 from their cough, you could randomly add some background noise or randomly clip the testing clips to simulate the variance in your users’ recordings. You might want to choose the model that works best on the perturbed data instead of the one that works best on the clean data.\nThe more sensitive your model is to noise, the harder it will be to maintain it, since if your users’ behaviors change just slightly, such as they change their phones, your model’s performance might degrade. It also makes your model susceptible to adversarial attack.\nInvariance tests A Berkeley study found that between 2008 and 2015, 1.3 million creditworthy Black and Latino applicants had their mortgage applications rejected because of their races.42 When the researchers used the income and credit scores of the rejected applications but deleted the race-identifying features, the applications were accepted.\nCertain changes to the inputs shouldn’t lead to changes in the output. In the preceding case, changes to race information shouldn’t affect the mortgage outcome. Similarly, changes to applicants’ names shouldn’t affect their resume screening results nor should someone’s gender affect how much they should be paid. If these happen, there are biases in your model, which might render it unusable no matter how good its performance is.\nTo avoid these biases, one solution is to do the same process that helped the Berkeley researchers discover the biases: keep the inputs the same but change the sensitive information to see if the outputs change. Better, you should exclude the sensitive information from the features used to train the model in the first place.43\nDirectional expectation tests Certain changes to the inputs should, however, cause predictable changes in outputs. For example, when developing a model to predict housing prices, keeping all the features the same but increasing the lot size shouldn’t decrease the predicted price, and decreasing the square footage shouldn’t increase it. If the outputs change in the opposite expected direction, your model might not be learning the right thing, and you need to investigate it further before deploying it.\nModel calibration Model calibration is a subtle but crucial concept to grasp. Imagine that someone makes a prediction that something will happen with a probability of 70%. What this prediction means is that out of all the times this prediction is made, the predicted outcome matches the actual outcome 70% of the time. If a model predicts that team A will beat team B with a 70% probability, and out of the 1,000 times these two teams play together, team A only wins 60% of the time, then we say that this model isn’t calibrated. A calibrated model should predict that team A wins with a 60% probability.\nTo measure a model’s calibration, a simple method is counting: you count the number of times your model outputs the probability X and the frequency Y of that prediction coming true, and plot X against Y. The graph for a perfectly calibrated model will have X equal Y at all data points. In scikit-learn, you can plot the calibration curve of a binary classifier with the method sklearn.calibration.calibration_curve,\nTo calibrate your models, a common method is Platt scaling, which is implemented in scikit-learn with sklearn.calibration.CalibratedClassifierCV. Another good open source implementation by Geoff Pleiss can be found on GitHub. For readers who want to learn more about the importance of model calibration and how to calibrate neural networks, Lee Richardson and Taylor Pospisil have an excellent blog post based on their work at Google.\nconfidence measurement * Confidence measurement can be considered a way to think about the usefulness threshold for each individual prediction. Indiscriminately showing all a model’s predictions to users, even the predictions that the model is unsure about, can, at best, cause annoyance and make users lose trust in the system * confidence measurement is a metric for each individual sample.\nslice based evaluation * Slicing means to separate your data into subsets and look at your model’s performance on each subset separately. * To overcome Simpson’s paradox - aggregation can conceal and contradict actual situations\nHow to identify critical slices * Heuristics-based * Error Analysis - Manually go through misclassified examples and find patterns among them * Slice Finder * slice Finder: Automated data slicing for model validation"
  },
  {
    "objectID": "ds_lifecycle/Training_Data.html",
    "href": "ds_lifecycle/Training_Data.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "It’s a good practice to keep track of the origin of the data samples and labels\n\n\n\n\nNatural ground truth labels\nFeedback loop length is important to consider\n\n\n\n\n\nweak supervision\nSemi-supervision\nTransfer learning\nActive learning\n\n\n\n\n\n\nSnorkel\nHeuristics are used to develop the label data which are called label fuctions (LF)\nLFs are combined, denoised and reweighted to get a set of most likely correct labels\n\n\n\n\n\n\n\nLeverages structural assumptions to generate new labels based on a small set of initial labels\nSelf-training - Train a model on existing set of labeled data and use this model to make predictions for unlabeled samples\nA semi-supervision method that has gained popularity in recent years is the perturbation-based method. It’s based on the assumption that small perturbations to a sample shouldn’t change its label. So you apply small perturbations to your training instances to obtain new training instances.\nliterature survey\n\n\n\n\n\nInstead of randomly labeling data samples, you label the samples that are most helpful to your models according to some metrics or heuristics. The most straightforward metric is uncertainty measurement—label the examples that your model is the least certain about, hoping that they will help your model learn the decision boundary better.\nliterature survey\n\n\n\n\n\nUsing the right evaluation metrics\n\nUse precision-recall curve in place of ROC curve for imbalance datasets\n\nChanging the data distribution\n\nTomek links - A popular method of undersampling low-dimensional data that was developed back in 1976 is Tomek links.With this technique, you find pairs of samples from opposite classes that are close in proximity and remove the sample of the majority class in each pair.\noversampling low-dimensional data - SMOTE (synthetic minority oversampling technique)\nWhen you resample your training data, never evaluate your model on resampled data, since it will cause your model to overfit to that resampled distribution\nUndersampling runs the risk of losing important data from removing data. Oversampling runs the risk of overfitting on training data, especially if the added copies of the minority class are replicas of existing data.\nTwo phase learning - Train the model on resampled data by undersampling large classes until each class has only N instances. Then fine-tune the model on the original data\nDynamic sampling - oversample the low-performing classes and undersample the high-performing classes during the training process.The method aims to show the model less of what it has already learned and more of what it has not.\n\nAlgorithm-level methods\n\nAdjustment to the loss function\nCost-sensitive learning\n\nClass-balanced loss\nFocal loss\n\nIf a sample has a lower probability of being right, it’ll have a higher weight\n\n\n\n\n\n\n\nUseful even when we have a lot of data, can make our models more robust to noise and adversarial attacks\nSimple label-preserving transformations\nPerturbations - NN are sensitive to noise. 67.97% of the natural images in the Kaggle CIFAR-10 test dataset and 16.04% of the ImageNet test images can be misclassified by changing just one pixel. Adding noisy samples to training data can help models recognize the weak spots in their learned decision boundary and improve their performance. Moosavi-Dezfooli et al. proposed an algorithm, called DeepFool, that finds the minimum possible noise injection needed to cause a misclassification with high confidence.\nData synthesis - In computer vision, a straightforward way to synthesize new data is to combine existing examples with discrete labels to generate continuous labels. Consider a task of classifying images with two possible labels: DOG (encoded as 0) and CAT (encoded as 1). From example _x_1 of label DOG and example _x_2 of label CAT, you can generate x’ such as:\n\nx’=γx1+(1-γ)x2\nThe label of x’ is a combination of the labels of _x_1 and _x_2: γ×0+(1-γ)×1. This method is called mixup. The authors showed that mixup improves models’ generalization, reduces their memorization of corrupt labels, increases their robustness to adversarial examples, and stabilizes the training of generative adversarial networks."
  },
  {
    "objectID": "ds_lifecycle/Regularization.html",
    "href": "ds_lifecycle/Regularization.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "L2 regularization or squared\nWe add penalty term along with SSR to optimize \nRigde regression does not make slope of any variable to zero\nIf a variable is not very important then it’s slope will be closer to zero and its parameter will be shrinked\nRidge regression is useful when all the variables in a model are useful\n\n\n\n\n\nL1 regularization or absolute\nWe add absolute value of the slope to the SSR to optimize\nThe value of slope can become zero\nWe can use lasso where unimportant variables are included in the model\n\n\nUse both Lasso and Ridge regression to get best of the both worlds\n\nElastic Net combines both types of regularization"
  },
  {
    "objectID": "ds_lifecycle/gradient_descent.html",
    "href": "ds_lifecycle/gradient_descent.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Gradient Descent\n\nThe algorithm estimates the direction of steepest descent by computing the gradient of the loss function. The gradient points uphill, so the algorithm steps in the opposite direction by subtracting a fraction of the gradient\nToo bad your phone is out of juice, because the algorithm may not have propelled you to the bottom of a convex mountain. Instead, you may be stuck in a nonconvex landscape of multiple valleys (local minima), peaks (local maxima), saddles (saddle points), and plateaus. In fact, tasks like image recognition, text generation, and speech recognition are nonconvex, and many variations on gradient descent have emerged to handle such situations. For example, the algorithm may have momentum that helps it zoom over small rises and dips, giving it a better chance at arriving at the bottom. Luckily, local and global minima tend to be roughly equivalent."
  },
  {
    "objectID": "ds_lifecycle/Model_Development.html",
    "href": "ds_lifecycle/Model_Development.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "When considering what model to use, it’s important to consider not only the model’s performance, measured by metrics such as accuracy, F1 score, and log loss, but also its other properties, such as how much data, compute, and time it needs to train, what’s its inference latency, and interpretability.\nAvoid the state-of-the-art trap\n\nResearchers often only evaluate models in academic settings, which means that a model being state of the art often means that it performs better than existing models on some static datasets. It doesn’t mean that this model will be fast enough or cheap enough for you to implement. It doesn’t even mean that this model will perform better than other models on your data.\n\nStart with the simplest models\n\nSimplest models are not always the same as models with the least effort. For example, pretrained BERT models are complex, but they require little effort to get started with, especially if you use a ready-made implementation like the one in Hugging Face’s Transformer. In this case, it’s not a bad idea to use the complex solution, given that the community around this solution is well developed enough to help you get through any problems you might encounter. However, you might still want to experiment with simpler solutions to ensure that pretrained BERT is indeed better than those simpler solutions for your problem. Pretrained BERT might be low effort to start with, but it can be quite high effort to improve upon. Whereas if you start with a simpler model, there’ll be a lot of room for you to improve upon your model.\n\nAvoid human biases in selecting models\nEvaluate good performance now versus good performance later\n\nWhile evaluating models, you might want to take into account their potential for improvements in the near future, and how easy/difficult it is to achieve those improvements.\nA situation that I’ve encountered is when a team evaluates a simple neural network against a collaborative filtering model for making recommendations. When evaluating both models offline, the collaborative filtering model outperformed. However, the simple neural network can update itself with each incoming example, whereas the collaborative filtering has to look at all the data to update its underlying matrix. The team decided to deploy both the collaborative filtering model and the simple neural network. They used the collaborative filtering model to make predictions for users, and continually trained the simple neural network in production with new, incoming data. After two weeks, the simple neural network was able to outperform the collaborative filtering model.\n\nEvaluate trade-offs\nUnderstand the model’s assumptions\n\n\n\n\n\n\n\nBagging, shortened from bootstrap aggregating, is designed to improve both the training stability and accuracy of ML algorithms. It reduces variance and helps to avoid overfitting.\n\n\n\n\n\n\nBoosting is a family of iterative ensemble algorithms that convert weak learners to strong ones. Each learner in this ensemble is trained on the same set of samples, but the samples are weighted differently among iterations. As a result, future weak learners focus more on the examples that previous weak learners misclassified.\n\n\n\n\n\n\n\n\n\n\n\nKaggle Ensembling guide\n\n\n\nStart simple and gradually add more components\nOverfit a single batch - If it’s for image recognition, overfit on 10 images and see if you can get the accuracy to be 100%, or if it’s for machine translation, overfit on 100 sentence pairs and see if you can get to a BLEU score of near 100. If it can’t overfit a small amount of data, there might be something wrong with your implementation.\nSet a random seed\n\n\n\n\n\nGradient-checkpointing\n\n\n\n\nEach worker has its own copy of the whole model and does all the computation necessary for its copy of the model\nSynchronous SGD and Asynchronous SGD - The way in which gradients are combined in parallel training\n\n\n\n\n\n\n Model parallelism is when different components of your model are trained on different machines\n\n\n\n\n\n\n\n\n\n\n\nAutoML: Methods, Systems, Challenges\n\n\n\n\nwhat if we replace the functions that specify the update rule with a neural network? How much to update the model’s weights will be calculated by this neural network. This approach results in learned optimizers, as opposed to hand-designed optimizers."
  },
  {
    "objectID": "ds_lifecycle/Sampling.html",
    "href": "ds_lifecycle/Sampling.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Two families of sampling\n\nnonprobability sampling\nrandom sampling (probability based sampling)\n\nNonprobability sampling\n\nconvenience sampling - selection based on availability\nsnowball sampling - Future samples selected based on exisitng samples\nJudgment sampling\nQuota sampling - You select samples based on quotas for certain slices of data without any randomization\n\nRandom sampling\n\nsimple random sampling - All samples in the population equal probability of being selected\nStratified sampling - to sample 1% of data that has two classes, A and B, you can sample 1% of class A and 1% of class B. Challenging in case of multilabel tasks\n\nWeighted sampling\n\nIn weighted sampling, each sample is given a weight, which determines the probability of it being selected.\n\n\n# Choose two items from the list such that 1, 2, 3, 4 each has\n# 20% chance of being selected, while 100 and 1000 each have only 10% chance.\nimport random\nrandom.choices(population=[1, 2, 3, 4, 100, 1000],\n               weights=[0.2, 0.2, 0.2, 0.2, 0.1, 0.1],\n               k=2)\n# This is equivalent to the following\nrandom.choices(population=[1, 1, 2, 2, 3, 3, 4, 4, 100, 1000],\n               k=2)\n\nReservoir sampling - useful when have to deal with streaming data"
  },
  {
    "objectID": "ds_lifecycle/MLOPS.html",
    "href": "ds_lifecycle/MLOPS.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "MlOps\nData science specific orchestrators\n\nAirflow\nArgo\nPrefect\nDagster\n\nScheduler is different from Orchestrator Scheduler is high level - If schedulers are concerned with when to run jobs and what resources are needed to run those jobs, orchestrators are concerned with where to get those resources. Schedulers deal with job-type abstractions such as DAGs, priority queues, user-level quotas (i.e., the maximum number of instances a user can use at a given time), etc. Orchestrators deal with lower-level abstractions like machines, instances, clusters, service-level grouping, replication, etc. If the orchestrator notices that there are more jobs than the pool of available instances, it can increase the number of instances in the available instance pool."
  },
  {
    "objectID": "ds_lifecycle/Model_Deployment.html",
    "href": "ds_lifecycle/Model_Deployment.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Model Deployment\n\nThree main modes of prediction\n\n\n\n\n\nUnifying Batch and streaming pipeline\n\n\n\n\nModel Compression\n\nLow rank Factorization\n\nReplace high-dimensional tensors with lower-dimensional tensors Ex - compact convolutional filters\n\n\nKnowledge distillation\n\nA small model (student) is trained to mimic a larger model or ensemble of models. The advantage of this approach is that it can work regardless of the architectural differences between the teacher and the student networks.\n\nPruning\n\nIn neural networks it means either removing the nodes there by altering the architecture. Or keeping the architecture same and driving the non-important parameters to zero.\n\nQuantization\n\nQuantization reduces a model’s size by using fewer bits to represent its parameters.\n\n\n\nCompiling for Edge devices\n\n\n\nModel Optimization\n\nThere are two ways to optimize your ML models: locally and globally. Locally is when you optimize an operator or a set of operators of your model. Globally is when you optimize the entire computation graph end to end.\nLocal optimization techniques\n\nVectorization\nParallelization\nLoop tiling\nOperator fusion"
  },
  {
    "objectID": "ds_lifecycle/Monitoring_ML_Models_in_production.html",
    "href": "ds_lifecycle/Monitoring_ML_Models_in_production.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Dependency failure\nDeployment failure\nHardware failures\nDowntime or crashing\n\n\n\n\n\nData distribution shifts in production\nEdge cases\nDegenerate feedback loops\n\nwe can detect degenerate feedback loops by measuring the popularity diversity of a system’s output in recommenders. Other metrics - Aggregate diversity, average coverage of long-tail items can help measure the diversity of outputs of a recommender system\nIn 2021, Chia et al. went a step further and proposed the measurement of hit rate against popularity. They first divided items into buckets based on their popularity—e.g., bucket 1 consists of items that have been interacted with less than 100 times, bucket 2 consists of items that have been interacted with more than 100 times but less than 1,000 times, etc. Then they measured the prediction accuracy of a recommender system for each of these buckets. If a recommender system is much better at recommending popular items than recommending less popular items, it likely suffers from popularity bias. Once your system is in production and you notice that its predictions become more homogeneous over time, it likely suffers from degenerate feedback loops.\n\nCorrecting degenerate feedback loops\n\nIntroducing randomization in the predictions can reduce homogeneity\nEach new video is randomly assigned an initial pool of traffic (which can be up to hundreds of impressions). This pool of traffic is used to evaluate each video’s unbiased quality to determine whether it should be moved to a bigger pool of traffic or be marked as irrelevant.\nTo exclude positional bias, we may need to encode the position information using positional features.\nDuring inference, you want to predict whether a user will click on a song regardless of where the song is recommended, so you might want to set the 1st Position feature to be False. Then you look at the model’s predictions for various songs for each user and can choose the order in which to show each song."
  },
  {
    "objectID": "ds_lifecycle/Monitoring_ML_Models_in_production.html#data-distribution-shifts",
    "href": "ds_lifecycle/Monitoring_ML_Models_in_production.html#data-distribution-shifts",
    "title": "My Datascience Journey",
    "section": "Data distribution shifts",
    "text": "Data distribution shifts\n\nConcept Drift\nCovariate shift\nLabel shift\n\nP(X) - probability density of the input P(Y) - probability density of the output\n\n\n\nCovariate Shift\n\n\nDuring model development, covariate shifts can happen due to biases during the data selection process\nCovariate shifts can also happen because the training data is artifically altered to make it easier for the model to learn, Ex - oversampling or undersampling the imbalanced datasets\nIn production, this can happen because of major changes in environment or in the way application is used\n\n\n\nLabel Shift\n\nRemember that covariate shift is when the input distribution changes. When the input distribution changes, the output distribution also changes, resulting in both covariate shift and label shift happening at the same time. Because label shift is closely related to covariate shift, methods for detecting and adapting models to label shifts are similar to covariate shift adaptation methods. We’ll discuss them more later in this chapter.\n\n\n\nConcept Drift\n\nConsider you’re in charge of a model that predicts the price of a house based on its features. Before COVID-19, a three-bedroom apartment in San Francisco could cost $2,000,000. However, at the beginning of COVID-19, many people left San Francisco, so the same apartment would cost only $1,500,000. So even though the distribution of house features remains the same, the conditional distribution of the price of a house given its features has changed.\nConcept drift is seasonal\n\n\n\nDetecting Data Distribution Shifts\n\nAre the model’s performance degrading?\nWhen ground truth labels are unavailable or too delayed, we can monitor other distributions of interest - P(X)\nMonitor P(X), P(Y), P(X|y), P(y|X) when Y is available\n\n\nStatistical Methods\n\nCompute the stats of the values of feature during inference and compare them to the metrics computed during training - Min, Max, Mean, Median, Variance, Quantiles, Skewness, Kurtosis etc\nUse two-sample hypothesis test to test if the difference between two populations is statistically significant.\n\nAlibi Detect is an open-source package for drift detection.\n\n\n\n\nMonitoring ML Specific Metrics\n\nAccuracy related metrics\nPredictions\n\nMonitor predictions for distribution shifts\nPrediction distribution shifts are also a proxy for input distribution shifts\nMonitor predictions for anything odd happening, such as predicting an unusual number of False in a row\n\nMonitoring features\n\nFeature validation: ensuring that features follow an expected schema\nGreat Expectations and Deequ are open-source packages to do data validation\n\n\n\n\nMonitoring Raw inputs"
  },
  {
    "objectID": "ds_lifecycle/Monitoring_ML_Models_in_production.html#resources",
    "href": "ds_lifecycle/Monitoring_ML_Models_in_production.html#resources",
    "title": "My Datascience Journey",
    "section": "Resources",
    "text": "Resources\nDataset shift in Machine learning by Quinonero-Candela"
  },
  {
    "objectID": "ds_lifecycle/ML_Algorithms.html",
    "href": "ds_lifecycle/ML_Algorithms.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Ordered logistic regression in which the outcomes are ordered values\nMulti-class classification is also called multinomial or sotmax regression.\n\nLogistic Slides\nyoutube video\n\nLogit (log of odds) function takes input values in the range of 0 to 1 and transforms them into values over the entire real-number range\nUnder the logistic model, we assume there is a linear relationship between the weighted inputs and the log-odds      \n\n\n\n\n\n\n\nHandle both categorical and continuous data\nLeaves that contain mixtures of classifications are called Impure\nGini Impurity, Entropy and Information Gain are used to measure the impurity of the trees\n\n\nThe lower the Gini Impurity of a variable, the better it is at prediction\nIn case of continuous variables, we sort the numbers and take the average of the two consequtive numbers. The average will be the threshold which will be used for building the tree. The average number with low gini impurity will be preferred\n\n\n\n\n\n\nK-medoids use actual data points as centroids rather than mean positions in a given cluster. The medoids are points that minimize the distance to all other points in their cluster. This variation is more interpretable because the centroids are always data points\nFuzzy C-Means Clustering enables the data points to participate in multiple clusters to varying degrees. It replaces hard cluster assignments with degrees of membership depending on distance from the centroids"
  },
  {
    "objectID": "data_engineering/azure/synapse_analytics.html",
    "href": "data_engineering/azure/synapse_analytics.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "A centralized service for data storage and processing with an extensible architecture through which linked services enable you to integrate commonly used data stores, processing platforms, and visualization tools.\n\n\n\n\nWorking with files in a data lake\nIngesting and transforming data with pipelines\nAzure Synapse Analytics includes built-in support for creating, running, and managing pipelines that orchestrate the activities necessary to retrieve data from a range of sources, transform the data as required, and load the resulting transformed data into an analytical store\nQuerying and manipulating data with SQL through two kinds of SQL pool\n\nA built-in serverless pool that is optimized for using relational SQL semantics to query file-based data in a data lake.\nCustom dedicated SQL pools that host relational data warehouses.\n\nProcessing and analyzing data with Apache Spark\n\nExploring data with Data Explorer\nIntegration with other Azure data services\n\nAzure Synapse Link enables near-realtime synchronization between operational data in Azure Cosmos DB, Azure SQL Database, SQL Server, and Microsoft Power Platform Dataverse and analytical data storage that can be queried in Azure Synapse Analytics.\nMicrosoft Power BI\nMicrosoft Purview (data lineage and catalog of data assets)\nAzure ML\n\n\n\n\n\n\n\nLarge scale data warehousing\nAdvance analytics\nData exploration and discovery\nReal time analytics\nData integration\nIntegrated analytics"
  },
  {
    "objectID": "data_engineering/azure/azure_data_lake_storage_gen2.html",
    "href": "data_engineering/azure/azure_data_lake_storage_gen2.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "File based storage, in a distributed file system that supports high scalability for massive volumes of data.\nIt combines a file system with a storage platform\nIt builds on Azure blob storage capabilities to optimize it specifically for analytics workload.\nIt can be used for both real-time and batch solutions\nIt supports ACLs and POSIX permissions. Permissions can be set at a directory level or file level\nIt organizes the stored data into a hierarchy of directories and subdirectories, much like a file system\nProvides data redundancy\nHierarchical namespace option should be enabled to use Azure Data Lake Storage Gen2`\n\n\n\n\nIn Azure Blob storage, you can store large amounts of unstructured (“object”) data in a flat namespace within a blob container. Blob names can include “/” characters to organize blobs into virtual “folders”, but in terms of blob manageability the blobs are stored as a single-level hierarchy in a flat namespace.\nAzure Data Lake Storage Gen2 builds on blob storage and optimizes I/O of high-volume data by using a hierarchical namespace that organizes blob data into directories, and stores metadata about each directory and the files within it. This structure allows operations, such as directory renames and deletes, to be performed in a single atomic operation. Flat namespaces, by contrast, require several operations proportionate to the number of objects in the structure. Hierarchical namespaces keep the data organized, which yields better storage and retrieval performance for an analytical use case and lowers the cost of analysis.\nIf analysis is not needed, then blob storage is a better choice"
  },
  {
    "objectID": "notes.html",
    "href": "notes.html",
    "title": "Notes",
    "section": "",
    "text": "My Notes on Data Science, Machine learning and Artificial Intelligence, I am learning from different books, videos, courses and Notebooks. These pages are summaries, explanations, important points and replication of various source material from which I am learning."
  },
  {
    "objectID": "deep_learning/02_training_tips.html",
    "href": "deep_learning/02_training_tips.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Tips to Train Your Model\n\nFor images, it’s a good idea to normalize the pixel values to the range of -1 and 1. It works better than normalizing the pixel values to the range of 0 and 1."
  },
  {
    "objectID": "deep_learning/01_intro.html",
    "href": "deep_learning/01_intro.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Dot Product\n\nDot product offers a neat way to represent the weighted sum model output\nThe dot product is defined only if the vectors have the same dimensions\nSometimes the dot product is also referred to as inner products\nThe product of a matrix (feature matrix) and column vector (weights) is another vector\n\n\n\n\nMatrix Vector Multiplication\n\n\n\n\n\nMatrix Multiplication\n\n\n\nSquared magnitude or length or L2-Norm of a vector: dot product of the vector with itself. (dot product of the squared difference between target and prediction)\nDot-product between a pair of vectors can be used as a measure of similarity between them. Similar vectors have larger dot product and dissimilar vectors have near zero dot products.\nA component of a vector along another vector is yielded by the dot product. If the vectors point in more or less the same direction their dot products are higher compared to when the vectors are perpendicular to each other. If the vectors point in opposite direction their dot product will be negative\nThe dot product can be expressed using the cosine of the angle between the vectors\n\n\n\n\nExpressing dot product using cosine\n\n\n\nThe dot product between two vectors is also proportional to the lengths of the vectors. If we want the agreement score to be neutral to the vector length, we can use a normalized dot product - between unit length vectors along the same direction. Normalized dot product (cosine similarity) is used for document similarity (we want similarity score to be independent of document length)\n\n\n\n\nNormalized dot product\n\n\n\nTwo vectors are orthogonal if their dot product is zero"
  },
  {
    "objectID": "study.html",
    "href": "study.html",
    "title": "Study",
    "section": "",
    "text": "A Study undertook to eradicate poverty using Data science and Machine learning"
  },
  {
    "objectID": "ml_optimization_algorithms/gradient_descent.html",
    "href": "ml_optimization_algorithms/gradient_descent.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Gradient Descent\n\n\n\nDifferent initializations will cause gradient descent to converge to different local minimizers"
  },
  {
    "objectID": "python_packages/create_python_package.html",
    "href": "python_packages/create_python_package.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Github repo for the Python Package\nLink to Python Package\nDocumentation for the Python Package\n\n\n\n\nchapter 3 of the book python packages"
  },
  {
    "objectID": "Interview_resources/resources.html",
    "href": "Interview_resources/resources.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "The Data Science Interview Book\nThe Illustrated Machine Learning\n\n\n\n\nHow Adaboost works\nDifference between Adaboost and Gradient Boosting\nHow Catboost handle categorical variables\nWhat are the tricks used by lightgbm to handle large datasets\nwhat is back propagation\nwhat is exploding and vanishing gradients\nCan we get derivative of RELU at all points? If not how is it overcome in practical applications?\nHow RNN and LSTM works? what are their disadvantages?\nHow transformers work?\nExplain how BERT works?\nwhy we divide by square root of dimensions in transformers scaled dot product attention?\nwhat is label smoothing?\nHow the weights are initialized in deep learning? what different methods are there for weight initialization?\nWhat optimization techniques are there for deep learning?\nExplain how single shot object detection works?\nwhat’s the advantage of skip connections in Resnet?\nHow openAI CLIP works?\nWhat are Anchor boxes and how are they selected for object detection?\nL1 and L2 regularization\nFeature selection methods\nHow embeddings are created for words\nHow to handle quadratic explosion if we want to handle high definition Images using transformer models?\nSome puzzles on probability, which I dont remember now\nWhat is maximum likelihood\nwhat is the difference between args and kwargs in python\nExplain try-except-finally in python\nwhat is GIL in python\nDifference between self and cross attention"
  },
  {
    "objectID": "research/unknown.html",
    "href": "research/unknown.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "What is warmup ratio\nsoftmax temperature\nWhat is dice loss\n\n\nResearch papers to read and summarize\nGoogle MusicLM []\nSWIN DETR ConvNeXt DiNAT MSDeformAttn Dilated Neighborbood Attention"
  },
  {
    "objectID": "unsupervised_learning/00_intro.html",
    "href": "unsupervised_learning/00_intro.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Dimensionality Reduction\nDimensionality reduction can be categorised as follows:-\n* unsupervised / supervised (PCA / LDA)\n* linear / non-linear (PCA / Kernel PCA)\n* parametric / non-parametric (PCA / MDS)"
  },
  {
    "objectID": "unsupervised_learning/02_svd.html",
    "href": "unsupervised_learning/02_svd.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Any linear transformation in a 2D space can be achieved through three fundamental operations: rotation, scaling (or stretching), and another rotation. This is known as the “rotation-stretching-rotation” theorem, or the “Singular Value Decomposition” (SVD) theorem.\nMore generally, in an n-dimensional space, a linear transformation can be decomposed into a sequence of at most n rotations, followed by a scaling transformation, and then another sequence of at most n rotations.\nIn SVD, we try to reduce the dimensions of the data by transforming the non-singular matrix with higer rank into matrices of lower rank. (Thereby helping us to drop the redudant data)\nSpecifically, we decompose the original matrix A into three matrices: U, Σ, and V, where U and V are orthogonal matrices, and Σ is a diagonal matrix of singular values. The singular values in Σ provide information about the importance of each dimension or feature in the original data, and they are sorted in decreasing order.\nBy keeping only the top k singular values and their corresponding columns in U and V, we can reduce the dimensionality of the data to k dimensions. This is a form of matrix approximation, where we construct a lower-rank approximation of the original matrix that captures the most important information in the data.\nThe lower-rank approximation consists of the product of the k leading columns of U, the k leading singular values in Σ, and the k leading rows of V. This approximation has lower rank than the original matrix, and it captures most of the variation in the data. By keeping only the top k singular values and vectors, we effectively reduce the dimensionality of the data from the original rank to k, while retaining most of the information.\nIn SVD (Singular Value Decomposition), if we consider all the singular values and perform the matrix multiplication, we will get the original matrix without loss of information. The SVD decomposition of a matrix A is given by:\n\\(A = U \\Sigma V^T\\)\nwhere U and V are orthogonal matrices, and Σ is a diagonal matrix with the singular values of A on its diagonal. The singular values are arranged in decreasing order.\nIf we multiply U, Σ, and V^T back together, we obtain:\n\\(A' = U \\Sigma V^T\\)\nwhich is the reconstructed matrix A’. If we use all the singular values in Σ, then A’ will be equal to A, and there will be no loss of information.\nHowever, if we use only a subset of the singular values (i.e., truncate Σ), then A’ will be a lower-rank approximation of A, and there will be some loss of information. The amount of information loss depends on how many singular values we truncate and how important those singular values are to the overall structure of the data. Therefore, if we use all the singular values in SVD and perform the matrix multiplication, we will get the original matrix without loss of information.\n\n\n\n\nLuis Serrano youtube video on SVD"
  },
  {
    "objectID": "unsupervised_learning/03_lda.html",
    "href": "unsupervised_learning/03_lda.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "LDA Blueprint\n\n\n\n\n\nProbability of a document\n\n\n\n\n\nProbability of a document\n\n\n\n\n\nLDA Blueprint with what they are doing\n\n\n\n\n\n\n\nIn this distribution we have a parameter `alpha’. Depending on the values of alpha we will get different distributions.\n\n\n\nDirichlet distributions based on alpha values\n\n\nwe consider two Dirichlet distributions - One which will associate document with topics and the other topics with words\n\n\n\nTwo Dirichlet distributions\n\n\nThese two Dirichlet distributions are the parameters in the blueprint. We generate different documents by adjusting the points in these two distributions\n\n\n\nTwo Dirichlet distributions as knobs in the blueprint\n\n\n\n\n\n\n\n\n\n\nLDA in action\n\n\nWe generate documents by assigning documents to topics and topics to words. The probability of generating the same article as training data will be very low.\n\n\n\nGenerating documents with assigning of topics for different documents and assigning topics to different words\n\n\n\n\n\nComparing the probability of the document generated with the ground-truth\n\n\n\n\n\n\n\nThe number of topcis is a hyperparameter\n\n\n\n\n\nGibbs Sampling in the context of LDA is trying to tag the words in the document to be monochromatic (belonging to a single category) and trying to tag the document to be monochromatic\n\n\n\nGibbs Sampling\n\n\n\n\n\nEnsuring we are considering all the topics are considered in Gibbs sampling\n\n\n\n\n\nAssigning to topics to documents based on assigning topics to words\n\n\nMaximizing the probability of the LDA equation is very difficult. Hence we use Gibbs sampling\n\n\n\n\n\nGibbs sampling is a statistical algorithm used to generate samples from a probability distribution that might be too complex to calculate directly. It is often used in Bayesian inference, where the goal is to estimate the unknown parameters of a model given some observed data.\nThe idea behind Gibbs sampling is to iteratively sample from the conditional distributions of each variable in the model, while holding all other variables fixed. This means that we generate a sample for one variable at a time, based on the values of the other variables in the model.\nThe process starts with some initial values for all the variables in the model. Then, for each iteration of the algorithm, we randomly select one of the variables and update its value based on the values of the other variables in the model. We keep doing this for all the variables until we have generated enough samples.\n\n\n\n\n\n\nHow the length of the document is treated by LDA\n\n\n\n\n\nYoutube video on LDA by Luis Serrano\nPart_2 of the youtube video"
  },
  {
    "objectID": "unsupervised_learning/04_mds.html",
    "href": "unsupervised_learning/04_mds.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "This is a non-parametric method\nArrange the points in 2D such that the pairwise distance between them are preserved\n\nThe loss function for MDS is reducing the original distance between two points in the dataset and the distance between points after arranging the points in 2D\n\n\n\nMDS Loss function\n\n\nIt is difficult to MDS for large datasets as it considers pairwise distances between all points. (Quadratic complexity for memory)\nIt is hard to scale MDS for large datasets\n\n\n\n\nTrying to preserve distances in high-dimensions in low dimensions is not a good idea (curse of dimensionality)\nAs the number of dimensions increases, the mean of the pairwise distances will also increase and we will not find pairwise distances which are closer to zero\nFor example, in the below image we are trying to fit the data with green distribution to blue distribution (which is difficult)\n\n\n\n\n\n\nTubingen ML Course"
  },
  {
    "objectID": "unsupervised_learning/05_t-sne.html",
    "href": "unsupervised_learning/05_t-sne.html",
    "title": "My Datascience Journey",
    "section": "",
    "text": "Neighbour Embeddings - Preserve the nearest neighbours instead of preserving distances\n\n\n\n\nLoss function is KL divergence between pairwise similarities in the high-dimensional and in the low-dimensional spaces. Similarities are defined such that they sum to 1\n\nHigh price for putting close neighbors in high dimensions far away\n\nIf the distance between points high then their similarity is less and vice versa\n\n\n\n\n\n\n\n\n\n\nKernel width by default is chosen such that 30 values are large and others should be zero (perplexity - 30)\n\n\n\n\n\n\n\n\n\n\n\n\nwe can use Gradient Descent for optimizing the loss\nThis works as a many body simulation: close neighbors attract each other while all points repulse each other\n\n\n\n\n\n\n\nDuring the early stages of iteration to keep the similar points together, the attractive forces between nearby points are amplified, making them more dominant than the repulsive forces between distant points. (if this is not done, then it was observed that the quality of the output is not good)\nAfter a few steps, early exaggeration is turned off, then repulsion will expand the clusters\n\n\n\n\n\nVanilla T-SNE is quadratic complexity\nTo speed up we need to take care of Attractive and repulsive forces\n\n\n\n\nUse a small number of non-zero affinities i.e. sparse k-nearest neighbour (KNN) graph.\nUsing approximate KNN\n\n\n\n\n\n\n\n\nWe replace a grid of points by its representative point\n\n\nUsing approximate repulsive forces\nBarnes-Hut t-SNE O(nlog(n))\nFFT-accelerated interpolation based t-SNE - O(n)\nNoise contrastive estimation / negative sampling - O(n)\n\n\n\n\n\n\n\n\nPerplexity is the ‘effective’ number of neighbors considered for similarity. Default is 30\n\n\n\nt-SNE with different perplexities\n\n\nMuch smaller values of perplexity is not useful\nMuch larger values are computationally prohibitive\n\n\n\n\n\nKernel - Heavy tailed kernel provide more fain grained cluster results\nt-SNE uses Cauchy kernel (distribution with heavier tails)\n\n\n\n\n\nt-SNE preserves local structure (neighbours) but often struggle to preserve global structure. The loss function has many local minima and intialization can play a large role\nWe can use PCA as initialization for t-SNE\n\n\n\n\n\n\n\n\nAttraction-repulsion spectrum\n\n\nIf the Exaggeration is very high then the white space between clusters is reduced\nIf the Exaggeration is not there then the points in a cluster will end up in different places due to repulsion\n\n\n\n\n\n\nyoutube video"
  }
]