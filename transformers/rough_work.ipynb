{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_embed = 512       # embedding size for the attention modules\n",
    "num_heads = 8       # Number of attention heads\n",
    "num_batches = 1     # number of batches (1 makes it easier to see what is going on)\n",
    "vocab = 50000       # vocab size\n",
    "max_len = 5000      # Max length of TODO what exactly?\n",
    "n_layers = 1        # number of attention layers (not used but would be an expected hyper-parameter)\n",
    "d_ff = 2048         # hidden state size in the feed forward layers\n",
    "epsilon = 1e-6      # epsilon to use when we need a small non-zero number\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x torch.Size([1, 3])\n",
      "y torch.Size([1, 3])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x = torch.tensor([[1, 2, 3]]) # input will be 3 tokens\n",
    "y = torch.tensor([[1, 2, 3]]) # target will be same as the input for many applications\n",
    "x_mask = torch.tensor([[1, 0, 1]]) # Mask the 2nd input token\n",
    "y_mask = torch.tensor([[1, 0, 1]]) # Mask the 2nd target token\n",
    "print(\"x\", x.size())\n",
    "print(\"y\", y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 512])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Make the embedding module. It understands that each token should result in a separate embedding.\n",
    "emb = nn.Embedding(vocab, d_embed)\n",
    "x = emb(x)\n",
    "# Scale the embedding\n",
    "x = x * math.sqrt(d_embed)\n",
    "print(x.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe = torch.zeros(max_len,d_embed, requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5000, 512])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pe.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5000, 1])\n"
     ]
    }
   ],
   "source": [
    "position = torch.arange(0, max_len).unsqueeze(1)\n",
    "print(position.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 512])\n"
     ]
    }
   ],
   "source": [
    "# Start with an empty tensor\n",
    "pe = torch.zeros(max_len, d_embed, requires_grad=False)\n",
    "# array containing index values 0...max_len\n",
    "position = torch.arange(0, max_len).unsqueeze(1)\n",
    "divisor = torch.exp(torch.arange(0, d_embed, 2) * -(math.log(10000.0) / d_embed))\n",
    "# Make overlapping sine and cosine wave inside positional embedding tensor\n",
    "pe[:, 0::2] = torch.sin(position * divisor)\n",
    "pe[:, 1::2] = torch.cos(position * divisor)\n",
    "pe = pe.unsqueeze(0)\n",
    "# Add the position embedding to the main embedding\n",
    "x = x + pe[:, :x.size(1)]\n",
    "print(x.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 512])\n"
     ]
    }
   ],
   "source": [
    "mean = x.mean(-1, keepdim=True)\n",
    "std = x.std(-1, keepdim=True)\n",
    "W1 = nn.Parameter(torch.ones(d_embed))\n",
    "b1 = nn.Parameter(torch.zeros(d_embed))\n",
    "x = W1 * (x - mean) / (std + epsilon) + b1\n",
    "print(x.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q torch.Size([1, 8, 3, 64])\n",
      "x torch.Size([1, 8, 3, 64])\n",
      "v torch.Size([1, 8, 3, 64])\n"
     ]
    }
   ],
   "source": [
    "# Make three versions of x, for the query, key, and value\n",
    "# We don't need to clone because these will immediately go through linear layers, making new tensors\n",
    "k = x # key\n",
    "q = x # query\n",
    "v = x # value\n",
    "# Make three linear layers\n",
    "# This is where the network learns to make scores\n",
    "linear_k = nn.Linear(d_embed, d_embed)\n",
    "linear_q = nn.Linear(d_embed, d_embed)\n",
    "linear_v = nn.Linear(d_embed, d_embed)\n",
    "# We are going to fold the embedding dimensions and treat each fold as an attention head\n",
    "d_k = d_embed // num_heads\n",
    "# Pass q, k, v through their linear layers\n",
    "q = linear_q(q)\n",
    "k = linear_k(k)\n",
    "v = linear_v(v)\n",
    "# Do the fold, treating each h dimenssions as a head\n",
    "# Put the head in the second position\n",
    "q = q.view(num_batches, -1, num_heads, d_k).transpose(1, 2)\n",
    "k = k.view(num_batches, -1, num_heads, d_k).transpose(1, 2)\n",
    "v = v.view(num_batches, -1, num_heads, d_k).transpose(1, 2)\n",
    "print(\"q\", q.size())\n",
    "print(\"x\", k.size())\n",
    "print(\"v\", v.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 64, 3])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.transpose(-2, -1).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores torch.Size([1, 8, 3, 3])\n",
      "attention torch.Size([1, 8, 3, 3])\n",
      "x torch.Size([1, 8, 3, 64])\n"
     ]
    }
   ],
   "source": [
    "d_k = q.size(-1)\n",
    "# Compute the raw scores by multiplying k and q (and normalize)\n",
    "scores = torch.matmul(k, q.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "print(\"scores\", scores.size())\n",
    "# Mask out the scores\n",
    "scores = scores.masked_fill(x_mask == 0, -epsilon)\n",
    "attn = F.softmax(scores, dim = -1)\n",
    "print(\"attention\", attn.size())\n",
    "# Apply the scores to v\n",
    "x = torch.matmul(attn, v)\n",
    "print(\"x\", x.size())    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 3, 3])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(scores, dim=-1).size()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.2252, 0.3892, 0.3856],\n",
       "          [0.4831, 0.2371, 0.2798],\n",
       "          [0.3575, 0.3727, 0.2698]],\n",
       "\n",
       "         [[0.5478, 0.2035, 0.2487],\n",
       "          [0.3136, 0.3352, 0.3512],\n",
       "          [0.3864, 0.3723, 0.2413]],\n",
       "\n",
       "         [[0.3558, 0.3108, 0.3334],\n",
       "          [0.3114, 0.3117, 0.3768],\n",
       "          [0.4919, 0.3081, 0.2000]],\n",
       "\n",
       "         [[0.2825, 0.3268, 0.3907],\n",
       "          [0.2674, 0.4068, 0.3257],\n",
       "          [0.4279, 0.3212, 0.2510]],\n",
       "\n",
       "         [[0.2240, 0.4012, 0.3748],\n",
       "          [0.5012, 0.2689, 0.2299],\n",
       "          [0.3869, 0.3520, 0.2611]],\n",
       "\n",
       "         [[0.2569, 0.3529, 0.3902],\n",
       "          [0.2282, 0.2568, 0.5150],\n",
       "          [0.2551, 0.3774, 0.3676]],\n",
       "\n",
       "         [[0.4467, 0.2480, 0.3053],\n",
       "          [0.3810, 0.3086, 0.3104],\n",
       "          [0.3773, 0.3159, 0.3068]],\n",
       "\n",
       "         [[0.2699, 0.4120, 0.3181],\n",
       "          [0.3626, 0.3377, 0.2997],\n",
       "          [0.2675, 0.3964, 0.3361]]]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(scores, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
