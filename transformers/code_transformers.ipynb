{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the library "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_embed = 512\n",
    "num_heads = 8\n",
    "num_batches = 1\n",
    "vocab = 50_000\n",
    "max_len = 5000\n",
    "n_layers = 1\n",
    "d_ff = 2048\n",
    "epsilon = 1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Dummy data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x torch.Size([1, 3])\n",
      "y torch.Size([1, 3])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[1,2,3]]) # Input is size batch_size x sequence_length\n",
    "y = torch.tensor([[1,2,3]]) \n",
    "x_mask = torch.tensor([[1,0,1]])\n",
    "y_mask = torch.tensor([[1,0,1]])\n",
    "print(\"x\",x.size())\n",
    "print(\"y\",y.size())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Encoder Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 512])\n"
     ]
    }
   ],
   "source": [
    "emb = nn.Embedding(vocab, d_embed)\n",
    "# We are extracting the embeddings for the tokens from the vocabulary\n",
    "# The dimensions after this operation will be batch_size x sequence_length x d_embed\n",
    "x = emb(x) \n",
    "# scale the embedding by sqrt(d_model) to make them bigger\n",
    "x = x * math.sqrt(d_embed)\n",
    "print(x.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding positional embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 512])\n"
     ]
    }
   ],
   "source": [
    "# start with empty tensor\n",
    "pe = torch.zeros(max_len, d_embed, requires_grad=False)\n",
    "# array containing index values 0 to max_len\n",
    "position = torch.arange(0,max_len).unsqueeze(1)\n",
    "divisor = torch.exp(torch.arange(0,d_embed,2)) * -(math.log(10000.0)/d_embed)\n",
    "# Make overlapping sine and cosine wave inside positional embedding tensor\n",
    "pe[:,0::2] = torch.sin(position * divisor)\n",
    "pe[:,1::2] = torch.cos(position * divisor)\n",
    "pe = pe.unsqueeze(0)\n",
    "# Add the positional embedding to the main embedding\n",
    "x = x + pe[:,:x.size(1)]\n",
    "print(x.size())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Encoder Attention Layers "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2.1.1 Set aside Residuals "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 512])\n"
     ]
    }
   ],
   "source": [
    "x_residual = x.clone()\n",
    "print(x.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2.1.2 Pre-Self Attention Layer Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 512])\n"
     ]
    }
   ],
   "source": [
    "# Centering all the values relative to mean\n",
    "# W and b are hyperparameters which needs tuning\n",
    "mean = x.mean(-1,keepdim=True)\n",
    "std = x.std(-1,keepdim=True)\n",
    "W1 = nn.Parameter(torch.ones(d_embed))\n",
    "b1 = nn.Parameter(torch.zeros(d_embed))\n",
    "x = W1 * (x - mean) / (std + epsilon) + b1\n",
    "print(x.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2.1.3 Self-Attention "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Self-attention is a process of generating scores that indicate how each token is to every other token. So we would expect a `seq_length x seg_length` matrix of values between 0 and 1, each indicating the importance of the i-th token to the j-th token.\n",
    "\n",
    "The input to self-attention is `batch_size x sequence_length x embedding_size` matrix.\n",
    "\n",
    "Self-attention copies the input `x` , three tiles and calls them `query(q)`, `key(k)` and `values(v)`. Each of these matrices go through a linear layer. The marix learns to make scores in the linear layersa. It makes each matrix different. If the networks comes up with the right, different, matrices, it will get good attention scores.\n",
    "\n",
    "`We designate chunks of each token embedding to different heads`.\n",
    "\n",
    "The q and k tensors are multiplied together. This creates a batch_size x num_heads x sequence_length x sequence_length matrix. Ignoring batching and heads, one can interpret this matrix as containing the raw scores where each cell computes how related the i-th token is to the j-th token (i is the row and j is the column).\n",
    "\n",
    "Next we pass this matrix through a softmax layer. The secret to softmax is that it can act like an argmax---it can pick the best match. Softmax squishes all values along a particular dimenion into 0...1. But what it is really doing is trying to force one particular cell to have a number close to 1 and all the rest close to 0. If we multiply this softmaxed score matrix to the v matrix, we are in essence asking (for each head), which column is best for each row. Recall that rows and columns correspond to tokens. So we are asking, which token goes best with every other token. Again, if the earlier linear layers get their parameters right, this multiplication will make good choices and loss will improve.\n",
    "\n",
    "At this point we can think of the softmaxed scores multiplied against v as tryinng to zero out everything but the most relevant token embedding (several because of multiple heads). The result, which we will store back in x for consistency is mainly the most-attended token embedding (several because of multiple heads) plus a little bit of every other embedded token sprinkled in because we can't do an actual argmax---the best we can do is get everything irrelevant to be close to zero so it doesn't impact anything else.\n",
    "\n",
    "This multiplication of the scores against the v matrix is what we refer to as self-attention. It is essentially a dot-product with an underlying learned scoring function. It basically tells us where we should look for good information. The Decoder will use this later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q torch.Size([1, 8, 3, 64])\n",
      "k torch.Size([1, 8, 3, 64])\n",
      "v torch.Size([1, 8, 3, 64])\n"
     ]
    }
   ],
   "source": [
    "# Make three versions of x for the query, key and values\n",
    "k = x\n",
    "q = x\n",
    "v = x\n",
    "# Make three linear layers\n",
    "# This is where the network learns to make scores\n",
    "linear_k = nn.Linear(d_embed, d_embed)\n",
    "linear_q = nn.Linear(d_embed, d_embed)\n",
    "linear_v = nn.Linear(d_embed, d_embed)\n",
    "# We are going to fold the embedding dimensions and treat each fold as an attention head\n",
    "d_k = d_embed // num_heads\n",
    "# Pass q, k, v through their linear layers\n",
    "q = linear_q(q)\n",
    "k = linear_k(k)\n",
    "v = linear_v(v)\n",
    "# Do the fold, treating each h dimensions as a head\n",
    "# Put the head in the second position\n",
    "q = q.view(num_batches, -1, num_heads, d_k).transpose(1,2)\n",
    "k = k.view(num_batches, -1, num_heads, d_k).transpose(1,2)\n",
    "v = v.view(num_batches, -1, num_heads, d_k).transpose(1,2)\n",
    "print(\"q\",q.size())\n",
    "print(\"k\",k.size())\n",
    "print(\"v\",v.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To produce the attention scores we multiply q and k (and normalize). We need to apply the mask so masked tokens don't attend to themselves. Apply softmax to emulate argmax (good stuff close to 1 irrelevant stuff close to 0). You won't see this happen if you look at attn because the linear layers aren't trained yet. The attention scores are finally applied to v."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention torch.Size([1, 8, 3, 3])\n",
      "x torch.Size([1, 8, 3, 64])\n"
     ]
    }
   ],
   "source": [
    "d_k = q.size(-1)\n",
    "# compute the scores by multiplying k and q (and normalize)\n",
    "scores = torch.matmul(k,q.transpose(-2,-1)) / math.sqrt(d_k)\n",
    "# Mask out the scores\n",
    "scores = scores.masked_fill(x_mask == 0, -epsilon)\n",
    "# Softmax the scores, ideally creating one score close to 1 and the rest close to 0 \n",
    "# (Note: this won't happen if you look at the numbers because the linear layers haven't \n",
    "# learned anything yet.)\n",
    "attn = F.softmax(scores,dim = -1)\n",
    "print(\"attention\",attn.size())\n",
    "# Apply the scores to v\n",
    "x = torch.matmul(attn,v)\n",
    "print(\"x\",x.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x torch.Size([1, 3, 512])\n"
     ]
    }
   ],
   "source": [
    "# Recombine the multiple attention heads (unfold)\n",
    "x = x.transpose(1,2).contiguous().view(num_batches, -1, num_heads * (d_embed // num_heads))\n",
    "print(\"x\",x.size())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2.1.4 Post Self-attention Feed forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 512])\n"
     ]
    }
   ],
   "source": [
    "ff = nn.Linear(d_embed, d_embed)\n",
    "x = ff(x)\n",
    "print(x.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2.1.5 Add residual back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 512])\n"
     ]
    }
   ],
   "source": [
    "# Adding the residual - This is changing the original embedding values for each token by some delta up or down\n",
    "x = x_residual + x\n",
    "print(x.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2 Feed Forward Module "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of this layer is a stack of hidden states, one for each token. The decoder will be able to look back and attend to the hidden state that will be most useful for decoding by looking just at this stack. To move the matrix toward a hidden state we expand the embeddings, giving the network some capacity, and then collapse it down again to force it to make trade-offs."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2.2.1 Set aside residual "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 512])\n"
     ]
    }
   ],
   "source": [
    "x_residual = x.clone()\n",
    "print(x.size())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2.2.2 Pre-Feed_Forward Layer Normalization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 512])\n"
     ]
    }
   ],
   "source": [
    "mean = x.mean(-1,keepdim=True)\n",
    "std = x.std(-1,keepdim=True)\n",
    "W2 = nn.Parameter(torch.ones(d_embed))\n",
    "b2 = nn.Parameter(torch.zeros(d_embed))\n",
    "x = W2 * (x - mean) / (std + epsilon) + b2\n",
    "print(x.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2.2.2 Pre-Feed Forward Layer Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 512])\n"
     ]
    }
   ],
   "source": [
    "# The embeddings is grown and compressed again.  This is part of process of transforming the outputs of the self-attention module into a hidden state encoding.\n",
    "linear_expand = nn.Linear(d_embed, d_ff)\n",
    "linear_compress = nn.Linear(d_ff, d_embed)\n",
    "x = linear_compress(F.relu(linear_expand(x)))\n",
    "print(x.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 512])\n"
     ]
    }
   ],
   "source": [
    "##### 1.1.2.4 Add residual block back\n",
    "x = x_residual + x\n",
    "print(x.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Final Encoder layer Normalization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 512])\n"
     ]
    }
   ],
   "source": [
    "# After repeating the self-attention and feed forward sub-layers for N times, we apply one last layer normalization\n",
    "mean = x.mean(-1, keepdim=True)\n",
    "std = x.std(-1, keepdim=True)\n",
    "Wn = nn.Parameter(torch.ones(d_embed))\n",
    "bn = nn.Parameter(torch.zeros(d_embed))\n",
    "x = Wn * (x - mean) / (std + epsilon) + bn\n",
    "print(x.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we should have a matrix, stored in x that we can interpret as a stack of hidden states. The Decoder will attempt to attend to this stack and pick out (via softmax emulating argmax) the hidden state that is most helpful in guessing the work that goes in the masked position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 512])\n"
     ]
    }
   ],
   "source": [
    "# The output is the hidden state\n",
    "hidden = x\n",
    "print(hidden.size())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
