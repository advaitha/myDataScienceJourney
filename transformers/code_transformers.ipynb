{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the library "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_embed = 512\n",
    "num_heads = 8\n",
    "num_batches = 1\n",
    "vocab = 50_000\n",
    "max_len = 5000\n",
    "n_layers = 1\n",
    "d_ff = 2048\n",
    "epsilon = 1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Dummy data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x torch.Size([1, 3])\n",
      "y torch.Size([1, 3])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[1,2,3]]) # Input is size batch_size x sequence_length\n",
    "y = torch.tensor([[1,2,3]]) \n",
    "x_mask = torch.tensor([[1,0,1]])\n",
    "y_mask = torch.tensor([[1,0,1]])\n",
    "print(\"x\",x.size())\n",
    "print(\"y\",y.size())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Encoder Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 512])\n"
     ]
    }
   ],
   "source": [
    "emb = nn.Embedding(vocab, d_embed)\n",
    "# We are extracting the embeddings for the tokens from the vocabulary\n",
    "# The dimensions after this operation will be batch_size x sequence_length x d_embed\n",
    "x = emb(x) \n",
    "# scale the embedding by sqrt(d_model) to make them bigger\n",
    "x = x * math.sqrt(d_embed)\n",
    "print(x.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding positional embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 512])\n"
     ]
    }
   ],
   "source": [
    "# start with empty tensor\n",
    "pe = torch.zeros(max_len, d_embed, requires_grad=False)\n",
    "# array containing index values 0 to max_len\n",
    "position = torch.arange(0,max_len).unsqueeze(1)\n",
    "divisor = torch.exp(torch.arange(0,d_embed,2)) * -(math.log(10000.0)/d_embed)\n",
    "# Make overlapping sine and cosine wave inside positional embedding tensor\n",
    "pe[:,0::2] = torch.sin(position * divisor)\n",
    "pe[:,1::2] = torch.cos(position * divisor)\n",
    "pe = pe.unsqueeze(0)\n",
    "# Add the positional embedding to the main embedding\n",
    "x = x + pe[:,:x.size(1)]\n",
    "print(x.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Encoder Attention Layers "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2.1.1 Set aside Residuals "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 512])\n"
     ]
    }
   ],
   "source": [
    "x_residuals = x.clone()\n",
    "print(x.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2.1.2 Pre-Self Attention Layer Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 512])\n"
     ]
    }
   ],
   "source": [
    "# Centering all the values relative to mean\n",
    "# W and b are hyperparameters which needs tuning\n",
    "mean = x.mean(-1,keepdim=True)\n",
    "std = x.std(-1,keepdim=True)\n",
    "W1 = nn.Parameter(torch.ones(d_embed))\n",
    "b1 = nn.Parameter(torch.zeros(d_embed))\n",
    "x = W1 * (x - mean) / (std + epsilon) + b1\n",
    "print(x.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2.1.3 Self-Attention "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Self-attention is a process of generating scores that indicate how each token is to every other token. So we would expect a `seq_length x seg_length` matrix of values between 0 and 1, each indicating the importance of the i-th token to the j-th token.\n",
    "\n",
    "The input to self-attention is `batch_size x sequence_length x embedding_size` matrix.\n",
    "\n",
    "Self-attention copies the input `x` , three tiles and calls them `query(q)`, `key(k)` and `values(v)`. Each of these matrices go through a linear layer. The marix learns to make scores in the linear layersa. It makes each matrix different. If the networks comes up with the right, different, matrices, it will get good attention scores.\n",
    "\n",
    "`We designate chunks of each token embedding to different heads`.\n",
    "\n",
    "The q and k tensors are multiplied together. This creates a batch_size x num_heads x sequence_length x sequence_length matrix. Ignoring batching and heads, one can interpret this matrix as containing the raw scores where each cell computes how related the i-th token is to the j-th token (i is the row and j is the column).\n",
    "\n",
    "Next we pass this matrix through a softmax layer. The secret to softmax is that it can act like an argmax---it can pick the best match. Softmax squishes all values along a particular dimenion into 0...1. But what it is really doing is trying to force one particular cell to have a number close to 1 and all the rest close to 0. If we multiply this softmaxed score matrix to the v matrix, we are in essence asking (for each head), which column is best for each row. Recall that rows and columns correspond to tokens. So we are asking, which token goes best with every other token. Again, if the earlier linear layers get their parameters right, this multiplication will make good choices and loss will improve.\n",
    "\n",
    "At this point we can think of the softmaxed scores multiplied against v as tryinng to zero out everything but the most relevant token embedding (several because of multiple heads). The result, which we will store back in x for consistency is mainly the most-attended token embedding (several because of multiple heads) plus a little bit of every other embedded token sprinkled in because we can't do an actual argmax---the best we can do is get everything irrelevant to be close to zero so it doesn't impact anything else.\n",
    "\n",
    "This multiplication of the scores against the v matrix is what we refer to as self-attention. It is essentially a dot-product with an underlying learned scoring function. It basically tells us where we should look for good information. The Decoder will use this later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q torch.Size([1, 8, 3, 64])\n",
      "k torch.Size([1, 8, 3, 64])\n",
      "v torch.Size([1, 8, 3, 64])\n"
     ]
    }
   ],
   "source": [
    "# Make three versions of x for the query, key and values\n",
    "k = x\n",
    "q = x\n",
    "v = x\n",
    "# Make three linear layers\n",
    "# This is where the network learns to make scores\n",
    "linear_k = nn.Linear(d_embed, d_embed)\n",
    "linear_q = nn.Linear(d_embed, d_embed)\n",
    "linear_v = nn.Linear(d_embed, d_embed)\n",
    "# We are going to fold the embedding dimensions and treat each fold as an attention head\n",
    "d_k = d_embed // num_heads\n",
    "# Pass q, k, v through their linear layers\n",
    "q = linear_q(q)\n",
    "k = linear_k(k)\n",
    "v = linear_v(v)\n",
    "# Do the fold, treating each h dimensions as a head\n",
    "# Put the head in the second position\n",
    "q = q.view(num_batches, -1, num_heads, d_k).transpose(1,2)\n",
    "k = k.view(num_batches, -1, num_heads, d_k).transpose(1,2)\n",
    "v = v.view(num_batches, -1, num_heads, d_k).transpose(1,2)\n",
    "print(\"q\",q.size())\n",
    "print(\"k\",k.size())\n",
    "print(\"v\",v.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To produce the attention scores we multiply q and k (and normalize). We need to apply the mask so masked tokens don't attend to themselves. Apply softmax to emulate argmax (good stuff close to 1 irrelevant stuff close to 0). You won't see this happen if you look at attn because the linear layers aren't trained yet. The attention scores are finally applied to v."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_k = q.size(-1)\n",
    "# compute the scores by multiplying k and q (and normalize)\n",
    "scores = torch.matmul(k,q.transpose(-2,-1)) / math.sqrt(d_K)\n",
    "# Mask out the scores\n",
    "scores = scores.masked_fill(x_mask == 0, -epsilon)\n",
    "# Softmax the scores, ideally creating one score close to 1 and the rest close to 0 \n",
    "# (Note: this won't happen if you look at the numbers because the linear layers haven't \n",
    "# learned anything yet.)\n",
    "attn = F.softmax(scores,dim = -1)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
